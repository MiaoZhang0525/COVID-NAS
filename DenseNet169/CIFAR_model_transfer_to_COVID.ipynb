{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils_imagenet\n",
    "import logging\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import genotypes\n",
    "import torch.utils\n",
    "import torchvision.datasets as dset\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from model import NetworkCIFAR as Network\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\"cifar\")\n",
    "parser.add_argument('--data', type=str, default='/home/mzhang3/Data/DARTS/darts-master/data', help='location of the data corpus')\n",
    "parser.add_argument('--batch_size', type=int, default=48, help='batch size')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.025, help='init learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
    "parser.add_argument('--weight_decay', type=float, default=3e-4, help='weight decay')\n",
    "parser.add_argument('--report_freq', type=float, default=50, help='report frequency')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
    "parser.add_argument('--epochs', type=int, default=600, help='num of training epochs')\n",
    "parser.add_argument('--init_channels', type=int, default=36, help='num of init channels')\n",
    "parser.add_argument('--layers', type=int, default=20, help='total number of layers')\n",
    "parser.add_argument('--model_path', type=str, default='saved_models', help='path to save the model')\n",
    "parser.add_argument('--auxiliary', action='store_true', default=True, help='use auxiliary tower')\n",
    "parser.add_argument('--auxiliary_weight', type=float, default=0.4, help='weight for auxiliary loss')\n",
    "parser.add_argument('--cutout', action='store_true', default=True, help='use cutout')\n",
    "parser.add_argument('--cutout_length', type=int, default=16, help='cutout length')\n",
    "parser.add_argument('--drop_path_prob', type=float, default=0.2, help='drop path probability')\n",
    "parser.add_argument('--save', type=str, default='EXP', help='experiment name')\n",
    "parser.add_argument('--seed', type=int, default=0, help='random seed')\n",
    "parser.add_argument('--arch', type=str, default='ENNAS', help='which architecture to use')\n",
    "parser.add_argument('--grad_clip', type=float, default=5, help='gradient clipping')\n",
    "\n",
    "args = parser.parse_args([])\n",
    "CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.cuda.set_device(args.gpu)\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(args.seed)\n",
    "cudnn.enabled=True\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "logging.info('gpu device = %d' % args.gpu)\n",
    "logging.info(\"args = %s\", args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 108 36\n",
      "108 144 36\n",
      "144 144 36\n",
      "144 144 36\n",
      "144 144 36\n",
      "144 144 36\n",
      "144 144 72\n",
      "144 288 72\n",
      "288 288 72\n",
      "288 288 72\n",
      "288 288 72\n",
      "288 288 72\n",
      "288 288 72\n",
      "288 288 144\n",
      "288 576 144\n",
      "576 576 144\n",
      "576 576 144\n",
      "576 576 144\n",
      "576 576 144\n",
      "576 576 144\n"
     ]
    }
   ],
   "source": [
    "genotype = eval(\"genotypes.%s\" % args.arch)\n",
    "pre_model = Network(args.init_channels, CLASSES, args.layers, args.auxiliary, genotype)\n",
    "pre_model = pre_model.cuda()\n",
    "utils_imagenet.load(pre_model, 'cifar10_600.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_model.load_state_dict(torch.load('cifar10_600.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/taoliu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data/taoliu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data/taoliu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data/taoliu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data/taoliu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data/taoliu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "from PIL import ImageFile\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random \n",
    "from shutil import copyfile\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensor\n",
    "from catalyst.data import Augmentor\n",
    "import torchxrayvision as xrv\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import re\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensor\n",
    "from catalyst.data import Augmentor\n",
    "from skimage.io import imread, imsave\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "train_transformer = transforms.Compose([\n",
    "    transforms.Resize(48),\n",
    "    transforms.RandomResizedCrop((32),scale=(0.5,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "val_transformer = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.CenterCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "\n",
    "# In[80]:\n",
    "\n",
    "\n",
    "batchsize=4\n",
    "def read_txt(txt_path):\n",
    "    with open(txt_path) as f:\n",
    "        lines = f.readlines()\n",
    "    txt_data = [line.strip() for line in lines]\n",
    "    return txt_data\n",
    "\n",
    "class CovidCTDataset(Dataset):\n",
    "    def __init__(self, root_dir, txt_COVID, txt_NonCOVID, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            txt_path (string): Path to the txt file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        File structure:\n",
    "        - root_dir\n",
    "            - CT_COVID\n",
    "                - img1.png\n",
    "                - img2.png\n",
    "                - ......\n",
    "            - CT_NonCOVID\n",
    "                - img1.png\n",
    "                - img2.png\n",
    "                - ......\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.txt_path = [txt_COVID,txt_NonCOVID]\n",
    "        self.classes = ['CT_COVID', 'CT_NonCOVID']\n",
    "        self.num_cls = len(self.classes)\n",
    "        self.img_list = []\n",
    "        for c in range(self.num_cls):\n",
    "            cls_list = [[os.path.join(self.root_dir,self.classes[c],item), c] for item in read_txt(self.txt_path[c])]\n",
    "            self.img_list += cls_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = self.img_list[idx][0]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        sample = {'img': image,\n",
    "                  'label': int(self.img_list[idx][1])}\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424\n",
      "116\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "trainset = CovidCTDataset(root_dir='/tmp/mozilla_taoliu0/COVID-CT-master/Images-processed',\n",
    "                          txt_COVID='/tmp/mozilla_taoliu0/COVID-CT-master/Data-split/COVID/trainCT_COVID.txt',\n",
    "                          txt_NonCOVID='/tmp/mozilla_taoliu0/COVID-CT-master/Data-split/NonCOVID/trainCT_NonCOVID.txt',\n",
    "                          transform= train_transformer)\n",
    "valset = CovidCTDataset(root_dir='/tmp/mozilla_taoliu0/COVID-CT-master/Images-processed',\n",
    "                          txt_COVID='/tmp/mozilla_taoliu0/COVID-CT-master/Data-split/COVID/valCT_COVID.txt',\n",
    "                          txt_NonCOVID='/tmp/mozilla_taoliu0/COVID-CT-master/Data-split/NonCOVID/valCT_NonCOVID.txt',\n",
    "                          transform= val_transformer)\n",
    "testset = CovidCTDataset(root_dir='/tmp/mozilla_taoliu0/COVID-CT-master/Images-processed',\n",
    "                          txt_COVID='/tmp/mozilla_taoliu0/COVID-CT-master/Data-split/COVID/testCT_COVID.txt',\n",
    "                          txt_NonCOVID='/tmp/mozilla_taoliu0/COVID-CT-master/Data-split/NonCOVID/testCT_NonCOVID.txt',\n",
    "                          transform= val_transformer)\n",
    "print(trainset.__len__())\n",
    "print(valset.__len__())\n",
    "print(testset.__len__())\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batchsize, drop_last=True, shuffle=True,pin_memory=True, num_workers=4)\n",
    "val_loader = DataLoader(valset, batch_size=batchsize, drop_last=True, shuffle=False,pin_memory=True, num_workers=4)\n",
    "test_loader = DataLoader(testset, batch_size=batchsize, drop_last=True, shuffle=False,pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'NAS_CIFAR_Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 108 36\n",
      "108 144 36\n",
      "144 144 36\n",
      "144 144 36\n",
      "144 144 36\n",
      "144 144 36\n",
      "144 144 72\n",
      "144 288 72\n",
      "288 288 72\n",
      "288 288 72\n",
      "288 288 72\n",
      "288 288 72\n",
      "288 288 72\n",
      "288 288 144\n",
      "288 576 144\n",
      "576 576 144\n",
      "576 576 144\n",
      "576 576 144\n",
      "576 576 144\n",
      "576 576 144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): NetworkCIFAR(\n",
       "    (stem): Sequential(\n",
       "      (0): Conv2d(3, 108, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (cells): ModuleList(\n",
       "      (0): Cell(\n",
       "        (preprocess0): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(108, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (preprocess1): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(108, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (_ops): ModuleList(\n",
       "          (0): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (6): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)\n",
       "              (6): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (6): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          (6): Identity()\n",
       "          (7): Zero()\n",
       "        )\n",
       "      )\n",
       "      (1): Cell(\n",
       "        (preprocess0): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(108, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (preprocess1): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (_ops): ModuleList(\n",
       "          (0): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (6): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)\n",
       "              (6): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (6): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          (6): Identity()\n",
       "          (7): Zero()\n",
       "        )\n",
       "      )\n",
       "      (2): Cell(\n",
       "        (preprocess0): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (preprocess1): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (_ops): ModuleList(\n",
       "          (0): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (6): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)\n",
       "              (6): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (6): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          (6): Identity()\n",
       "          (7): Zero()\n",
       "        )\n",
       "      )\n",
       "      (3): Cell(\n",
       "        (preprocess0): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (preprocess1): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (_ops): ModuleList(\n",
       "          (0): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (6): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)\n",
       "              (6): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (6): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          (6): Identity()\n",
       "          (7): Zero()\n",
       "        )\n",
       "      )\n",
       "      (4): Cell(\n",
       "        (preprocess0): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (preprocess1): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (_ops): ModuleList(\n",
       "          (0): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (6): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)\n",
       "              (6): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (6): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          (6): Identity()\n",
       "          (7): Zero()\n",
       "        )\n",
       "      )\n",
       "      (5): Cell(\n",
       "        (preprocess0): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (preprocess1): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (_ops): ModuleList(\n",
       "          (0): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (6): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)\n",
       "              (6): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (2): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(36, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=36, bias=False)\n",
       "              (6): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          (6): Identity()\n",
       "          (7): Zero()\n",
       "        )\n",
       "      )\n",
       "      (6): Cell(\n",
       "        (preprocess0): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (preprocess1): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (_ops): ModuleList(\n",
       "          (0): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Zero()\n",
       "          (2): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "          (4): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (6): Zero()\n",
       "          (7): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "              (6): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Cell(\n",
       "        (preprocess0): FactorizedReduce(\n",
       "          (relu): ReLU()\n",
       "          (conv_1): Conv2d(144, 36, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (conv_2): Conv2d(144, 36, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (preprocess1): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (_ops): ModuleList(\n",
       "          (0): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (6): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "              (6): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (6): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          (6): Identity()\n",
       "          (7): Zero()\n",
       "        )\n",
       "      )\n",
       "      (8): Cell(\n",
       "        (preprocess0): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (preprocess1): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (_ops): ModuleList(\n",
       "          (0): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (6): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "              (6): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (6): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          (6): Identity()\n",
       "          (7): Zero()\n",
       "        )\n",
       "      )\n",
       "      (9): Cell(\n",
       "        (preprocess0): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (preprocess1): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (_ops): ModuleList(\n",
       "          (0): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (6): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "              (6): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (6): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          (6): Identity()\n",
       "          (7): Zero()\n",
       "        )\n",
       "      )\n",
       "      (10): Cell(\n",
       "        (preprocess0): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (preprocess1): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (_ops): ModuleList(\n",
       "          (0): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (6): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "              (6): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (6): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          (6): Identity()\n",
       "          (7): Zero()\n",
       "        )\n",
       "      )\n",
       "      (11): Cell(\n",
       "        (preprocess0): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (preprocess1): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (_ops): ModuleList(\n",
       "          (0): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (6): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "              (6): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (6): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          (6): Identity()\n",
       "          (7): Zero()\n",
       "        )\n",
       "      )\n",
       "      (12): Cell(\n",
       "        (preprocess0): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (preprocess1): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (_ops): ModuleList(\n",
       "          (0): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (6): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "              (6): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (2): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
       "              (6): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          (6): Identity()\n",
       "          (7): Zero()\n",
       "        )\n",
       "      )\n",
       "      (13): Cell(\n",
       "        (preprocess0): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(288, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (preprocess1): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(288, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (_ops): ModuleList(\n",
       "          (0): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Zero()\n",
       "          (2): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "          (4): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (6): Zero()\n",
       "          (7): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (6): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): Cell(\n",
       "        (preprocess0): FactorizedReduce(\n",
       "          (relu): ReLU()\n",
       "          (conv_1): Conv2d(288, 72, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (conv_2): Conv2d(288, 72, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (preprocess1): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (_ops): ModuleList(\n",
       "          (0): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (6): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (6): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (6): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          (6): Identity()\n",
       "          (7): Zero()\n",
       "        )\n",
       "      )\n",
       "      (15): Cell(\n",
       "        (preprocess0): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (preprocess1): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (_ops): ModuleList(\n",
       "          (0): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (6): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (6): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (6): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          (6): Identity()\n",
       "          (7): Zero()\n",
       "        )\n",
       "      )\n",
       "      (16): Cell(\n",
       "        (preprocess0): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (preprocess1): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (_ops): ModuleList(\n",
       "          (0): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (6): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (6): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (6): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          (6): Identity()\n",
       "          (7): Zero()\n",
       "        )\n",
       "      )\n",
       "      (17): Cell(\n",
       "        (preprocess0): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (preprocess1): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (_ops): ModuleList(\n",
       "          (0): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (6): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (6): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (6): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          (6): Identity()\n",
       "          (7): Zero()\n",
       "        )\n",
       "      )\n",
       "      (18): Cell(\n",
       "        (preprocess0): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (preprocess1): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (_ops): ModuleList(\n",
       "          (0): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (6): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (6): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (6): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          (6): Identity()\n",
       "          (7): Zero()\n",
       "        )\n",
       "      )\n",
       "      (19): Cell(\n",
       "        (preprocess0): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (preprocess1): ReLUConvBN(\n",
       "          (op): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (_ops): ModuleList(\n",
       "          (0): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (6): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DilConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (6): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): SepConv(\n",
       "            (op): Sequential(\n",
       "              (0): ReLU()\n",
       "              (1): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (2): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (4): ReLU()\n",
       "              (5): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "              (6): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (7): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          (6): Identity()\n",
       "          (7): Zero()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (auxiliary_head): AuxiliaryHeadCIFAR(\n",
       "      (features): Sequential(\n",
       "        (0): ReLU(inplace=True)\n",
       "        (1): AvgPool2d(kernel_size=5, stride=3, padding=0)\n",
       "        (2): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 768, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "      (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "    )\n",
       "    (global_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Linear(in_features=576, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = None\n",
    "device = 'cuda'\n",
    "\n",
    "model = Network(args.init_channels, 2, args.layers, args.auxiliary, genotype)\n",
    "model = torch.nn.DataParallel(model)\n",
    "new_model = model.cuda()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model = torch.nn.DataParallel(pre_model)\n",
    "\n",
    "newmodel_dict=model.state_dict()\n",
    "premodel_dict=pre_model.state_dict()\n",
    "new_list=list(newmodel_dict.keys())\n",
    "pre_list=list(premodel_dict.keys())\n",
    "for i in range(1352):####The model contains 1356 keys, we need exclude the last layer\n",
    "    newmodel_dict[new_list[i]]=premodel_dict[pre_list[i]]\n",
    "\n",
    "model.load_state_dict(newmodel_dict)\n",
    "\n",
    "model=model.module\n",
    "#dict_name=list(model.state_dict())\n",
    "#for i,p in enumerate(dict_name):\n",
    "  #  print(i,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<input>:19: DeprecationWarning:\n",
      "\n",
      "'async' and 'await' will become reserved keywords in Python 3.7\n",
      "\n",
      "<ipython-input-11-731ad0c3d451>:19: DeprecationWarning:\n",
      "\n",
      "'async' and 'await' will become reserved keywords in Python 3.7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(train_loader, model, criterion, optimizer):\n",
    "    objs = utils_imagenet.AvgrageMeter()\n",
    "    top1 = utils_imagenet.AvgrageMeter()\n",
    "    model.train()\n",
    "    \n",
    "    loss = 0\n",
    "    train_correct = 0\n",
    "\n",
    "    for step, batch_samples in enumerate(train_loader):\n",
    "        \n",
    "        input, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n",
    "      #  input = input[:, 0, :, :]\n",
    "      #  input = input[:, None, :, :]\n",
    "#         data, targets_a, targets_b, lam = mixup_data(data, target.long(), alpha, use_cuda=True)\n",
    "      #  input = input.repeat(1,3,1,1)        \n",
    "        \n",
    "        \n",
    "        target = target.cuda(async=True)\n",
    "        input = input.cuda()\n",
    "        input = Variable(input)\n",
    "        target = Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, logits_aux = model(input)\n",
    "        loss = criterion(logits, target)\n",
    "        if args.auxiliary:\n",
    "            loss_aux = criterion(logits_aux, target)\n",
    "            loss += args.auxiliary_weight*loss_aux\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(model.parameters(), args.grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        prec1, prec2 = utils_imagenet.accuracy(logits, target, topk=(1,2))\n",
    "        n = input.size(0)\n",
    "        objs.update(loss.data, n)\n",
    "        top1.update(prec1.data, n)\n",
    "       # top2.update(prec5.data, n)\n",
    "\n",
    "        if step % args.report_freq == 0:\n",
    "            logging.info('train %03d %e %f', step, objs.avg, top1.avg)\n",
    "            \n",
    "\n",
    "\n",
    "        pred = logits.argmax(dim=1, keepdim=True)\n",
    "        train_correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
    "    \n",
    "        # Display progress and write to tensorboard\n",
    "        if step % bs == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}'.format(\n",
    "                epoch, step, len(train_loader),\n",
    "                100.0 * step / len(train_loader), loss.item()/ bs))\n",
    "    \n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss/len(train_loader.dataset), train_correct, len(train_loader.dataset),\n",
    "        100.0 * train_correct / len(train_loader.dataset)))\n",
    "    f = open('model_result/{}.txt'.format(modelname), 'a+')\n",
    "    f.write('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss/len(train_loader.dataset), train_correct, len(train_loader.dataset),\n",
    "        100.0 * train_correct / len(train_loader.dataset)))\n",
    "    f.write('\\n')\n",
    "    f.close()\n",
    "            \n",
    "            \n",
    "\n",
    "    return top1.avg, objs.avg    \n",
    "    \n",
    "    \n",
    "def infer(val_loader, model, criterion,epoch):\n",
    "    \n",
    "    objs = utils_imagenet.AvgrageMeter()\n",
    "    top1 = utils_imagenet.AvgrageMeter()\n",
    "    top5 = utils_imagenet.AvgrageMeter()\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    results = []\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    \n",
    "    \n",
    "    # Don't update model\n",
    "    with torch.no_grad():\n",
    "        tpr_list = []\n",
    "        fpr_list = []\n",
    "        \n",
    "        predlist=[]\n",
    "        scorelist=[]\n",
    "        targetlist=[]\n",
    "        # Predict\n",
    "        \n",
    "    \n",
    "        for batch_index, batch_samples in enumerate(val_loader):\n",
    "            data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n",
    "            data = data[:, 0, :, :]\n",
    "            data = data[:, None, :, :]\n",
    "            data = data.repeat(1,3,1,1)\n",
    "            data = Variable(data, volatile=True).cuda()\n",
    "            target = Variable(target, volatile=True).cuda(async=True)            \n",
    "            \n",
    "            \n",
    "            output, logits_aux = model(data)\n",
    "            \n",
    "            test_loss += criterion(output, target)\n",
    "            score = F.softmax(output, dim=1)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "#             print('target',target.long()[:, 2].view_as(pred))\n",
    "            correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
    "            \n",
    "#             print(output[:,1].cpu().numpy())\n",
    "#             print((output[:,1]+output[:,0]).cpu().numpy())\n",
    "#             predcpu=(output[:,1].cpu().numpy())/((output[:,1]+output[:,0]).cpu().numpy())\n",
    "            targetcpu=target.long().cpu().numpy()\n",
    "            predlist=np.append(predlist, pred.cpu().numpy())\n",
    "            scorelist=np.append(scorelist, score.cpu().numpy()[:,1])\n",
    "            targetlist=np.append(targetlist,targetcpu)\n",
    "           \n",
    "          \n",
    "    return targetlist, scorelist, predlist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "bs = 10\n",
    "votenum = 10\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "r_list = []\n",
    "p_list = []\n",
    "acc_list = []\n",
    "AUC_list = []\n",
    "# TP = 0\n",
    "# TN = 0\n",
    "# FN = 0\n",
    "# FP = 0\n",
    "#vote_pred = np.zeros(valset.__len__())#####since we dropped several data\n",
    "#vote_score = np.zeros(valset.__len__())\n",
    "\n",
    "vote_pred = np.zeros(valset.__len__())\n",
    "vote_score = np.zeros(valset.__len__())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum = 0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "#scheduler = StepLR(optimizer, step_size=1)\n",
    "\n",
    "total_epoch = 3000\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/106 (0%)]\tTrain Loss: 0.093001\n",
      "Train Epoch: 0 [10/106 (9%)]\tTrain Loss: 0.098486\n",
      "Train Epoch: 0 [20/106 (19%)]\tTrain Loss: 0.074945\n",
      "Train Epoch: 0 [30/106 (28%)]\tTrain Loss: 0.092415\n",
      "Train Epoch: 0 [40/106 (38%)]\tTrain Loss: 0.072195\n",
      "Train Epoch: 0 [50/106 (47%)]\tTrain Loss: 0.071060\n",
      "Train Epoch: 0 [60/106 (57%)]\tTrain Loss: 0.099106\n",
      "Train Epoch: 0 [70/106 (66%)]\tTrain Loss: 0.086640\n",
      "Train Epoch: 0 [80/106 (75%)]\tTrain Loss: 0.092945\n",
      "Train Epoch: 0 [90/106 (85%)]\tTrain Loss: 0.113790\n",
      "Train Epoch: 0 [100/106 (94%)]\tTrain Loss: 0.067097\n",
      "\n",
      "Train set: Average loss: 0.0024, Accuracy: 282/424 (67%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.26936156 0.25690106 0.17919816 0.17492713 0.17961182 0.20131047\n",
      " 0.25100252 0.23434259 0.25404701 0.23168008 0.16219713 0.19836845\n",
      " 0.16137245 0.3395412  0.40140298 0.17978996 0.1690281  0.27389568\n",
      " 0.16702473 0.12233921 0.12078448 0.1587851  0.27011052 0.19283865\n",
      " 0.12200337 0.33607554 0.43926194 0.26524258 0.17565222 0.20647143\n",
      " 0.42751801 0.563739   0.63407207 0.11248046 0.16431838 0.14326623\n",
      " 0.12016786 0.35260153 0.34972715 0.22765701 0.22149804 0.19896039\n",
      " 0.37463892 0.30640569 0.44787371 0.37298825 0.36099669 0.42166144\n",
      " 0.55060709 0.15490757 0.52401471 0.21330342 0.24285206 0.26628017\n",
      " 0.15981704 0.3009918  0.20785564 0.16409206 0.23403323 0.29046708\n",
      " 0.50809777 0.40483579 0.54108775 0.58139139 0.55904895 0.65594798\n",
      " 0.32744133 0.51354325 0.29938975 0.22747172 0.37283766 0.3586832\n",
      " 0.44872731 0.33545229 0.55863273 0.40124494 0.45875978 0.50187027\n",
      " 0.54036212 0.58534402 0.57594329 0.61915112 0.60073113 0.47495696\n",
      " 0.21401955 0.43746212 0.38290849 0.40312818 0.15308419 0.1542016\n",
      " 0.2477375  0.38678452 0.45398438 0.3457796  0.21305989 0.1264009\n",
      " 0.17863271 0.27804771 0.15558578 0.14823629 0.20815091 0.50149906\n",
      " 0.23848556 0.26593801 0.17098381 0.28952327 0.25614551 0.20538674\n",
      " 0.11800352 0.12878427 0.11457931 0.110886   0.21184519 0.145981\n",
      " 0.40501529 0.37450695]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 0 TN= 60 FN= 56 FP= 0\n",
      "TP+FP 0\n",
      "precision nan\n",
      "recall 0.0\n",
      "F1 nan\n",
      "acc 0.5172413793103449\n",
      "AUCp 0.5\n",
      "AUC 0.6458333333333334\n",
      "\n",
      " The epoch is 0, average recall: 0.0000, average precision: nan,average F1: nan, average accuracy: 0.5172, average AUC: 0.6458\n",
      "Train Epoch: 1 [0/106 (0%)]\tTrain Loss: 0.097423\n",
      "Train Epoch: 1 [10/106 (9%)]\tTrain Loss: 0.056708\n",
      "Train Epoch: 1 [20/106 (19%)]\tTrain Loss: 0.070735\n",
      "Train Epoch: 1 [30/106 (28%)]\tTrain Loss: 0.088530\n",
      "Train Epoch: 1 [40/106 (38%)]\tTrain Loss: 0.056314\n",
      "Train Epoch: 1 [50/106 (47%)]\tTrain Loss: 0.071516\n",
      "Train Epoch: 1 [60/106 (57%)]\tTrain Loss: 0.102759\n",
      "Train Epoch: 1 [70/106 (66%)]\tTrain Loss: 0.051885\n",
      "Train Epoch: 1 [80/106 (75%)]\tTrain Loss: 0.053712\n",
      "Train Epoch: 1 [90/106 (85%)]\tTrain Loss: 0.048583\n",
      "Train Epoch: 1 [100/106 (94%)]\tTrain Loss: 0.111783\n",
      "\n",
      "Train set: Average loss: 0.0012, Accuracy: 316/424 (75%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.12807606 0.18894631 0.04426926 0.01900324 0.03228967 0.04004996\n",
      " 0.18458495 0.04189278 0.09947663 0.01877263 0.01175966 0.02697759\n",
      " 0.00694657 0.35033381 0.6189248  0.01950834 0.01025704 0.1280732\n",
      " 0.01131093 0.03487804 0.04823321 0.0240628  0.08688796 0.08718722\n",
      " 0.0119387  0.17799988 0.42315608 0.04442226 0.01181952 0.01278482\n",
      " 0.0475824  0.0889046  0.40005362 0.00511118 0.00503362 0.01093669\n",
      " 0.01256316 0.03132387 0.12000836 0.05453169 0.05124042 0.03879722\n",
      " 0.0756163  0.03426001 0.02131525 0.00731594 0.06398956 0.10032885\n",
      " 0.25053164 0.02781839 0.14531147 0.00928894 0.02749961 0.01072906\n",
      " 0.0312141  0.03934016 0.01609204 0.01132578 0.0693206  0.01531479\n",
      " 0.63491708 0.53833389 0.76270062 0.71201557 0.10982766 0.12087592\n",
      " 0.08506964 0.3280507  0.0667624  0.10699541 0.33453977 0.25267667\n",
      " 0.01063076 0.03118155 0.03379916 0.39737362 0.62753206 0.65843093\n",
      " 0.75625944 0.3511942  0.58243334 0.58903378 0.75520498 0.34036782\n",
      " 0.10681301 0.16074817 0.15255207 0.23198995 0.20780867 0.04677079\n",
      " 0.26632673 0.02307356 0.06356252 0.14685811 0.03375059 0.02222064\n",
      " 0.01201524 0.34073204 0.14586543 0.02093419 0.17822802 0.51169538\n",
      " 0.22353344 0.01119261 0.00857762 0.28670371 0.05386306 0.03246945\n",
      " 0.02430437 0.01447539 0.00854858 0.03359893 0.06524353 0.00927032\n",
      " 0.44865447 0.26250193]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 2 [0/106 (0%)]\tTrain Loss: 0.036704\n",
      "Train Epoch: 2 [10/106 (9%)]\tTrain Loss: 0.064045\n",
      "Train Epoch: 2 [20/106 (19%)]\tTrain Loss: 0.108732\n",
      "Train Epoch: 2 [30/106 (28%)]\tTrain Loss: 0.049816\n",
      "Train Epoch: 2 [40/106 (38%)]\tTrain Loss: 0.073063\n",
      "Train Epoch: 2 [50/106 (47%)]\tTrain Loss: 0.076370\n",
      "Train Epoch: 2 [60/106 (57%)]\tTrain Loss: 0.136043\n",
      "Train Epoch: 2 [70/106 (66%)]\tTrain Loss: 0.024811\n",
      "Train Epoch: 2 [80/106 (75%)]\tTrain Loss: 0.046546\n",
      "Train Epoch: 2 [90/106 (85%)]\tTrain Loss: 0.042264\n",
      "Train Epoch: 2 [100/106 (94%)]\tTrain Loss: 0.132554\n",
      "\n",
      "Train set: Average loss: 0.0015, Accuracy: 337/424 (79%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.0915743  0.30149788 0.04579914 0.01029771 0.00999937 0.01480849\n",
      " 0.28722519 0.02746155 0.07185458 0.02307734 0.00768969 0.01236376\n",
      " 0.0034345  0.2318341  0.60432744 0.00922628 0.0091487  0.25497609\n",
      " 0.00891424 0.05856987 0.06730691 0.01573471 0.07421558 0.40540612\n",
      " 0.00900229 0.14420544 0.8352915  0.0463006  0.005269   0.00739192\n",
      " 0.08213571 0.05838874 0.58928537 0.00251366 0.00335424 0.00401979\n",
      " 0.00791314 0.02704474 0.03637878 0.02884952 0.03273025 0.03811473\n",
      " 0.10262948 0.01823294 0.00997027 0.00600623 0.08965841 0.21901411\n",
      " 0.73626512 0.24697566 0.69610256 0.00265634 0.02017997 0.002602\n",
      " 0.16025853 0.01709459 0.08563881 0.002913   0.02889921 0.01154053\n",
      " 0.50200075 0.29031414 0.62674665 0.71705538 0.06786532 0.24131839\n",
      " 0.09206304 0.28673729 0.06842197 0.33799392 0.4016616  0.38085335\n",
      " 0.01758495 0.0747231  0.12243548 0.40418425 0.70276767 0.71803415\n",
      " 0.82635891 0.34598774 0.10824727 0.22138274 0.85939693 0.14924113\n",
      " 0.06056019 0.0870679  0.15491205 0.42590523 0.06082347 0.01357964\n",
      " 0.23867951 0.0171416  0.05477099 0.71651441 0.05036866 0.01633938\n",
      " 0.0058308  0.13082875 0.04592364 0.02752364 0.07347161 0.63111418\n",
      " 0.08865456 0.0082351  0.00803987 0.0556921  0.26451224 0.1212889\n",
      " 0.04450812 0.01702694 0.02421973 0.03459381 0.10550815 0.02188003\n",
      " 0.30914414 0.13160093]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [0/106 (0%)]\tTrain Loss: 0.095509\n",
      "Train Epoch: 3 [10/106 (9%)]\tTrain Loss: 0.161592\n",
      "Train Epoch: 3 [20/106 (19%)]\tTrain Loss: 0.043424\n",
      "Train Epoch: 3 [30/106 (28%)]\tTrain Loss: 0.129203\n",
      "Train Epoch: 3 [40/106 (38%)]\tTrain Loss: 0.026931\n",
      "Train Epoch: 3 [50/106 (47%)]\tTrain Loss: 0.044906\n",
      "Train Epoch: 3 [60/106 (57%)]\tTrain Loss: 0.050206\n",
      "Train Epoch: 3 [70/106 (66%)]\tTrain Loss: 0.082228\n",
      "Train Epoch: 3 [80/106 (75%)]\tTrain Loss: 0.021845\n",
      "Train Epoch: 3 [90/106 (85%)]\tTrain Loss: 0.122352\n",
      "Train Epoch: 3 [100/106 (94%)]\tTrain Loss: 0.015238\n",
      "\n",
      "Train set: Average loss: 0.0007, Accuracy: 339/424 (80%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.38562059e-02 1.14653796e-01 1.73719358e-02 7.37660611e-03\n",
      " 5.11214789e-03 3.66745633e-03 1.24987289e-01 1.99860893e-02\n",
      " 1.15382290e-02 6.38513081e-03 4.36443929e-03 3.15274275e-03\n",
      " 2.53692060e-03 3.66452821e-02 3.70814830e-01 2.53652222e-03\n",
      " 2.60201539e-03 7.22209811e-02 6.20400812e-03 5.86399026e-02\n",
      " 3.20925787e-02 8.51446576e-03 1.02029279e-01 2.77754903e-01\n",
      " 4.73004999e-03 1.88882887e-01 8.80641520e-01 3.19685973e-02\n",
      " 2.17260187e-03 4.28553764e-03 1.37912363e-01 2.61344582e-01\n",
      " 4.20020133e-01 7.14258058e-04 8.48032651e-04 1.44004030e-03\n",
      " 4.53409506e-03 2.61389129e-02 2.89165843e-02 1.66834053e-02\n",
      " 1.75675154e-02 2.40992811e-02 3.71438898e-02 9.05431435e-03\n",
      " 1.14632091e-02 5.44112781e-03 1.05068512e-01 2.85241812e-01\n",
      " 8.23743522e-01 1.54313281e-01 8.58216524e-01 1.02289103e-03\n",
      " 7.44952541e-03 1.26573408e-03 8.21601301e-02 6.34314539e-03\n",
      " 3.07309255e-02 8.62958026e-04 8.21513124e-03 7.10222917e-03\n",
      " 3.75156343e-01 1.58642128e-01 5.86440861e-01 6.42043233e-01\n",
      " 2.11863399e-01 1.89360276e-01 4.14520390e-02 4.19356287e-01\n",
      " 6.58534542e-02 6.44563496e-01 3.57721746e-01 3.43370616e-01\n",
      " 2.54932679e-02 1.89936072e-01 2.28553399e-01 3.08404684e-01\n",
      " 6.81305647e-01 7.13792086e-01 8.50053370e-01 4.05529261e-01\n",
      " 3.97153437e-01 4.76888508e-01 8.33345115e-01 8.94062966e-02\n",
      " 3.02610919e-02 1.03298314e-01 1.99736640e-01 2.51159251e-01\n",
      " 2.94998642e-02 1.92743652e-02 2.00292364e-01 1.71837602e-02\n",
      " 5.99971153e-02 6.36082530e-01 1.50245521e-02 6.71144016e-03\n",
      " 2.78330338e-03 9.35577303e-02 4.52478006e-02 3.05848569e-02\n",
      " 6.89157471e-02 4.90100741e-01 4.92097735e-02 1.85743049e-02\n",
      " 5.53697906e-03 2.11444452e-01 3.22579205e-01 8.09571967e-02\n",
      " 1.40616708e-02 1.06401341e-02 1.35452291e-02 1.41179394e-02\n",
      " 3.76745500e-02 8.15815851e-03 2.72600114e-01 6.85111061e-02]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 4 [0/106 (0%)]\tTrain Loss: 0.058778\n",
      "Train Epoch: 4 [10/106 (9%)]\tTrain Loss: 0.158952\n",
      "Train Epoch: 4 [20/106 (19%)]\tTrain Loss: 0.016117\n",
      "Train Epoch: 4 [30/106 (28%)]\tTrain Loss: 0.036294\n",
      "Train Epoch: 4 [40/106 (38%)]\tTrain Loss: 0.159341\n",
      "Train Epoch: 4 [50/106 (47%)]\tTrain Loss: 0.027756\n",
      "Train Epoch: 4 [60/106 (57%)]\tTrain Loss: 0.078270\n",
      "Train Epoch: 4 [70/106 (66%)]\tTrain Loss: 0.071155\n",
      "Train Epoch: 4 [80/106 (75%)]\tTrain Loss: 0.074792\n",
      "Train Epoch: 4 [90/106 (85%)]\tTrain Loss: 0.020076\n",
      "Train Epoch: 4 [100/106 (94%)]\tTrain Loss: 0.031675\n",
      "\n",
      "Train set: Average loss: 0.0021, Accuracy: 340/424 (80%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.05892291 0.12151635 0.01567571 0.00687128 0.00871866 0.00890372\n",
      " 0.09379636 0.01263206 0.03170952 0.01867225 0.00651893 0.00851189\n",
      " 0.0019675  0.02286926 0.26284799 0.00212958 0.00214908 0.06665841\n",
      " 0.00468045 0.05502307 0.04761264 0.00634673 0.13746391 0.315799\n",
      " 0.00481537 0.27265844 0.94327325 0.02930573 0.00280902 0.00640543\n",
      " 0.07462747 0.09641208 0.28510454 0.00129703 0.00159908 0.00228108\n",
      " 0.00301792 0.00961675 0.01597356 0.00872309 0.01103372 0.01117536\n",
      " 0.03931843 0.0064893  0.00653361 0.00693613 0.06856918 0.16197227\n",
      " 0.58742118 0.12295662 0.72091901 0.00163198 0.00789397 0.00123238\n",
      " 0.10762747 0.02090927 0.02121457 0.0014196  0.00910754 0.00689645\n",
      " 0.49980408 0.17445913 0.68018824 0.71930569 0.17004874 0.11912379\n",
      " 0.0369556  0.32921055 0.05473032 0.53565198 0.28146088 0.27520147\n",
      " 0.02535543 0.18738845 0.17230204 0.12337549 0.6803087  0.70256519\n",
      " 0.88512534 0.31485835 0.30460471 0.42438936 0.89897364 0.25613296\n",
      " 0.03530577 0.16395941 0.12776028 0.2277123  0.01999167 0.00992769\n",
      " 0.36726937 0.0075264  0.04164213 0.80857623 0.14047533 0.01775667\n",
      " 0.00437591 0.09325466 0.02202529 0.02288492 0.05589403 0.46201175\n",
      " 0.0170787  0.00740812 0.00644626 0.07971685 0.31272691 0.04645406\n",
      " 0.01289521 0.02364554 0.01326345 0.01705538 0.3982895  0.01814727\n",
      " 0.1221264  0.0306411 ]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 5 [0/106 (0%)]\tTrain Loss: 0.038419\n",
      "Train Epoch: 5 [10/106 (9%)]\tTrain Loss: 0.025434\n",
      "Train Epoch: 5 [20/106 (19%)]\tTrain Loss: 0.011389\n",
      "Train Epoch: 5 [30/106 (28%)]\tTrain Loss: 0.035217\n",
      "Train Epoch: 5 [40/106 (38%)]\tTrain Loss: 0.011455\n",
      "Train Epoch: 5 [50/106 (47%)]\tTrain Loss: 0.018109\n",
      "Train Epoch: 5 [60/106 (57%)]\tTrain Loss: 0.039668\n",
      "Train Epoch: 5 [70/106 (66%)]\tTrain Loss: 0.024377\n",
      "Train Epoch: 5 [80/106 (75%)]\tTrain Loss: 0.015367\n",
      "Train Epoch: 5 [90/106 (85%)]\tTrain Loss: 0.021503\n",
      "Train Epoch: 5 [100/106 (94%)]\tTrain Loss: 0.024918\n",
      "\n",
      "Train set: Average loss: 0.0008, Accuracy: 349/424 (82%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.03124913 0.09690955 0.01202362 0.00490453 0.00777649 0.00542185\n",
      " 0.08945812 0.00609904 0.01662862 0.01159228 0.00556575 0.00407735\n",
      " 0.0015088  0.01209057 0.14117429 0.00251564 0.00371294 0.06632481\n",
      " 0.00437823 0.05038116 0.03203165 0.00637462 0.08050051 0.10932522\n",
      " 0.00411412 0.12199369 0.89076412 0.0222617  0.00208038 0.0057118\n",
      " 0.04519295 0.07243317 0.11341724 0.00123632 0.00196595 0.00215901\n",
      " 0.00400396 0.00653869 0.01665842 0.00690196 0.00810436 0.00905888\n",
      " 0.01646719 0.00481015 0.00622382 0.00490474 0.03693867 0.09273418\n",
      " 0.44742429 0.0156916  0.47165203 0.00115311 0.00701604 0.00116185\n",
      " 0.08085398 0.02134233 0.01215644 0.00165847 0.00547226 0.00552795\n",
      " 0.18562172 0.10369441 0.31225544 0.47278553 0.07850266 0.12279512\n",
      " 0.03164938 0.20604996 0.03263152 0.21488281 0.11911494 0.09313746\n",
      " 0.01442053 0.08578135 0.11311211 0.09093076 0.66341525 0.70695192\n",
      " 0.79156137 0.11152003 0.22287366 0.33918938 0.79560518 0.17571522\n",
      " 0.02557978 0.05833589 0.06666994 0.10352764 0.01873367 0.01048862\n",
      " 0.23628251 0.00559192 0.01958833 0.83485508 0.22507231 0.01306111\n",
      " 0.00412781 0.06257324 0.01494628 0.06873504 0.04011468 0.28327391\n",
      " 0.01202866 0.0096985  0.00384106 0.10394033 0.11870674 0.04575307\n",
      " 0.01258126 0.02193675 0.01650556 0.03954731 0.3604455  0.01776019\n",
      " 0.07446607 0.02644189]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [0/106 (0%)]\tTrain Loss: 0.012671\n",
      "Train Epoch: 6 [10/106 (9%)]\tTrain Loss: 0.015499\n",
      "Train Epoch: 6 [20/106 (19%)]\tTrain Loss: 0.018147\n",
      "Train Epoch: 6 [30/106 (28%)]\tTrain Loss: 0.023679\n",
      "Train Epoch: 6 [40/106 (38%)]\tTrain Loss: 0.035353\n",
      "Train Epoch: 6 [50/106 (47%)]\tTrain Loss: 0.032411\n",
      "Train Epoch: 6 [60/106 (57%)]\tTrain Loss: 0.020572\n",
      "Train Epoch: 6 [70/106 (66%)]\tTrain Loss: 0.043394\n",
      "Train Epoch: 6 [80/106 (75%)]\tTrain Loss: 0.024900\n",
      "Train Epoch: 6 [90/106 (85%)]\tTrain Loss: 0.187018\n",
      "Train Epoch: 6 [100/106 (94%)]\tTrain Loss: 0.020190\n",
      "\n",
      "Train set: Average loss: 0.0003, Accuracy: 372/424 (88%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02052706 0.06914397 0.01180914 0.00416302 0.00765635 0.00544109\n",
      " 0.06398416 0.00617542 0.01118503 0.00953313 0.00469585 0.00421958\n",
      " 0.00189965 0.01504468 0.0620529  0.00319187 0.00371577 0.02082896\n",
      " 0.00301243 0.03211522 0.02299796 0.00432563 0.02555366 0.07597905\n",
      " 0.00263587 0.03203452 0.88333106 0.00983324 0.00261921 0.00548658\n",
      " 0.02836635 0.03316378 0.02663055 0.00159932 0.00217552 0.0021836\n",
      " 0.00304804 0.00485992 0.00969489 0.00612106 0.00728993 0.00934303\n",
      " 0.01477374 0.00464633 0.00508728 0.00443027 0.04019019 0.09226155\n",
      " 0.41357362 0.01520516 0.47504732 0.00177693 0.00518783 0.00152583\n",
      " 0.06657135 0.00774358 0.0063585  0.00177526 0.00436764 0.00409153\n",
      " 0.16566637 0.08544125 0.32654631 0.42867088 0.04248511 0.02402052\n",
      " 0.01210982 0.18153518 0.02747233 0.13913091 0.1738838  0.14140838\n",
      " 0.01384892 0.09770504 0.09229126 0.04132567 0.66489387 0.67922306\n",
      " 0.72012228 0.07497981 0.15338135 0.22060798 0.77896929 0.06097957\n",
      " 0.01265033 0.04331377 0.05840623 0.09599333 0.01324896 0.00610478\n",
      " 0.13273883 0.00424244 0.00814805 0.63042665 0.054495   0.01005544\n",
      " 0.00259235 0.03605579 0.01741356 0.02610453 0.04373537 0.31634337\n",
      " 0.01089973 0.00706661 0.0043625  0.04656878 0.0954565  0.03679663\n",
      " 0.00909418 0.01565528 0.0116977  0.01983327 0.10913052 0.01056611\n",
      " 0.07494929 0.02025738]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 7 [0/106 (0%)]\tTrain Loss: 0.010619\n",
      "Train Epoch: 7 [10/106 (9%)]\tTrain Loss: 0.007014\n",
      "Train Epoch: 7 [20/106 (19%)]\tTrain Loss: 0.150833\n",
      "Train Epoch: 7 [30/106 (28%)]\tTrain Loss: 0.035812\n",
      "Train Epoch: 7 [40/106 (38%)]\tTrain Loss: 0.174845\n",
      "Train Epoch: 7 [50/106 (47%)]\tTrain Loss: 0.016622\n",
      "Train Epoch: 7 [60/106 (57%)]\tTrain Loss: 0.015505\n",
      "Train Epoch: 7 [70/106 (66%)]\tTrain Loss: 0.101407\n",
      "Train Epoch: 7 [80/106 (75%)]\tTrain Loss: 0.035059\n",
      "Train Epoch: 7 [90/106 (85%)]\tTrain Loss: 0.090471\n",
      "Train Epoch: 7 [100/106 (94%)]\tTrain Loss: 0.007886\n",
      "\n",
      "Train set: Average loss: 0.0006, Accuracy: 370/424 (87%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02184922 0.07532318 0.01006928 0.00315092 0.00624933 0.00408107\n",
      " 0.05895339 0.00481974 0.00829947 0.00604612 0.00373018 0.00292736\n",
      " 0.00139151 0.00670148 0.04553434 0.00270982 0.00215092 0.01539423\n",
      " 0.00256886 0.02390322 0.0155483  0.00259037 0.016111   0.04740925\n",
      " 0.00196325 0.02405919 0.83590162 0.00576934 0.00185826 0.00375725\n",
      " 0.02229563 0.03314979 0.02118378 0.00106007 0.00144635 0.00159558\n",
      " 0.0020984  0.00388455 0.00568573 0.00411785 0.00537004 0.00541039\n",
      " 0.01155372 0.00421855 0.00497369 0.00428529 0.03862001 0.08416317\n",
      " 0.38603574 0.01312524 0.45071468 0.00123565 0.00373597 0.00113199\n",
      " 0.03790867 0.00547062 0.00427589 0.00119974 0.00304107 0.00342022\n",
      " 0.14187928 0.06345481 0.2044749  0.36306837 0.0435343  0.01713547\n",
      " 0.00750724 0.12643993 0.02260963 0.20456536 0.05781998 0.04968246\n",
      " 0.01137586 0.06651993 0.07277411 0.02000879 0.62793374 0.67392766\n",
      " 0.68336302 0.04628653 0.08509706 0.1288645  0.68195117 0.0247622\n",
      " 0.00595759 0.02110003 0.05315607 0.0889437  0.00740422 0.0031658\n",
      " 0.09779108 0.00371153 0.00695626 0.59861296 0.04102616 0.0064133\n",
      " 0.00201323 0.01586909 0.00683942 0.01175709 0.0172157  0.12265706\n",
      " 0.00684005 0.00370265 0.00432022 0.02541467 0.13769765 0.02760323\n",
      " 0.00688567 0.01110988 0.0078557  0.01503688 0.06917698 0.00803288\n",
      " 0.0189244  0.00966776]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 8 [0/106 (0%)]\tTrain Loss: 0.208088\n",
      "Train Epoch: 8 [10/106 (9%)]\tTrain Loss: 0.013953\n",
      "Train Epoch: 8 [20/106 (19%)]\tTrain Loss: 0.032607\n",
      "Train Epoch: 8 [30/106 (28%)]\tTrain Loss: 0.006453\n",
      "Train Epoch: 8 [40/106 (38%)]\tTrain Loss: 0.017498\n",
      "Train Epoch: 8 [50/106 (47%)]\tTrain Loss: 0.009125\n",
      "Train Epoch: 8 [60/106 (57%)]\tTrain Loss: 0.091675\n",
      "Train Epoch: 8 [70/106 (66%)]\tTrain Loss: 0.047258\n",
      "Train Epoch: 8 [80/106 (75%)]\tTrain Loss: 0.006705\n",
      "Train Epoch: 8 [90/106 (85%)]\tTrain Loss: 0.028819\n",
      "Train Epoch: 8 [100/106 (94%)]\tTrain Loss: 0.081407\n",
      "\n",
      "Train set: Average loss: 0.0005, Accuracy: 378/424 (89%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.21855885e-02 5.45777753e-02 7.35056354e-03 1.92445714e-03\n",
      " 4.01405664e-03 2.48369365e-03 4.66144942e-02 3.45242606e-03\n",
      " 6.36779610e-03 4.91766632e-03 2.46721646e-03 1.98053359e-03\n",
      " 8.66400253e-04 7.82966893e-03 4.19617929e-02 1.60103594e-03\n",
      " 1.61013729e-03 6.50537899e-03 1.34967209e-03 1.76502597e-02\n",
      " 1.25561599e-02 1.80977688e-03 1.52326263e-02 3.54004353e-02\n",
      " 1.29880314e-03 1.98896304e-02 8.65606487e-01 5.20025985e-03\n",
      " 1.27175567e-03 2.52325740e-03 1.58839691e-02 1.45502454e-02\n",
      " 9.81457904e-03 6.54854521e-04 9.45016451e-04 1.00880663e-03\n",
      " 1.33943337e-03 1.75646530e-03 3.48647614e-03 2.62896926e-03\n",
      " 2.90461979e-03 3.71663412e-03 5.86059131e-03 2.03205040e-03\n",
      " 2.83876155e-03 2.56188912e-03 2.77862065e-02 6.84314966e-02\n",
      " 3.25282067e-01 9.90464166e-03 4.10536855e-01 6.99512486e-04\n",
      " 2.43388978e-03 6.99698168e-04 5.37128821e-02 3.89589369e-03\n",
      " 3.40692280e-03 8.46976414e-04 1.72392314e-03 2.03619828e-03\n",
      " 1.08506680e-01 4.88299541e-02 2.48950467e-01 3.56295645e-01\n",
      " 2.93966830e-02 1.15733361e-02 5.06613078e-03 9.23493877e-02\n",
      " 1.57073792e-02 1.46546438e-01 7.05601797e-02 6.35442734e-02\n",
      " 7.65802898e-03 5.83410338e-02 7.61161223e-02 1.87459644e-02\n",
      " 6.45874619e-01 6.75976574e-01 6.89386487e-01 3.30448635e-02\n",
      " 1.16059445e-01 1.35598466e-01 7.46214986e-01 2.35604011e-02\n",
      " 5.21575008e-03 1.70771927e-02 3.17616612e-02 6.54243082e-02\n",
      " 6.86491886e-03 3.13350535e-03 8.35200548e-02 1.73905527e-03\n",
      " 3.39211663e-03 7.08875895e-01 3.59890796e-02 5.39406808e-03\n",
      " 1.25933252e-03 1.85239855e-02 7.44369254e-03 1.13002555e-02\n",
      " 1.47434091e-02 1.62790805e-01 4.16773604e-03 3.01961554e-03\n",
      " 2.44243606e-03 2.93015968e-02 8.37921500e-02 2.15727501e-02\n",
      " 4.39846050e-03 8.30635428e-03 5.68593899e-03 9.61478055e-03\n",
      " 8.56668875e-02 5.75171318e-03 3.96271236e-02 7.03086471e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [0/106 (0%)]\tTrain Loss: 0.150686\n",
      "Train Epoch: 9 [10/106 (9%)]\tTrain Loss: 0.152149\n",
      "Train Epoch: 9 [20/106 (19%)]\tTrain Loss: 0.025796\n",
      "Train Epoch: 9 [30/106 (28%)]\tTrain Loss: 0.007444\n",
      "Train Epoch: 9 [40/106 (38%)]\tTrain Loss: 0.023224\n",
      "Train Epoch: 9 [50/106 (47%)]\tTrain Loss: 0.009139\n",
      "Train Epoch: 9 [60/106 (57%)]\tTrain Loss: 0.021919\n",
      "Train Epoch: 9 [70/106 (66%)]\tTrain Loss: 0.013991\n",
      "Train Epoch: 9 [80/106 (75%)]\tTrain Loss: 0.006521\n",
      "Train Epoch: 9 [90/106 (85%)]\tTrain Loss: 0.007248\n",
      "Train Epoch: 9 [100/106 (94%)]\tTrain Loss: 0.018593\n",
      "\n",
      "Train set: Average loss: 0.0035, Accuracy: 373/424 (88%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.79718807e-02 6.65688589e-02 8.29923991e-03 2.51139305e-03\n",
      " 5.59650827e-03 3.23501579e-03 5.37741370e-02 4.19201842e-03\n",
      " 8.12067464e-03 5.15786326e-03 3.39905848e-03 2.40867911e-03\n",
      " 9.29638161e-04 9.85411648e-03 6.47377446e-02 1.98144442e-03\n",
      " 1.79062784e-03 1.06068412e-02 1.76150491e-03 2.88721882e-02\n",
      " 1.81620307e-02 2.32560863e-03 2.17975434e-02 5.88335320e-02\n",
      " 1.61312264e-03 2.79868226e-02 8.83059263e-01 6.60028076e-03\n",
      " 1.46517553e-03 3.18079419e-03 1.99255086e-02 2.61535086e-02\n",
      " 1.54890111e-02 7.58664624e-04 1.05118961e-03 1.16775918e-03\n",
      " 1.69197400e-03 2.54989346e-03 4.81928233e-03 3.64485430e-03\n",
      " 4.25955467e-03 5.48616471e-03 9.00946651e-03 2.87895836e-03\n",
      " 4.04725410e-03 3.45204351e-03 3.63859423e-02 8.59451517e-02\n",
      " 4.07616854e-01 1.33555382e-02 5.80951393e-01 9.28576337e-04\n",
      " 3.08162766e-03 8.75991653e-04 4.31863256e-02 4.86734835e-03\n",
      " 4.79271449e-03 9.42090293e-04 2.19868892e-03 2.51211948e-03\n",
      " 1.35351866e-01 5.93150966e-02 2.62103885e-01 3.97657692e-01\n",
      " 4.62509803e-02 2.15674862e-02 7.42651336e-03 1.31684661e-01\n",
      " 2.04036906e-02 1.99134439e-01 1.12206116e-01 8.72838050e-02\n",
      " 1.10422000e-02 8.68270993e-02 9.05886739e-02 3.24596912e-02\n",
      " 6.51611924e-01 6.92994237e-01 7.11476386e-01 5.40934131e-02\n",
      " 1.57921165e-01 2.09242135e-01 8.01131368e-01 3.83462869e-02\n",
      " 8.05983413e-03 2.99285688e-02 4.76104170e-02 9.25567672e-02\n",
      " 7.89795350e-03 3.99912847e-03 9.94866565e-02 2.47416669e-03\n",
      " 5.45660406e-03 6.77743793e-01 4.72736545e-02 6.27010455e-03\n",
      " 1.45558815e-03 2.56558210e-02 9.11899097e-03 1.49596017e-02\n",
      " 2.20086109e-02 2.53780127e-01 5.67694707e-03 4.33860486e-03\n",
      " 3.27596418e-03 4.34657298e-02 1.07945994e-01 2.96150576e-02\n",
      " 5.83944051e-03 9.37769841e-03 8.80906451e-03 1.46757159e-02\n",
      " 1.03238404e-01 6.02649152e-03 4.76837233e-02 1.40275992e-02]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 10 [0/106 (0%)]\tTrain Loss: 0.049731\n",
      "Train Epoch: 10 [10/106 (9%)]\tTrain Loss: 0.007104\n",
      "Train Epoch: 10 [20/106 (19%)]\tTrain Loss: 0.031215\n",
      "Train Epoch: 10 [30/106 (28%)]\tTrain Loss: 0.010720\n",
      "Train Epoch: 10 [40/106 (38%)]\tTrain Loss: 0.017569\n",
      "Train Epoch: 10 [50/106 (47%)]\tTrain Loss: 0.182715\n",
      "Train Epoch: 10 [60/106 (57%)]\tTrain Loss: 0.191000\n",
      "Train Epoch: 10 [70/106 (66%)]\tTrain Loss: 0.028871\n",
      "Train Epoch: 10 [80/106 (75%)]\tTrain Loss: 0.157856\n",
      "Train Epoch: 10 [90/106 (85%)]\tTrain Loss: 0.008531\n",
      "Train Epoch: 10 [100/106 (94%)]\tTrain Loss: 0.026127\n",
      "\n",
      "Train set: Average loss: 0.0002, Accuracy: 363/424 (86%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.54565563e-02 6.34333342e-02 7.24724075e-03 2.39386200e-03\n",
      " 4.85127605e-03 2.63447501e-03 4.63485010e-02 3.68137588e-03\n",
      " 5.69594372e-03 4.16286243e-03 2.25415826e-03 2.00780644e-03\n",
      " 7.23530364e-04 5.01177693e-03 4.66838144e-02 1.84359006e-03\n",
      " 1.43743842e-03 1.02982195e-02 1.45304541e-03 1.14383688e-02\n",
      " 8.80408194e-03 1.53121166e-03 1.07922610e-02 2.12834626e-02\n",
      " 1.21253345e-03 1.47894081e-02 8.08151007e-01 3.38205812e-03\n",
      " 1.11950946e-03 2.42174650e-03 1.37960715e-02 2.96701919e-02\n",
      " 2.29882691e-02 6.26632071e-04 8.77377985e-04 1.00467762e-03\n",
      " 1.19801762e-03 2.52956850e-03 3.93277034e-03 2.96053104e-03\n",
      " 4.10305150e-03 3.66486446e-03 8.61055590e-03 3.05517949e-03\n",
      " 3.65119753e-03 2.93720886e-03 3.23145837e-02 6.69032931e-02\n",
      " 2.89145917e-01 6.00220170e-03 3.52944791e-01 8.29871453e-04\n",
      " 2.56231590e-03 8.02047318e-04 1.77654307e-02 4.28662170e-03\n",
      " 2.76555144e-03 8.05221498e-04 2.07221089e-03 2.39078002e-03\n",
      " 1.30013123e-01 5.83377145e-02 1.76801175e-01 2.96674341e-01\n",
      " 2.81266589e-02 1.29400454e-02 5.57622733e-03 9.38265920e-02\n",
      " 1.62238088e-02 1.31607696e-01 6.49249777e-02 5.42774908e-02\n",
      " 8.50267988e-03 3.99038717e-02 4.42559570e-02 2.16163788e-02\n",
      " 6.13584578e-01 6.73179924e-01 5.92726648e-01 4.42511141e-02\n",
      " 7.38689825e-02 1.05200484e-01 6.74258947e-01 2.53139995e-02\n",
      " 4.61788476e-03 1.73446182e-02 3.12045701e-02 5.74457906e-02\n",
      " 4.88564186e-03 2.63841613e-03 7.55976811e-02 2.76578707e-03\n",
      " 5.80885680e-03 4.34913486e-01 3.95073742e-02 4.56752488e-03\n",
      " 1.27217080e-03 1.32340118e-02 5.39385295e-03 9.27333813e-03\n",
      " 1.55298114e-02 1.43085897e-01 5.97502943e-03 3.46848276e-03\n",
      " 2.89409771e-03 2.78388988e-02 7.90539607e-02 1.38987247e-02\n",
      " 4.38519800e-03 7.17602856e-03 4.60681599e-03 1.07057299e-02\n",
      " 6.76131546e-02 4.65651136e-03 2.44122893e-02 8.79617129e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 5 TN= 59 FN= 51 FP= 1\n",
      "TP+FP 6\n",
      "precision 0.8333333333333334\n",
      "recall 0.08928571428571429\n",
      "F1 0.16129032258064516\n",
      "acc 0.5517241379310345\n",
      "AUCp 0.5363095238095238\n",
      "AUC 0.7568452380952381\n",
      "\n",
      " The epoch is 10, average recall: 0.0893, average precision: 0.8333,average F1: 0.1613, average accuracy: 0.5517, average AUC: 0.7568\n",
      "Train Epoch: 11 [0/106 (0%)]\tTrain Loss: 0.009257\n",
      "Train Epoch: 11 [10/106 (9%)]\tTrain Loss: 0.035853\n",
      "Train Epoch: 11 [20/106 (19%)]\tTrain Loss: 0.047236\n",
      "Train Epoch: 11 [30/106 (28%)]\tTrain Loss: 0.007674\n",
      "Train Epoch: 11 [40/106 (38%)]\tTrain Loss: 0.036552\n",
      "Train Epoch: 11 [50/106 (47%)]\tTrain Loss: 0.021168\n",
      "Train Epoch: 11 [60/106 (57%)]\tTrain Loss: 0.115486\n",
      "Train Epoch: 11 [70/106 (66%)]\tTrain Loss: 0.034252\n",
      "Train Epoch: 11 [80/106 (75%)]\tTrain Loss: 0.105053\n",
      "Train Epoch: 11 [90/106 (85%)]\tTrain Loss: 0.013847\n",
      "Train Epoch: 11 [100/106 (94%)]\tTrain Loss: 0.157510\n",
      "\n",
      "Train set: Average loss: 0.0034, Accuracy: 365/424 (86%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.10343033e-02 4.27440219e-02 4.63228580e-03 1.47128198e-03\n",
      " 3.22713470e-03 1.68830599e-03 3.17728557e-02 2.35817139e-03\n",
      " 4.08857642e-03 2.86734663e-03 1.54848979e-03 1.18345337e-03\n",
      " 4.46646241e-04 3.57693061e-03 2.44182628e-02 1.02921342e-03\n",
      " 9.50212358e-04 5.70289930e-03 9.39087244e-04 9.07678530e-03\n",
      " 6.33505126e-03 8.49333010e-04 8.09170865e-03 2.31185984e-02\n",
      " 7.00236822e-04 9.56820697e-03 8.50371599e-01 1.96766155e-03\n",
      " 6.76939089e-04 1.61904201e-03 1.17432838e-02 1.74044762e-02\n",
      " 9.45010316e-03 3.83363978e-04 4.77722002e-04 5.73464611e-04\n",
      " 6.89722539e-04 1.19108264e-03 2.11071805e-03 1.62768085e-03\n",
      " 2.31147464e-03 2.17035669e-03 5.11856424e-03 1.59596314e-03\n",
      " 1.93828624e-03 1.70313509e-03 2.16054413e-02 5.12554087e-02\n",
      " 3.51247042e-01 5.11139352e-03 5.08262157e-01 4.67270584e-04\n",
      " 1.41566165e-03 4.40667995e-04 1.68656241e-02 2.18529557e-03\n",
      " 1.66294305e-03 3.96758376e-04 1.30133471e-03 1.23794796e-03\n",
      " 1.20900452e-01 4.55941819e-02 1.84418768e-01 3.05894762e-01\n",
      " 2.67494097e-02 8.00971221e-03 3.70024471e-03 8.65400806e-02\n",
      " 1.15979956e-02 1.65236726e-01 6.25804290e-02 4.25128974e-02\n",
      " 7.37245986e-03 6.36515468e-02 4.48688492e-02 9.37528908e-03\n",
      " 6.19315028e-01 6.73228383e-01 6.18399203e-01 3.06356475e-02\n",
      " 6.38508722e-02 9.45657715e-02 7.42953300e-01 1.59437917e-02\n",
      " 2.72236741e-03 1.25616761e-02 2.09854916e-02 3.64611521e-02\n",
      " 2.85560801e-03 1.55736622e-03 6.99971989e-02 1.64169143e-03\n",
      " 2.72979029e-03 3.98871005e-01 2.98880339e-02 3.12601635e-03\n",
      " 7.16431707e-04 1.05771180e-02 3.61427665e-03 5.67295449e-03\n",
      " 1.04938634e-02 1.07098557e-01 2.62395572e-03 2.00527534e-03\n",
      " 2.07423721e-03 1.90151855e-02 6.43432215e-02 1.04622273e-02\n",
      " 2.84061651e-03 6.21361472e-03 3.72711569e-03 7.16860313e-03\n",
      " 5.25175184e-02 3.20515898e-03 1.44104753e-02 4.04816912e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [0/106 (0%)]\tTrain Loss: 0.014706\n",
      "Train Epoch: 12 [10/106 (9%)]\tTrain Loss: 0.011806\n",
      "Train Epoch: 12 [20/106 (19%)]\tTrain Loss: 0.040651\n",
      "Train Epoch: 12 [30/106 (28%)]\tTrain Loss: 0.136017\n",
      "Train Epoch: 12 [40/106 (38%)]\tTrain Loss: 0.014514\n",
      "Train Epoch: 12 [50/106 (47%)]\tTrain Loss: 0.048878\n",
      "Train Epoch: 12 [60/106 (57%)]\tTrain Loss: 0.015711\n",
      "Train Epoch: 12 [70/106 (66%)]\tTrain Loss: 0.042907\n",
      "Train Epoch: 12 [80/106 (75%)]\tTrain Loss: 0.029608\n",
      "Train Epoch: 12 [90/106 (85%)]\tTrain Loss: 0.005996\n",
      "Train Epoch: 12 [100/106 (94%)]\tTrain Loss: 0.011243\n",
      "\n",
      "Train set: Average loss: 0.0017, Accuracy: 378/424 (89%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.08781606e-02 3.33578065e-02 4.07602964e-03 2.78646452e-03\n",
      " 4.98107402e-03 2.04464188e-03 2.28591878e-02 3.92549252e-03\n",
      " 5.41190291e-03 6.21705409e-03 3.27754207e-03 1.99920777e-03\n",
      " 7.33509485e-04 3.54348286e-03 2.54287478e-02 1.54509337e-03\n",
      " 1.62785116e-03 9.34021827e-03 1.28973601e-03 1.10655874e-02\n",
      " 8.65989551e-03 1.57286355e-03 1.14666354e-02 2.13825013e-02\n",
      " 1.02353969e-03 1.23115378e-02 9.10601020e-01 5.22822654e-03\n",
      " 9.85092833e-04 2.57521495e-03 2.66096201e-02 3.83707695e-02\n",
      " 2.15981659e-02 5.94028505e-04 7.86747783e-04 8.48113385e-04\n",
      " 9.93507681e-04 2.17430713e-03 5.61870448e-03 2.45956727e-03\n",
      " 3.21342051e-03 3.34182172e-03 7.87287019e-03 2.36235722e-03\n",
      " 3.86916683e-03 3.55473091e-03 2.71673892e-02 7.18348920e-02\n",
      " 4.42337006e-01 6.21917890e-03 5.47221661e-01 1.03073742e-03\n",
      " 2.54134531e-03 7.68961327e-04 1.36459200e-02 3.28349136e-03\n",
      " 6.91782543e-03 7.51663640e-04 2.21895264e-03 2.20533623e-03\n",
      " 1.67712763e-01 6.76387250e-02 3.23183149e-01 4.78828013e-01\n",
      " 6.66231662e-02 6.84768111e-02 1.27299214e-02 1.62762836e-01\n",
      " 1.72007866e-02 2.91971505e-01 1.20090626e-01 8.07952434e-02\n",
      " 1.69862006e-02 1.66890696e-01 1.37047365e-01 4.14430611e-02\n",
      " 6.27541602e-01 6.61917746e-01 5.70087373e-01 4.63330448e-02\n",
      " 1.75384119e-01 3.09823662e-01 8.32560837e-01 1.47264019e-01\n",
      " 8.63191951e-03 3.34894396e-02 3.98491472e-02 5.98327443e-02\n",
      " 5.40348794e-03 3.03600472e-03 1.06839202e-01 3.07899178e-03\n",
      " 4.63408418e-03 4.94205952e-01 5.92478737e-02 3.86988535e-03\n",
      " 1.49335130e-03 3.45546678e-02 5.12975547e-03 1.30718425e-02\n",
      " 2.05817781e-02 4.02347028e-01 3.50284739e-03 8.76419991e-03\n",
      " 3.19948164e-03 6.31493181e-02 1.03582896e-01 1.12695489e-02\n",
      " 3.54638021e-03 7.43177719e-03 5.94295934e-03 9.44748800e-03\n",
      " 7.31616095e-02 4.21347888e-03 4.92643416e-02 9.77154076e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 13 [0/106 (0%)]\tTrain Loss: 0.017170\n",
      "Train Epoch: 13 [10/106 (9%)]\tTrain Loss: 0.094859\n",
      "Train Epoch: 13 [20/106 (19%)]\tTrain Loss: 0.065838\n",
      "Train Epoch: 13 [30/106 (28%)]\tTrain Loss: 0.029725\n",
      "Train Epoch: 13 [40/106 (38%)]\tTrain Loss: 0.020479\n",
      "Train Epoch: 13 [50/106 (47%)]\tTrain Loss: 0.004835\n",
      "Train Epoch: 13 [60/106 (57%)]\tTrain Loss: 0.038002\n",
      "Train Epoch: 13 [70/106 (66%)]\tTrain Loss: 0.015083\n",
      "Train Epoch: 13 [80/106 (75%)]\tTrain Loss: 0.009418\n",
      "Train Epoch: 13 [90/106 (85%)]\tTrain Loss: 0.206409\n",
      "Train Epoch: 13 [100/106 (94%)]\tTrain Loss: 0.044572\n",
      "\n",
      "Train set: Average loss: 0.0003, Accuracy: 366/424 (86%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [4.15176200e-03 1.26049444e-02 1.35504606e-03 3.03032342e-04\n",
      " 1.09837682e-03 5.16593223e-04 1.00087970e-02 6.84255152e-04\n",
      " 1.00165757e-03 8.42542329e-04 1.21405663e-03 2.87725386e-04\n",
      " 2.46416283e-04 1.38122251e-03 8.99426825e-03 3.38273065e-04\n",
      " 4.94256325e-04 1.08632620e-03 3.54599732e-04 4.01089340e-03\n",
      " 1.66603492e-03 3.52602074e-04 4.39568097e-03 9.38980840e-03\n",
      " 3.27197253e-04 6.02984335e-03 9.10230637e-01 1.46451581e-03\n",
      " 2.95900129e-04 8.37692351e-04 6.26399880e-03 9.80812311e-03\n",
      " 1.79407292e-03 1.72714936e-04 1.99539165e-04 2.29747966e-04\n",
      " 3.54101823e-04 3.37093195e-04 7.21858465e-04 4.47854691e-04\n",
      " 5.00214985e-04 6.62327337e-04 8.35762068e-04 4.99201124e-04\n",
      " 1.15397561e-03 7.82648975e-04 6.03705272e-03 1.78263653e-02\n",
      " 9.15429741e-02 1.51348021e-03 1.03058934e-01 2.33124083e-04\n",
      " 5.47587348e-04 1.88301448e-04 9.34940856e-03 1.47176476e-03\n",
      " 8.57072533e-04 2.00087670e-04 3.37886624e-04 4.91212122e-04\n",
      " 3.09913624e-02 1.14659807e-02 1.54898211e-01 2.12401718e-01\n",
      " 2.69258581e-02 1.04957847e-02 1.20131101e-03 7.48866051e-02\n",
      " 4.50370414e-03 1.27218455e-01 1.69121958e-02 6.45453949e-03\n",
      " 4.28375136e-03 1.46520555e-01 9.24202800e-02 6.58271415e-03\n",
      " 4.92088437e-01 4.93045062e-01 2.96123564e-01 7.28573091e-03\n",
      " 3.61433625e-02 2.58370750e-02 6.58211112e-01 1.51687609e-02\n",
      " 9.86538129e-04 5.40834619e-03 7.04177190e-03 1.26523683e-02\n",
      " 7.74161424e-04 7.98537745e-04 4.60349284e-02 6.29320624e-04\n",
      " 4.96807450e-04 5.46795189e-01 4.14242372e-02 1.12859008e-03\n",
      " 3.12429300e-04 7.32985279e-03 1.34151697e-03 7.53668696e-03\n",
      " 6.61088806e-03 4.58087996e-02 6.06977439e-04 1.44703116e-03\n",
      " 8.58885003e-04 6.48696302e-03 3.53029035e-02 8.14331323e-03\n",
      " 9.97752417e-04 3.61512857e-03 2.64974311e-03 1.77790050e-03\n",
      " 4.11520004e-02 1.40432210e-03 7.77621986e-03 1.20785180e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 14 [0/106 (0%)]\tTrain Loss: 0.082068\n",
      "Train Epoch: 14 [10/106 (9%)]\tTrain Loss: 0.011170\n",
      "Train Epoch: 14 [20/106 (19%)]\tTrain Loss: 0.169657\n",
      "Train Epoch: 14 [30/106 (28%)]\tTrain Loss: 0.004805\n",
      "Train Epoch: 14 [40/106 (38%)]\tTrain Loss: 0.016400\n",
      "Train Epoch: 14 [50/106 (47%)]\tTrain Loss: 0.053449\n",
      "Train Epoch: 14 [60/106 (57%)]\tTrain Loss: 0.003785\n",
      "Train Epoch: 14 [70/106 (66%)]\tTrain Loss: 0.004247\n",
      "Train Epoch: 14 [80/106 (75%)]\tTrain Loss: 0.027378\n",
      "Train Epoch: 14 [90/106 (85%)]\tTrain Loss: 0.147752\n",
      "Train Epoch: 14 [100/106 (94%)]\tTrain Loss: 0.208821\n",
      "\n",
      "Train set: Average loss: 0.0002, Accuracy: 371/424 (88%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02663428 0.02212553 0.00224124 0.00170495 0.00511648 0.00380955\n",
      " 0.01144513 0.0029504  0.00613153 0.00426406 0.005491   0.00200022\n",
      " 0.00173116 0.00242231 0.01449016 0.00205709 0.00192945 0.00206144\n",
      " 0.00203264 0.00392323 0.00254865 0.00121988 0.0076822  0.0107078\n",
      " 0.00124284 0.01870092 0.81693316 0.00411641 0.00199695 0.00344873\n",
      " 0.01730944 0.03693685 0.01704997 0.00155438 0.00182568 0.00191239\n",
      " 0.00162555 0.00200497 0.00163415 0.00133631 0.00128087 0.00183425\n",
      " 0.00458179 0.00391238 0.00533819 0.00571139 0.03021932 0.03613439\n",
      " 0.14077656 0.00327363 0.15734485 0.00147645 0.00458611 0.00177357\n",
      " 0.00416152 0.00931587 0.00311406 0.00128542 0.00290472 0.00442836\n",
      " 0.08441585 0.02668394 0.0753247  0.15674128 0.03903469 0.00701823\n",
      " 0.00460328 0.0467585  0.00785477 0.16095133 0.01148806 0.00737193\n",
      " 0.01303137 0.11927179 0.07118665 0.0243958  0.38953331 0.44515887\n",
      " 0.31616333 0.01327692 0.04550723 0.07575745 0.62480026 0.01777173\n",
      " 0.0019563  0.01298142 0.04706722 0.07010392 0.00199662 0.00189888\n",
      " 0.07824808 0.0038848  0.00396603 0.26054287 0.18029456 0.00507494\n",
      " 0.0018829  0.00369939 0.00343659 0.00861425 0.00437256 0.01868007\n",
      " 0.00183933 0.00677178 0.00736942 0.02965593 0.07339107 0.0083822\n",
      " 0.0025986  0.01164916 0.00303765 0.0057656  0.09595563 0.00409546\n",
      " 0.00351329 0.00209618]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [0/106 (0%)]\tTrain Loss: 0.007200\n",
      "Train Epoch: 15 [10/106 (9%)]\tTrain Loss: 0.004357\n",
      "Train Epoch: 15 [20/106 (19%)]\tTrain Loss: 0.011270\n",
      "Train Epoch: 15 [30/106 (28%)]\tTrain Loss: 0.093061\n",
      "Train Epoch: 15 [40/106 (38%)]\tTrain Loss: 0.085651\n",
      "Train Epoch: 15 [50/106 (47%)]\tTrain Loss: 0.004588\n",
      "Train Epoch: 15 [60/106 (57%)]\tTrain Loss: 0.100490\n",
      "Train Epoch: 15 [70/106 (66%)]\tTrain Loss: 0.007020\n",
      "Train Epoch: 15 [80/106 (75%)]\tTrain Loss: 0.010208\n",
      "Train Epoch: 15 [90/106 (85%)]\tTrain Loss: 0.007579\n",
      "Train Epoch: 15 [100/106 (94%)]\tTrain Loss: 0.005337\n",
      "\n",
      "Train set: Average loss: 0.0002, Accuracy: 369/424 (87%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [5.53413574e-03 3.19185518e-02 2.32071686e-03 1.18882698e-03\n",
      " 3.83078493e-03 1.46054430e-03 3.77488174e-02 6.45916397e-03\n",
      " 1.72913019e-02 4.38956590e-03 8.49297270e-03 6.17037120e-04\n",
      " 2.02722242e-03 7.48336129e-03 9.74389687e-02 2.48836679e-03\n",
      " 4.48115310e-03 6.94581773e-03 1.21651841e-02 1.33753657e-01\n",
      " 5.17192408e-02 8.42180476e-03 3.78600091e-01 9.61181223e-02\n",
      " 5.93843684e-03 7.69595802e-01 9.44120169e-01 3.33783664e-02\n",
      " 2.96279765e-03 7.58269383e-03 1.41839638e-01 5.55625260e-01\n",
      " 8.32170174e-02 1.47000549e-03 1.80820969e-03 1.66695833e-03\n",
      " 3.23250121e-03 3.05158249e-03 6.46586251e-03 3.35801276e-03\n",
      " 3.79973510e-03 5.89655014e-03 4.60466323e-03 6.10029092e-03\n",
      " 1.14871375e-02 6.80900458e-03 4.13793288e-02 6.70676082e-02\n",
      " 7.00766265e-01 1.14276055e-02 8.70443821e-01 6.47239038e-04\n",
      " 5.80573780e-03 9.47271881e-04 8.87182727e-02 8.54642317e-03\n",
      " 1.02159671e-01 2.38967943e-03 3.15291085e-03 1.15992092e-02\n",
      " 5.82128346e-01 3.73451591e-01 8.20200920e-01 9.13053393e-01\n",
      " 7.64020979e-01 6.14299476e-01 8.40255246e-02 4.38175857e-01\n",
      " 9.66176242e-02 9.17237282e-01 2.49469243e-02 2.36475356e-02\n",
      " 1.14055306e-01 7.31074393e-01 7.37202227e-01 4.87176031e-01\n",
      " 5.93285382e-01 5.36835432e-01 9.63378310e-01 9.73233953e-02\n",
      " 8.47478986e-01 8.24010670e-01 9.62337911e-01 3.96934509e-01\n",
      " 5.69598237e-03 1.19776860e-01 4.90730703e-01 1.89403042e-01\n",
      " 7.33898655e-02 1.01629598e-02 2.17760757e-01 4.87772468e-03\n",
      " 6.81242673e-03 9.41278517e-01 1.37089372e-01 6.85276696e-03\n",
      " 2.28966633e-03 2.39390776e-01 1.54778231e-02 7.26683214e-02\n",
      " 1.95603259e-02 6.22600973e-01 4.14128136e-03 3.26893963e-02\n",
      " 1.23336669e-02 1.49713427e-01 2.34411478e-01 6.13208190e-02\n",
      " 1.06282244e-02 1.35279223e-02 1.80003904e-02 3.22104134e-02\n",
      " 8.23132545e-02 1.15170749e-02 2.71923631e-01 2.65093762e-02]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 16 [0/106 (0%)]\tTrain Loss: 0.014696\n",
      "Train Epoch: 16 [10/106 (9%)]\tTrain Loss: 0.007653\n",
      "Train Epoch: 16 [20/106 (19%)]\tTrain Loss: 0.012634\n",
      "Train Epoch: 16 [30/106 (28%)]\tTrain Loss: 0.002663\n",
      "Train Epoch: 16 [40/106 (38%)]\tTrain Loss: 0.004184\n",
      "Train Epoch: 16 [50/106 (47%)]\tTrain Loss: 0.051540\n",
      "Train Epoch: 16 [60/106 (57%)]\tTrain Loss: 0.193607\n",
      "Train Epoch: 16 [70/106 (66%)]\tTrain Loss: 0.002464\n",
      "Train Epoch: 16 [80/106 (75%)]\tTrain Loss: 0.047524\n",
      "Train Epoch: 16 [90/106 (85%)]\tTrain Loss: 0.003122\n",
      "Train Epoch: 16 [100/106 (94%)]\tTrain Loss: 0.008223\n",
      "\n",
      "Train set: Average loss: 0.0002, Accuracy: 364/424 (86%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.40200229e-03 2.68754885e-02 1.38117454e-03 1.96065957e-04\n",
      " 6.68935652e-04 6.92524700e-05 2.36722510e-02 8.91598058e-04\n",
      " 2.04348427e-04 3.07476497e-04 2.90040788e-03 8.52410813e-05\n",
      " 5.99885941e-04 6.07735477e-03 7.99248517e-02 4.15876944e-04\n",
      " 1.36683544e-03 2.44038319e-03 4.84925316e-04 1.29970647e-02\n",
      " 1.59443065e-03 6.89487963e-04 1.75171699e-02 3.24753784e-02\n",
      " 6.06102636e-04 6.95675090e-02 9.55409944e-01 2.86110910e-03\n",
      " 4.58904833e-04 1.90264708e-03 5.38839810e-02 5.39961338e-01\n",
      " 2.01381035e-02 4.31732100e-04 5.92323369e-04 3.68211739e-04\n",
      " 4.67624370e-04 3.85545398e-04 1.10559026e-03 3.01624590e-04\n",
      " 4.00213350e-04 4.79193754e-04 4.22286423e-04 7.40345451e-04\n",
      " 7.58554274e-03 6.26320019e-03 5.01720719e-02 1.32051870e-01\n",
      " 8.29517305e-01 4.97864000e-03 9.35437441e-01 9.63237107e-05\n",
      " 2.62272009e-03 6.86594634e-04 6.96123391e-03 6.09460659e-03\n",
      " 9.59296152e-03 3.78402299e-04 1.68067767e-04 4.76786355e-03\n",
      " 6.18884563e-01 4.20903295e-01 8.49946797e-01 8.80771339e-01\n",
      " 7.30585337e-01 6.46435440e-01 3.30076506e-03 8.99966180e-01\n",
      " 6.11977763e-02 9.26781416e-01 6.83590174e-02 4.96115796e-02\n",
      " 5.41204989e-01 8.11733246e-01 8.41807067e-01 3.51364970e-01\n",
      " 7.10549593e-01 6.72234774e-01 9.64986265e-01 2.52123803e-01\n",
      " 7.99335122e-01 7.85165548e-01 9.46572423e-01 3.35711926e-01\n",
      " 9.10483592e-04 2.15851143e-01 8.83366317e-02 9.98047441e-02\n",
      " 6.02126401e-03 2.19010864e-03 4.65988442e-02 2.98810471e-03\n",
      " 1.24891207e-03 7.56832242e-01 2.94535402e-02 3.14247649e-04\n",
      " 4.74342378e-04 5.19425683e-02 2.94209062e-03 4.18513447e-01\n",
      " 4.74225916e-03 5.06564856e-01 7.90382212e-04 4.21922840e-02\n",
      " 9.42362659e-03 1.86055407e-01 6.49803579e-01 4.91663255e-02\n",
      " 5.38080465e-04 1.56610906e-02 2.46464647e-03 2.14537978e-03\n",
      " 2.81677600e-02 2.65208818e-03 2.58533694e-02 2.78872973e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 17 [0/106 (0%)]\tTrain Loss: 0.069380\n",
      "Train Epoch: 17 [10/106 (9%)]\tTrain Loss: 0.007119\n",
      "Train Epoch: 17 [20/106 (19%)]\tTrain Loss: 0.006275\n",
      "Train Epoch: 17 [30/106 (28%)]\tTrain Loss: 0.220909\n",
      "Train Epoch: 17 [40/106 (38%)]\tTrain Loss: 0.006363\n",
      "Train Epoch: 17 [50/106 (47%)]\tTrain Loss: 0.080645\n",
      "Train Epoch: 17 [60/106 (57%)]\tTrain Loss: 0.041405\n",
      "Train Epoch: 17 [70/106 (66%)]\tTrain Loss: 0.059844\n",
      "Train Epoch: 17 [80/106 (75%)]\tTrain Loss: 0.139426\n",
      "Train Epoch: 17 [90/106 (85%)]\tTrain Loss: 0.008176\n",
      "Train Epoch: 17 [100/106 (94%)]\tTrain Loss: 0.036327\n",
      "\n",
      "Train set: Average loss: 0.0001, Accuracy: 365/424 (86%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.73113698e-03 8.83544609e-03 3.40779370e-04 5.00539376e-04\n",
      " 1.06442510e-03 2.64713191e-04 9.76503547e-03 1.13636651e-03\n",
      " 1.42685976e-03 7.08613428e-04 4.29445849e-04 1.55817062e-04\n",
      " 8.54665413e-05 2.68823244e-02 1.74398601e-01 7.34451678e-05\n",
      " 5.72426536e-04 1.67302564e-02 1.20521669e-04 3.13099660e-03\n",
      " 1.28148543e-03 2.11975211e-03 1.69630677e-01 4.81516644e-02\n",
      " 5.31430938e-04 4.26607490e-01 9.61485684e-01 8.95222358e-04\n",
      " 2.72698497e-04 9.85376653e-04 5.46245836e-02 1.23131521e-01\n",
      " 1.80163812e-02 9.34025302e-05 9.29344096e-05 1.21633719e-04\n",
      " 5.32838400e-04 2.44246656e-03 4.14598035e-03 3.79852136e-04\n",
      " 3.57675104e-04 8.10486206e-04 2.86733033e-03 1.15954457e-03\n",
      " 1.12569588e-03 1.99902290e-03 9.76738334e-03 4.35741208e-02\n",
      " 5.23408890e-01 2.46830354e-03 6.60697222e-01 3.58407815e-05\n",
      " 4.77270485e-04 1.03592894e-04 4.92024561e-03 6.33873104e-04\n",
      " 2.56668050e-02 9.98823962e-05 1.98018010e-04 1.64209248e-03\n",
      " 3.98269892e-01 2.14318737e-01 8.89204800e-01 8.18870425e-01\n",
      " 1.12780996e-01 5.19861639e-01 3.21566388e-02 6.28714085e-01\n",
      " 4.15082760e-02 8.30891609e-01 1.31596863e-01 1.02378875e-01\n",
      " 3.80562752e-01 8.17615390e-01 2.44635433e-01 1.80755869e-01\n",
      " 9.17593360e-01 7.85821438e-01 9.19505060e-01 3.55238616e-01\n",
      " 8.35633516e-01 6.40194774e-01 9.68232751e-01 1.53655112e-01\n",
      " 3.93421296e-03 4.83103007e-01 6.75613135e-02 3.94564345e-02\n",
      " 1.18128033e-02 1.19637074e-02 6.00637019e-01 3.62480845e-04\n",
      " 1.37508463e-03 9.29248333e-01 1.03844702e-01 7.81422365e-04\n",
      " 1.19970711e-04 1.05122387e-01 9.96424933e-04 1.88821256e-01\n",
      " 1.07068112e-02 7.09612548e-01 2.35332665e-03 8.21616650e-02\n",
      " 4.98047844e-03 3.11802596e-01 1.09987371e-01 1.26415312e-01\n",
      " 1.24010467e-03 1.16414961e-03 2.50300020e-03 4.36014775e-03\n",
      " 1.95066072e-02 5.95273741e-04 7.53040254e-01 9.62276664e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 18 [0/106 (0%)]\tTrain Loss: 0.002427\n",
      "Train Epoch: 18 [10/106 (9%)]\tTrain Loss: 0.120513\n",
      "Train Epoch: 18 [20/106 (19%)]\tTrain Loss: 0.003765\n",
      "Train Epoch: 18 [30/106 (28%)]\tTrain Loss: 0.186134\n",
      "Train Epoch: 18 [40/106 (38%)]\tTrain Loss: 0.013219\n",
      "Train Epoch: 18 [50/106 (47%)]\tTrain Loss: 0.006708\n",
      "Train Epoch: 18 [60/106 (57%)]\tTrain Loss: 0.239717\n",
      "Train Epoch: 18 [70/106 (66%)]\tTrain Loss: 0.015720\n",
      "Train Epoch: 18 [80/106 (75%)]\tTrain Loss: 0.156443\n",
      "Train Epoch: 18 [90/106 (85%)]\tTrain Loss: 0.003172\n",
      "Train Epoch: 18 [100/106 (94%)]\tTrain Loss: 0.003002\n",
      "\n",
      "Train set: Average loss: 0.0004, Accuracy: 378/424 (89%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.61499570e-03 1.54481065e-02 1.91592635e-03 3.70398950e-04\n",
      " 1.04124262e-03 4.73555410e-04 1.44014088e-02 1.08583050e-03\n",
      " 1.49796205e-03 7.85968849e-04 3.19749990e-04 8.34515493e-04\n",
      " 2.22286399e-04 8.94771132e-04 4.95871566e-02 1.83698721e-04\n",
      " 8.73869227e-04 2.26274133e-03 2.29500132e-04 2.05504801e-03\n",
      " 5.40694280e-04 3.46464280e-04 3.30906454e-03 2.18216423e-03\n",
      " 1.27764783e-04 3.85623542e-03 3.89452428e-01 3.98847158e-04\n",
      " 2.45128904e-04 4.07245912e-04 5.11716027e-03 4.31562401e-02\n",
      " 1.23020940e-01 2.13456253e-04 4.36305883e-04 3.79164005e-04\n",
      " 3.60589678e-04 2.85685854e-03 9.37763893e-04 2.87584699e-04\n",
      " 2.88257754e-04 4.81143099e-04 2.66004074e-03 2.20450223e-03\n",
      " 3.52279725e-03 1.38630869e-03 2.48845313e-02 4.88588735e-02\n",
      " 2.39248410e-01 4.92330873e-04 3.18922967e-01 1.06274310e-04\n",
      " 1.46349240e-03 3.71362432e-04 3.65100976e-04 4.51286184e-03\n",
      " 9.77678807e-04 3.05683876e-04 1.19438302e-03 2.71059410e-03\n",
      " 5.90936899e-01 1.46580815e-01 6.65344834e-01 5.97962201e-01\n",
      " 2.62353458e-02 3.96875665e-03 1.11508882e-03 1.35070086e-01\n",
      " 7.17353215e-03 1.78439245e-02 1.03057995e-02 8.67789704e-03\n",
      " 3.31044011e-02 5.77867636e-03 1.47909233e-02 2.48483438e-02\n",
      " 7.93196321e-01 8.60337496e-01 9.59313571e-01 5.86057678e-02\n",
      " 6.71913400e-02 1.14321731e-01 8.84019434e-01 3.87033783e-02\n",
      " 3.79008969e-04 1.70305129e-02 3.00426595e-02 2.80715879e-02\n",
      " 1.20338611e-03 1.26241113e-03 1.39931208e-02 3.68732843e-04\n",
      " 2.25617760e-03 5.08763967e-03 2.88728182e-03 1.37160678e-04\n",
      " 5.55441016e-04 9.54393577e-03 5.19242545e-04 2.20616418e-03\n",
      " 2.46849144e-03 5.83406016e-02 2.53660185e-03 1.44034333e-03\n",
      " 1.33551017e-03 1.41878696e-02 6.58758953e-02 4.48913081e-03\n",
      " 7.07651416e-05 6.16604695e-04 2.48406490e-04 1.08310941e-03\n",
      " 1.13099231e-03 9.96783056e-05 2.46173842e-03 1.38437783e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 19 [0/106 (0%)]\tTrain Loss: 0.183359\n",
      "Train Epoch: 19 [10/106 (9%)]\tTrain Loss: 0.199419\n",
      "Train Epoch: 19 [20/106 (19%)]\tTrain Loss: 0.102141\n",
      "Train Epoch: 19 [30/106 (28%)]\tTrain Loss: 0.089553\n",
      "Train Epoch: 19 [40/106 (38%)]\tTrain Loss: 0.004117\n",
      "Train Epoch: 19 [50/106 (47%)]\tTrain Loss: 0.003370\n",
      "Train Epoch: 19 [60/106 (57%)]\tTrain Loss: 0.003091\n",
      "Train Epoch: 19 [70/106 (66%)]\tTrain Loss: 0.012203\n",
      "Train Epoch: 19 [80/106 (75%)]\tTrain Loss: 0.324309\n",
      "Train Epoch: 19 [90/106 (85%)]\tTrain Loss: 0.003879\n",
      "Train Epoch: 19 [100/106 (94%)]\tTrain Loss: 0.051977\n",
      "\n",
      "Train set: Average loss: 0.0008, Accuracy: 362/424 (85%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [5.53514052e-04 4.98558953e-03 4.00433550e-04 3.75077070e-04\n",
      " 4.05674148e-03 5.52129350e-04 3.69334407e-03 1.93813769e-03\n",
      " 1.14659197e-03 2.43934756e-03 5.42092777e-04 3.03930748e-04\n",
      " 3.09744937e-04 5.06951823e-04 1.99078526e-02 1.79976691e-04\n",
      " 1.62385011e-04 8.50099977e-03 9.18468868e-05 9.00808780e-04\n",
      " 2.78661755e-04 3.02372908e-04 7.83059280e-04 2.74601672e-03\n",
      " 1.58200419e-04 3.69819882e-03 6.77883267e-01 2.56293162e-04\n",
      " 1.94927779e-04 2.28228411e-04 1.29825650e-02 2.39063688e-02\n",
      " 1.90486237e-02 5.65074697e-05 1.81229625e-04 9.52662667e-05\n",
      " 2.15055610e-04 3.41668772e-03 3.83597123e-03 9.53727111e-04\n",
      " 6.56797725e-04 7.03707221e-04 3.65025736e-03 1.95615459e-03\n",
      " 2.74273218e-03 3.60589329e-04 2.55735945e-02 5.73000498e-02\n",
      " 7.49957919e-01 1.00706224e-04 7.42945254e-01 1.30595945e-04\n",
      " 2.76996434e-04 3.92886956e-04 9.25975546e-05 6.30175287e-04\n",
      " 3.24702269e-04 1.23861406e-04 8.76280596e-04 1.06941967e-03\n",
      " 7.68776750e-03 3.03166639e-03 1.46952331e-01 1.90355461e-02\n",
      " 6.10649474e-02 3.31667699e-02 2.12143222e-03 5.73366344e-01\n",
      " 2.33564209e-02 3.11573725e-02 2.67337216e-03 2.12367438e-03\n",
      " 1.93499252e-01 2.31459737e-01 2.15641946e-01 1.17775425e-02\n",
      " 9.74921644e-01 9.78091002e-01 5.22765398e-01 1.40963107e-01\n",
      " 4.00307886e-02 1.64540693e-01 8.78385603e-01 2.16796733e-02\n",
      " 1.13750597e-04 1.74355153e-02 4.67543630e-03 5.10424748e-03\n",
      " 1.40925811e-04 6.54875126e-04 1.48618266e-01 2.08263076e-03\n",
      " 3.67828854e-03 4.97332998e-02 2.06200872e-03 8.85313202e-05\n",
      " 2.42921611e-04 2.15329835e-03 4.62324329e-04 5.71972039e-03\n",
      " 3.60339228e-03 3.31670195e-02 8.31394631e-04 1.18044275e-03\n",
      " 1.65983173e-03 4.27505514e-03 3.96864340e-02 1.57924742e-03\n",
      " 2.63825990e-04 1.74180511e-03 4.63588862e-04 1.55294023e-03\n",
      " 2.35927338e-03 1.25388440e-04 3.70087597e-04 3.62890947e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 20 [0/106 (0%)]\tTrain Loss: 0.005888\n",
      "Train Epoch: 20 [10/106 (9%)]\tTrain Loss: 0.034180\n",
      "Train Epoch: 20 [20/106 (19%)]\tTrain Loss: 0.016753\n",
      "Train Epoch: 20 [30/106 (28%)]\tTrain Loss: 0.005936\n",
      "Train Epoch: 20 [40/106 (38%)]\tTrain Loss: 0.033011\n",
      "Train Epoch: 20 [50/106 (47%)]\tTrain Loss: 0.003661\n",
      "Train Epoch: 20 [60/106 (57%)]\tTrain Loss: 0.003122\n",
      "Train Epoch: 20 [70/106 (66%)]\tTrain Loss: 0.005470\n",
      "Train Epoch: 20 [80/106 (75%)]\tTrain Loss: 0.002915\n",
      "Train Epoch: 20 [90/106 (85%)]\tTrain Loss: 0.008539\n",
      "Train Epoch: 20 [100/106 (94%)]\tTrain Loss: 0.052047\n",
      "\n",
      "Train set: Average loss: 0.0026, Accuracy: 379/424 (89%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.28438638e-03 3.92144173e-03 3.37942503e-04 1.59205229e-04\n",
      " 1.35441509e-03 3.70192778e-04 2.63925688e-03 9.48333822e-04\n",
      " 3.18099599e-04 8.37187166e-04 3.14924575e-04 9.89006949e-05\n",
      " 1.17944604e-04 3.31899180e-04 2.93242442e-03 5.96938953e-05\n",
      " 2.55014951e-04 1.41699705e-03 1.30661239e-04 1.73255533e-03\n",
      " 6.28173002e-04 2.07834732e-04 2.30060075e-03 3.12531809e-03\n",
      " 1.51036176e-04 9.21025407e-03 9.55787539e-01 4.51843342e-04\n",
      " 1.14609204e-04 1.81536278e-04 1.89147107e-02 7.12848548e-03\n",
      " 3.43770348e-03 3.32162199e-05 6.13226075e-05 3.02292701e-05\n",
      " 9.13363037e-05 7.63533870e-04 5.18094399e-04 2.30368736e-04\n",
      " 2.28574223e-04 2.02475101e-04 2.97183613e-03 5.52730169e-04\n",
      " 1.25850213e-03 5.36946405e-04 1.25801442e-02 1.50244264e-02\n",
      " 5.81804037e-01 1.10563356e-03 8.23762238e-01 7.77795212e-05\n",
      " 4.77533002e-04 1.42753546e-04 1.07874838e-03 8.54360173e-04\n",
      " 2.97580002e-04 1.08257467e-04 1.96670357e-04 1.87489029e-03\n",
      " 5.62119782e-02 1.08424379e-02 6.85967445e-01 5.06278694e-01\n",
      " 5.10298759e-02 2.80737523e-02 1.03559485e-03 2.73410589e-01\n",
      " 6.28693448e-03 2.66798697e-02 2.86077592e-03 9.00498999e-04\n",
      " 7.58370608e-02 2.48495191e-01 1.90214202e-01 4.47163498e-03\n",
      " 4.66207176e-01 1.51273102e-01 8.19888353e-01 5.10407798e-02\n",
      " 1.62789151e-01 4.49800253e-01 9.85740244e-01 4.27225903e-02\n",
      " 4.54776920e-04 2.48066466e-02 4.52092895e-03 5.48480079e-03\n",
      " 8.11350066e-04 1.31125189e-03 7.90767372e-01 5.35658502e-04\n",
      " 8.57125269e-04 1.12109385e-01 8.31183512e-03 1.53679881e-04\n",
      " 1.10605790e-04 3.10102501e-03 2.04976794e-04 1.54192314e-01\n",
      " 6.29683025e-04 1.32398112e-02 1.26914209e-04 2.39287131e-03\n",
      " 4.04123543e-03 7.33236829e-03 2.26742774e-02 2.79377983e-03\n",
      " 1.14450420e-04 3.23553849e-03 1.03257038e-03 7.36816728e-04\n",
      " 2.04736404e-02 2.06020748e-04 6.34952390e-04 2.72480218e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 4 TN= 58 FN= 52 FP= 2\n",
      "TP+FP 6\n",
      "precision 0.6666666666666666\n",
      "recall 0.07142857142857142\n",
      "F1 0.12903225806451613\n",
      "acc 0.5344827586206896\n",
      "AUCp 0.519047619047619\n",
      "AUC 0.8035714285714286\n",
      "\n",
      " The epoch is 20, average recall: 0.0714, average precision: 0.6667,average F1: 0.1290, average accuracy: 0.5345, average AUC: 0.8036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 21 [0/106 (0%)]\tTrain Loss: 0.003236\n",
      "Train Epoch: 21 [10/106 (9%)]\tTrain Loss: 0.010973\n",
      "Train Epoch: 21 [20/106 (19%)]\tTrain Loss: 0.318293\n",
      "Train Epoch: 21 [30/106 (28%)]\tTrain Loss: 0.001292\n",
      "Train Epoch: 21 [40/106 (38%)]\tTrain Loss: 0.227332\n",
      "Train Epoch: 21 [50/106 (47%)]\tTrain Loss: 0.022658\n",
      "Train Epoch: 21 [60/106 (57%)]\tTrain Loss: 0.004782\n",
      "Train Epoch: 21 [70/106 (66%)]\tTrain Loss: 0.006454\n",
      "Train Epoch: 21 [80/106 (75%)]\tTrain Loss: 0.001870\n",
      "Train Epoch: 21 [90/106 (85%)]\tTrain Loss: 0.092676\n",
      "Train Epoch: 21 [100/106 (94%)]\tTrain Loss: 0.034441\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 372/424 (88%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.34041642e-03 4.26634820e-03 6.32555690e-04 1.38139556e-04\n",
      " 2.36191112e-03 1.66678685e-04 3.81989894e-03 4.42738296e-04\n",
      " 9.26415523e-05 2.19751702e-04 3.39373248e-04 2.03946274e-05\n",
      " 2.82568333e-04 3.63147730e-04 5.83657576e-03 9.88528918e-05\n",
      " 4.52041044e-04 4.49342857e-04 8.52598678e-05 3.39926872e-03\n",
      " 1.10300037e-03 2.61548645e-04 4.21336666e-03 2.25046780e-02\n",
      " 3.53803189e-04 5.24379173e-03 9.59072590e-01 3.51279479e-04\n",
      " 1.52724489e-04 1.71691194e-04 2.06998289e-02 2.67473399e-03\n",
      " 8.07958480e-04 1.21525649e-04 1.90986277e-04 6.17096521e-05\n",
      " 2.24104617e-04 3.35498044e-04 4.22686164e-04 7.90221384e-05\n",
      " 1.01405632e-04 1.07319276e-04 3.66278779e-04 3.80599027e-04\n",
      " 1.56324601e-03 1.05424959e-03 1.35635445e-02 2.25254670e-02\n",
      " 5.01304306e-02 3.87800595e-04 3.99484038e-01 5.09720521e-05\n",
      " 3.79758363e-04 1.52890600e-04 4.58450150e-03 7.70424551e-04\n",
      " 2.43581191e-04 2.68864009e-04 1.92766587e-04 1.86475751e-03\n",
      " 1.32694215e-01 3.10352072e-02 6.86839998e-01 6.71536088e-01\n",
      " 3.10972910e-02 5.17756538e-03 2.53703649e-04 1.39087541e-02\n",
      " 7.16908835e-03 2.26469740e-01 1.20905852e-02 1.11759333e-02\n",
      " 3.68861072e-02 7.90267289e-01 2.69171774e-01 1.21619611e-03\n",
      " 3.15414459e-01 2.83934534e-01 5.21865129e-01 5.08855656e-03\n",
      " 1.57543465e-01 3.79118286e-02 9.76029456e-01 7.85318611e-04\n",
      " 1.51111788e-04 6.77250605e-03 2.15305644e-03 1.27792160e-03\n",
      " 1.13035890e-03 1.04819867e-03 6.16472602e-01 2.26838456e-04\n",
      " 2.68247939e-04 3.44199866e-01 1.95154659e-02 3.18930426e-04\n",
      " 2.18281202e-04 3.54494480e-03 3.40553292e-04 3.44894007e-02\n",
      " 7.36884424e-04 2.63535883e-03 1.61930089e-04 3.53707769e-03\n",
      " 7.53060356e-03 7.18961209e-02 5.72604239e-02 1.72027189e-03\n",
      " 6.72005190e-05 3.51855485e-03 1.32898637e-03 1.07564323e-03\n",
      " 3.71091738e-02 3.46862304e-04 8.89678020e-04 4.86263052e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 22 [0/106 (0%)]\tTrain Loss: 0.079224\n",
      "Train Epoch: 22 [10/106 (9%)]\tTrain Loss: 0.003913\n",
      "Train Epoch: 22 [20/106 (19%)]\tTrain Loss: 0.007131\n",
      "Train Epoch: 22 [30/106 (28%)]\tTrain Loss: 0.013883\n",
      "Train Epoch: 22 [40/106 (38%)]\tTrain Loss: 0.088036\n",
      "Train Epoch: 22 [50/106 (47%)]\tTrain Loss: 0.098467\n",
      "Train Epoch: 22 [60/106 (57%)]\tTrain Loss: 0.001693\n",
      "Train Epoch: 22 [70/106 (66%)]\tTrain Loss: 0.005101\n",
      "Train Epoch: 22 [80/106 (75%)]\tTrain Loss: 0.083547\n",
      "Train Epoch: 22 [90/106 (85%)]\tTrain Loss: 0.005826\n",
      "Train Epoch: 22 [100/106 (94%)]\tTrain Loss: 0.005649\n",
      "\n",
      "Train set: Average loss: 0.0073, Accuracy: 383/424 (90%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.36530229e-03 1.26863178e-03 2.38760607e-04 1.99020418e-04\n",
      " 1.66385749e-03 1.50720516e-04 1.07848435e-03 1.47052913e-03\n",
      " 1.27589607e-04 3.77278600e-04 7.63688469e-04 2.70518831e-05\n",
      " 4.92071209e-04 2.39248504e-04 1.12553807e-02 1.89686834e-04\n",
      " 6.06512825e-04 3.18405870e-03 7.82792486e-05 2.17256248e-02\n",
      " 6.28572190e-04 4.51023167e-04 2.02845961e-01 3.36627036e-01\n",
      " 4.92363935e-04 1.16963655e-01 9.75419283e-01 9.07172216e-04\n",
      " 1.34633592e-04 3.04375833e-04 9.80947495e-01 9.49616671e-01\n",
      " 7.11758956e-02 3.32775744e-05 1.45757323e-04 3.30715920e-05\n",
      " 2.47970806e-04 2.90089613e-03 2.59487494e-03 2.63689959e-04\n",
      " 3.98225995e-04 3.01748572e-04 1.64494500e-03 8.90649681e-04\n",
      " 9.03965905e-03 9.93787218e-03 3.62328701e-02 1.40766397e-01\n",
      " 9.67282772e-01 1.60592375e-04 9.47676420e-01 2.26841265e-04\n",
      " 5.12506522e-04 1.71994790e-04 2.83839297e-03 3.76095210e-04\n",
      " 1.05928639e-02 1.11357716e-04 3.27146729e-04 8.28772597e-03\n",
      " 5.86879075e-01 1.14143193e-01 9.53031719e-01 9.72159624e-01\n",
      " 9.79829907e-01 9.20732200e-01 1.84666254e-02 9.07386541e-01\n",
      " 1.03229947e-01 9.95525897e-01 1.15185261e-01 7.88764805e-02\n",
      " 7.01032877e-01 9.69701231e-01 9.56105947e-01 5.18427074e-01\n",
      " 2.91558981e-01 1.40676200e-01 9.44147587e-01 1.92926228e-01\n",
      " 9.82600749e-01 9.68522668e-01 9.82378781e-01 7.39874840e-01\n",
      " 1.09214312e-03 8.45939890e-02 2.64331140e-02 2.05226392e-02\n",
      " 5.17866225e-04 9.47606750e-04 7.31565416e-01 2.16040295e-03\n",
      " 2.65392754e-03 9.80539680e-01 4.18994064e-03 1.33812166e-04\n",
      " 5.09409001e-04 4.34281796e-01 2.05297547e-04 9.27321553e-01\n",
      " 1.87863817e-03 7.47081876e-01 2.91981269e-04 2.98817754e-01\n",
      " 5.42996079e-02 9.81891155e-02 9.61118639e-01 3.10610584e-03\n",
      " 1.13635033e-04 2.47888104e-03 2.84942309e-03 9.47608554e-04\n",
      " 7.88898300e-03 3.67117953e-03 9.73502931e-04 5.22653281e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 23 [0/106 (0%)]\tTrain Loss: 0.108684\n",
      "Train Epoch: 23 [10/106 (9%)]\tTrain Loss: 0.001266\n",
      "Train Epoch: 23 [20/106 (19%)]\tTrain Loss: 0.281987\n",
      "Train Epoch: 23 [30/106 (28%)]\tTrain Loss: 0.004934\n",
      "Train Epoch: 23 [40/106 (38%)]\tTrain Loss: 0.000864\n",
      "Train Epoch: 23 [50/106 (47%)]\tTrain Loss: 0.055229\n",
      "Train Epoch: 23 [60/106 (57%)]\tTrain Loss: 0.002852\n",
      "Train Epoch: 23 [70/106 (66%)]\tTrain Loss: 0.002456\n",
      "Train Epoch: 23 [80/106 (75%)]\tTrain Loss: 0.000911\n",
      "Train Epoch: 23 [90/106 (85%)]\tTrain Loss: 0.005891\n",
      "Train Epoch: 23 [100/106 (94%)]\tTrain Loss: 0.036017\n",
      "\n",
      "Train set: Average loss: 0.0001, Accuracy: 381/424 (90%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.65574902e-02 7.12571852e-03 5.85027330e-04 4.60206112e-03\n",
      " 6.06845692e-02 1.38790056e-03 5.88901620e-03 6.25055432e-02\n",
      " 1.98530196e-03 1.02991732e-02 4.64824773e-02 9.15629382e-04\n",
      " 2.54429467e-02 2.28480119e-02 6.18519068e-01 2.21419963e-03\n",
      " 1.54551864e-03 1.02885313e-01 3.77454329e-04 1.70557633e-01\n",
      " 6.78550778e-03 2.68587098e-03 3.68340313e-01 9.61568713e-01\n",
      " 5.12694567e-03 2.61030436e-01 9.96215880e-01 4.77501843e-03\n",
      " 7.91100960e-04 8.77662748e-03 9.90722835e-01 4.88246739e-01\n",
      " 6.22462869e-01 2.49816832e-04 2.85689952e-03 7.85774435e-04\n",
      " 2.69536977e-03 3.32015716e-02 9.73069388e-03 1.25644798e-03\n",
      " 4.23689513e-03 7.23402074e-04 7.61239827e-02 2.60430072e-02\n",
      " 2.13274777e-01 2.49444023e-02 1.11527115e-01 2.51056761e-01\n",
      " 9.79847968e-01 1.32564094e-03 9.87032413e-01 8.80895182e-03\n",
      " 2.31132284e-03 7.79660605e-03 2.18568183e-02 1.94968423e-03\n",
      " 1.07939728e-02 4.12513781e-03 4.30352148e-03 9.92281921e-03\n",
      " 9.91288126e-01 9.76467967e-01 9.93809998e-01 9.94287074e-01\n",
      " 9.89064813e-01 9.48294640e-01 1.20518051e-01 9.77407634e-01\n",
      " 4.95812744e-01 9.87268388e-01 2.75938421e-01 1.69545218e-01\n",
      " 9.91015792e-01 9.97111440e-01 9.92066264e-01 8.69729936e-01\n",
      " 1.64947689e-01 1.15259156e-01 9.92071569e-01 2.50322133e-01\n",
      " 9.58090127e-01 6.62243664e-01 9.95409787e-01 9.90184128e-01\n",
      " 4.70885374e-02 8.47596645e-01 6.93410099e-01 9.59538639e-01\n",
      " 4.04668786e-03 4.24297526e-03 9.89536405e-01 1.68949049e-02\n",
      " 1.62627622e-02 9.38740253e-01 1.64885242e-02 5.46782569e-04\n",
      " 2.41650268e-03 1.03180476e-01 1.65282213e-03 9.81403351e-01\n",
      " 4.41628322e-03 4.68676567e-01 2.51257955e-03 2.55058616e-01\n",
      " 4.32117313e-01 1.82343334e-01 9.09664273e-01 4.65228595e-02\n",
      " 4.54469235e-04 1.15971841e-01 8.81623290e-03 1.23243197e-03\n",
      " 1.88856274e-02 3.69245708e-02 6.18791580e-03 3.53672472e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 24 [0/106 (0%)]\tTrain Loss: 0.011312\n",
      "Train Epoch: 24 [10/106 (9%)]\tTrain Loss: 0.000977\n",
      "Train Epoch: 24 [20/106 (19%)]\tTrain Loss: 0.001255\n",
      "Train Epoch: 24 [30/106 (28%)]\tTrain Loss: 0.045673\n",
      "Train Epoch: 24 [40/106 (38%)]\tTrain Loss: 0.000837\n",
      "Train Epoch: 24 [50/106 (47%)]\tTrain Loss: 0.012362\n",
      "Train Epoch: 24 [60/106 (57%)]\tTrain Loss: 0.000541\n",
      "Train Epoch: 24 [70/106 (66%)]\tTrain Loss: 0.001077\n",
      "Train Epoch: 24 [80/106 (75%)]\tTrain Loss: 0.001755\n",
      "Train Epoch: 24 [90/106 (85%)]\tTrain Loss: 0.000881\n",
      "Train Epoch: 24 [100/106 (94%)]\tTrain Loss: 0.134589\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 401/424 (95%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.63568696e-03 8.38067569e-03 8.96068406e-04 1.02158508e-03\n",
      " 7.70752272e-03 3.28306516e-04 7.69879390e-03 6.61055604e-03\n",
      " 5.24759409e-04 1.79457502e-03 1.42931519e-03 1.70485451e-04\n",
      " 9.80021898e-04 4.50548343e-03 3.86305265e-02 3.01441178e-04\n",
      " 7.35070673e-04 1.20946812e-03 5.24524890e-04 1.91368405e-02\n",
      " 3.07878945e-03 1.24968670e-03 1.71061754e-02 7.13760316e-01\n",
      " 1.38858485e-03 1.92982778e-02 9.95160162e-01 1.12701510e-03\n",
      " 3.39066057e-04 7.13170099e-04 1.58225209e-01 5.61924279e-02\n",
      " 1.62350554e-02 1.87081998e-04 5.78299514e-04 2.85858376e-04\n",
      " 7.20791286e-04 2.78718001e-03 2.07980140e-03 4.04343882e-04\n",
      " 6.34535623e-04 3.30284296e-04 4.90255747e-03 1.93984574e-03\n",
      " 1.14078792e-02 5.93907572e-03 4.68954667e-02 8.44397321e-02\n",
      " 9.81226921e-01 4.74809203e-04 9.85147536e-01 9.74534429e-04\n",
      " 1.44571078e-03 6.72628696e-04 1.65534560e-02 7.98795780e-04\n",
      " 1.72780897e-03 5.70467906e-04 7.11164612e-04 4.38884459e-03\n",
      " 9.72507238e-01 7.71028221e-01 9.89703476e-01 9.94603932e-01\n",
      " 8.66668403e-01 8.50994512e-02 1.17853591e-02 4.85530436e-01\n",
      " 6.44190460e-02 9.32383299e-01 2.73808036e-02 1.55428154e-02\n",
      " 9.05974686e-01 9.85626280e-01 8.58792186e-01 4.78632331e-01\n",
      " 9.64574158e-01 6.57795489e-01 9.93611276e-01 6.98793903e-02\n",
      " 7.01567292e-01 3.17614257e-01 9.96283472e-01 1.90970704e-01\n",
      " 4.45084693e-03 5.38772702e-01 2.80944914e-01 4.88676697e-01\n",
      " 4.05653007e-03 1.64220890e-03 9.72775161e-01 8.44440714e-04\n",
      " 1.48474367e-03 9.65680182e-01 4.15980909e-03 5.13328356e-04\n",
      " 7.35993090e-04 1.49697363e-02 5.83164510e-04 3.47695313e-02\n",
      " 1.42987876e-03 1.75063550e-01 9.69839690e-04 1.03967153e-02\n",
      " 2.90167052e-02 3.75997834e-02 6.33703113e-01 4.28943615e-03\n",
      " 2.09869060e-04 2.84965336e-03 1.35782850e-03 9.72731563e-04\n",
      " 1.59230956e-03 5.16018656e-04 1.72265316e-03 9.68135486e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 25 [0/106 (0%)]\tTrain Loss: 0.043786\n",
      "Train Epoch: 25 [10/106 (9%)]\tTrain Loss: 0.006338\n",
      "Train Epoch: 25 [20/106 (19%)]\tTrain Loss: 0.011926\n",
      "Train Epoch: 25 [30/106 (28%)]\tTrain Loss: 0.008452\n",
      "Train Epoch: 25 [40/106 (38%)]\tTrain Loss: 0.000539\n",
      "Train Epoch: 25 [50/106 (47%)]\tTrain Loss: 0.005142\n",
      "Train Epoch: 25 [60/106 (57%)]\tTrain Loss: 0.000831\n",
      "Train Epoch: 25 [70/106 (66%)]\tTrain Loss: 0.003585\n",
      "Train Epoch: 25 [80/106 (75%)]\tTrain Loss: 0.051551\n",
      "Train Epoch: 25 [90/106 (85%)]\tTrain Loss: 0.001618\n",
      "Train Epoch: 25 [100/106 (94%)]\tTrain Loss: 0.297159\n",
      "\n",
      "Train set: Average loss: 0.0033, Accuracy: 391/424 (92%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.01092349e-04 1.08395005e-03 1.03059443e-04 1.87110869e-04\n",
      " 3.12193553e-03 7.27028964e-05 1.11355365e-03 2.08994048e-03\n",
      " 6.16466641e-05 3.70756665e-04 2.48474680e-04 2.55467730e-05\n",
      " 1.92286549e-04 7.13191461e-04 5.22349915e-03 1.45032798e-04\n",
      " 2.56865838e-04 1.52971101e-04 6.28368653e-05 1.18481589e-03\n",
      " 1.84663921e-04 3.12538323e-04 1.24774687e-03 3.86827052e-01\n",
      " 2.92418670e-04 1.24775397e-03 9.86790717e-01 6.61311788e-05\n",
      " 6.05263049e-05 1.08836801e-04 4.17337613e-03 6.28439616e-03\n",
      " 3.72234499e-03 3.57672725e-05 2.08635654e-04 5.31532824e-05\n",
      " 1.76283298e-04 4.91637562e-04 4.13922302e-04 3.99380006e-05\n",
      " 6.08425144e-05 4.02413971e-05 8.49789532e-04 3.98761447e-04\n",
      " 1.83743518e-03 1.26401428e-03 2.37684865e-02 2.48501841e-02\n",
      " 9.61476445e-01 3.05117992e-05 9.58051205e-01 3.89438064e-04\n",
      " 2.16122018e-04 3.12988879e-04 1.20798832e-04 2.33391838e-04\n",
      " 1.55141897e-04 8.70281656e-05 1.21388191e-04 7.49920728e-04\n",
      " 8.85479450e-01 2.63846070e-01 9.68226850e-01 9.83727574e-01\n",
      " 4.03356180e-02 2.63538351e-03 9.55551630e-04 9.63478070e-03\n",
      " 1.03011653e-02 3.15766543e-01 7.32288929e-03 6.45626104e-03\n",
      " 2.43674546e-01 7.92896807e-01 2.25596756e-01 3.05329096e-02\n",
      " 8.64805400e-01 6.62823021e-01 9.80312884e-01 9.20937024e-03\n",
      " 8.70471746e-02 5.89290075e-03 9.87742305e-01 5.62562468e-03\n",
      " 2.08310652e-04 1.65558115e-01 5.13801724e-03 3.31570162e-03\n",
      " 2.92391371e-04 2.58844142e-04 5.54480702e-02 1.66836733e-04\n",
      " 3.19727405e-04 1.29974127e-01 1.82535528e-04 3.83869665e-05\n",
      " 9.42874831e-05 1.17241812e-03 2.04685028e-04 5.53975208e-03\n",
      " 2.69726705e-04 6.79597305e-03 2.12053710e-04 3.67068802e-03\n",
      " 2.27271020e-03 3.19752027e-03 1.87797919e-02 9.77773219e-04\n",
      " 2.90289081e-05 2.79909262e-04 2.11237639e-04 1.82327829e-04\n",
      " 1.49030777e-04 7.87323879e-05 3.53363313e-04 1.12776841e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 26 [0/106 (0%)]\tTrain Loss: 0.002463\n",
      "Train Epoch: 26 [10/106 (9%)]\tTrain Loss: 0.000729\n",
      "Train Epoch: 26 [20/106 (19%)]\tTrain Loss: 0.000516\n",
      "Train Epoch: 26 [30/106 (28%)]\tTrain Loss: 0.263658\n",
      "Train Epoch: 26 [40/106 (38%)]\tTrain Loss: 0.007851\n",
      "Train Epoch: 26 [50/106 (47%)]\tTrain Loss: 0.002104\n",
      "Train Epoch: 26 [60/106 (57%)]\tTrain Loss: 0.003502\n",
      "Train Epoch: 26 [70/106 (66%)]\tTrain Loss: 0.069404\n",
      "Train Epoch: 26 [80/106 (75%)]\tTrain Loss: 0.000508\n",
      "Train Epoch: 26 [90/106 (85%)]\tTrain Loss: 0.010635\n",
      "Train Epoch: 26 [100/106 (94%)]\tTrain Loss: 0.000559\n",
      "\n",
      "Train set: Average loss: 0.0060, Accuracy: 398/424 (94%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [4.42175253e-04 1.79938541e-03 2.33095183e-04 6.55568263e-04\n",
      " 5.47525380e-03 2.09256847e-04 2.14326358e-03 7.26925768e-03\n",
      " 2.36174965e-04 1.57940900e-03 1.30715442e-03 1.06573076e-04\n",
      " 8.90215742e-04 1.26815517e-03 1.21647483e-02 3.13909288e-04\n",
      " 4.13518486e-04 7.12590932e-04 1.67526479e-04 3.96837573e-03\n",
      " 5.09572623e-04 9.88830579e-04 6.97878515e-03 3.31835896e-01\n",
      " 8.76125647e-04 4.01363894e-03 9.94996786e-01 2.18312402e-04\n",
      " 2.27599958e-04 3.73609480e-04 2.77342182e-02 2.40451917e-02\n",
      " 4.29512113e-02 9.71969421e-05 4.62852593e-04 1.24276427e-04\n",
      " 3.27277783e-04 1.57149241e-03 1.23975985e-03 1.43355923e-04\n",
      " 2.52388039e-04 1.16274277e-04 2.29103258e-03 1.10511261e-03\n",
      " 7.18969246e-03 4.65058349e-03 2.82634869e-02 3.52673903e-02\n",
      " 9.56988692e-01 1.31458801e-04 9.72709954e-01 6.86481071e-04\n",
      " 5.54040365e-04 9.92681715e-04 6.03263616e-04 7.44273595e-04\n",
      " 7.07798114e-04 2.80125474e-04 5.10513084e-04 1.78506423e-03\n",
      " 9.51626778e-01 8.08233142e-01 9.94592130e-01 9.95598137e-01\n",
      " 3.69892031e-01 7.18062744e-02 4.54029394e-03 3.69332820e-01\n",
      " 4.29759659e-02 7.14544356e-01 1.16029270e-02 1.42307309e-02\n",
      " 6.08929336e-01 9.87631738e-01 8.96594703e-01 3.96977365e-01\n",
      " 5.58172703e-01 3.08812141e-01 9.91904080e-01 2.36208178e-02\n",
      " 1.37275249e-01 1.62856095e-02 9.94978726e-01 1.23756513e-01\n",
      " 4.89282247e-04 1.98239088e-01 1.35803660e-02 1.47167863e-02\n",
      " 7.10764783e-04 8.61077104e-04 1.90169737e-01 9.66225285e-04\n",
      " 2.27654725e-03 3.38459820e-01 6.79541030e-04 1.24083177e-04\n",
      " 2.84124108e-04 4.03881632e-03 5.31949801e-04 1.60932001e-02\n",
      " 9.14799341e-04 2.83199251e-02 6.71023212e-04 1.27117876e-02\n",
      " 7.01427553e-03 9.55033675e-03 3.79100084e-01 2.65820464e-03\n",
      " 9.90776898e-05 1.17157644e-03 4.74368630e-04 3.65944608e-04\n",
      " 5.42966707e-04 2.83559028e-04 1.08497974e-03 7.18118099e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 27 [0/106 (0%)]\tTrain Loss: 0.003887\n",
      "Train Epoch: 27 [10/106 (9%)]\tTrain Loss: 0.095071\n",
      "Train Epoch: 27 [20/106 (19%)]\tTrain Loss: 0.108449\n",
      "Train Epoch: 27 [30/106 (28%)]\tTrain Loss: 0.000562\n",
      "Train Epoch: 27 [40/106 (38%)]\tTrain Loss: 0.000632\n",
      "Train Epoch: 27 [50/106 (47%)]\tTrain Loss: 0.008041\n",
      "Train Epoch: 27 [60/106 (57%)]\tTrain Loss: 0.135089\n",
      "Train Epoch: 27 [70/106 (66%)]\tTrain Loss: 0.007278\n",
      "Train Epoch: 27 [80/106 (75%)]\tTrain Loss: 0.094253\n",
      "Train Epoch: 27 [90/106 (85%)]\tTrain Loss: 0.003744\n",
      "Train Epoch: 27 [100/106 (94%)]\tTrain Loss: 0.001944\n",
      "\n",
      "Train set: Average loss: 0.0009, Accuracy: 392/424 (92%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [7.07577507e-04 2.95186881e-03 3.51923372e-04 7.93099753e-04\n",
      " 5.90860937e-03 3.16046178e-04 4.59294394e-03 6.11313758e-03\n",
      " 4.92672843e-04 1.42849470e-03 1.41687947e-03 1.27491192e-04\n",
      " 1.00965763e-03 2.24408577e-03 2.60678623e-02 4.05957311e-04\n",
      " 4.85405151e-04 1.42283109e-03 3.34480341e-04 9.99809708e-03\n",
      " 1.33607618e-03 1.36606244e-03 1.26371402e-02 5.93653083e-01\n",
      " 1.35770964e-03 1.02442103e-02 9.92725372e-01 5.22388902e-04\n",
      " 4.54481167e-04 6.59633137e-04 4.95106466e-02 2.78530065e-02\n",
      " 4.79280427e-02 1.36669129e-04 5.63555281e-04 1.74327273e-04\n",
      " 5.65069378e-04 2.30520847e-03 1.91894977e-03 2.93026504e-04\n",
      " 4.48227773e-04 2.45399424e-04 4.10597958e-03 2.05777003e-03\n",
      " 8.96554533e-03 4.96060681e-03 2.15702429e-02 2.72831842e-02\n",
      " 8.45393777e-01 2.14477303e-04 9.41630781e-01 8.04954849e-04\n",
      " 8.13008577e-04 1.01956457e-03 1.04983361e-03 1.08168426e-03\n",
      " 1.31405599e-03 4.77240479e-04 8.95766658e-04 2.26720981e-03\n",
      " 9.82836425e-01 9.48477507e-01 9.91928458e-01 9.92820084e-01\n",
      " 6.84720874e-01 1.83782607e-01 8.32174905e-03 7.70639956e-01\n",
      " 6.12195320e-02 8.70676339e-01 1.48139559e-02 1.88434497e-02\n",
      " 4.37877476e-01 9.81288195e-01 9.63353395e-01 5.99767447e-01\n",
      " 4.99287874e-01 4.25513238e-01 9.89827514e-01 2.53176223e-02\n",
      " 1.53313518e-01 2.38030832e-02 9.93393838e-01 2.35932007e-01\n",
      " 1.36956410e-03 1.87071472e-01 4.08884510e-02 5.86600937e-02\n",
      " 1.91041781e-03 1.54198857e-03 2.37025321e-01 1.37037435e-03\n",
      " 3.75236245e-03 4.87997532e-01 9.40350990e-04 2.68893491e-04\n",
      " 5.01953764e-04 1.17991297e-02 6.82390004e-04 8.98615047e-02\n",
      " 1.22623646e-03 4.94917594e-02 7.90503866e-04 1.51476050e-02\n",
      " 6.48179324e-03 8.80869385e-03 3.37955654e-01 5.19305235e-03\n",
      " 1.81872558e-04 1.77753763e-03 9.41655482e-04 5.88215946e-04\n",
      " 7.01962272e-04 6.40198297e-04 1.95833994e-03 1.35630672e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 28 [0/106 (0%)]\tTrain Loss: 0.001374\n",
      "Train Epoch: 28 [10/106 (9%)]\tTrain Loss: 0.001234\n",
      "Train Epoch: 28 [20/106 (19%)]\tTrain Loss: 0.000876\n",
      "Train Epoch: 28 [30/106 (28%)]\tTrain Loss: 0.297107\n",
      "Train Epoch: 28 [40/106 (38%)]\tTrain Loss: 0.211910\n",
      "Train Epoch: 28 [50/106 (47%)]\tTrain Loss: 0.002458\n",
      "Train Epoch: 28 [60/106 (57%)]\tTrain Loss: 0.004477\n",
      "Train Epoch: 28 [70/106 (66%)]\tTrain Loss: 0.010224\n",
      "Train Epoch: 28 [80/106 (75%)]\tTrain Loss: 0.011298\n",
      "Train Epoch: 28 [90/106 (85%)]\tTrain Loss: 0.268341\n",
      "Train Epoch: 28 [100/106 (94%)]\tTrain Loss: 0.002642\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 403/424 (95%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.00776739e-03 4.14884789e-03 5.67811250e-04 1.29544281e-03\n",
      " 7.45580019e-03 5.66505420e-04 4.88338899e-03 1.00301662e-02\n",
      " 6.07139780e-04 2.57357047e-03 2.10206793e-03 2.46639829e-04\n",
      " 1.83118600e-03 3.45153827e-03 1.74697284e-02 5.40409121e-04\n",
      " 8.63285677e-04 1.74926210e-03 5.11972583e-04 7.73798721e-03\n",
      " 1.71571283e-03 1.53022911e-03 6.69518765e-03 4.56438750e-01\n",
      " 1.57235202e-03 7.59585947e-03 9.95508194e-01 7.40200048e-04\n",
      " 5.82618406e-04 8.88553972e-04 3.83652262e-02 2.31099222e-02\n",
      " 1.68449599e-02 2.58795306e-04 8.18281318e-04 3.04151297e-04\n",
      " 7.74163986e-04 2.91645830e-03 2.43161689e-03 4.88139223e-04\n",
      " 6.72487658e-04 3.93877388e-04 4.65982547e-03 2.10598344e-03\n",
      " 1.36517286e-02 7.65829766e-03 2.72130743e-02 3.79961058e-02\n",
      " 9.53762710e-01 4.65478079e-04 9.68765795e-01 1.21558178e-03\n",
      " 1.73064694e-03 1.22179557e-03 1.80953043e-03 1.37772399e-03\n",
      " 1.63929071e-03 7.23987236e-04 9.86278872e-04 3.89082893e-03\n",
      " 9.84214067e-01 8.79476130e-01 9.93003428e-01 9.95817840e-01\n",
      " 6.00706518e-01 1.52698353e-01 1.24714924e-02 5.85207522e-01\n",
      " 6.95765615e-02 5.84185839e-01 1.46719934e-02 1.75585561e-02\n",
      " 6.44136250e-01 9.87691164e-01 9.53172386e-01 2.45576769e-01\n",
      " 8.89810443e-01 4.78351653e-01 9.92763937e-01 3.53759974e-02\n",
      " 1.33528188e-01 2.39760689e-02 9.95626807e-01 2.18975201e-01\n",
      " 1.86424446e-03 2.49579772e-01 1.18363714e-02 1.34501988e-02\n",
      " 2.03442108e-03 1.76978996e-03 2.21933797e-01 2.01334944e-03\n",
      " 3.67235881e-03 5.01252651e-01 1.57083629e-03 4.39350930e-04\n",
      " 1.02014269e-03 8.44388548e-03 1.00946322e-03 5.20400517e-02\n",
      " 1.40456145e-03 8.57303590e-02 1.06579089e-03 1.37863364e-02\n",
      " 8.40214826e-03 1.24378912e-02 2.53671736e-01 6.45317556e-03\n",
      " 3.49159731e-04 2.71642371e-03 1.46613363e-03 9.50249494e-04\n",
      " 1.84553559e-03 9.94719099e-04 1.85338745e-03 1.06031750e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 29 [0/106 (0%)]\tTrain Loss: 0.000658\n",
      "Train Epoch: 29 [10/106 (9%)]\tTrain Loss: 0.002416\n",
      "Train Epoch: 29 [20/106 (19%)]\tTrain Loss: 0.000485\n",
      "Train Epoch: 29 [30/106 (28%)]\tTrain Loss: 0.072486\n",
      "Train Epoch: 29 [40/106 (38%)]\tTrain Loss: 0.000604\n",
      "Train Epoch: 29 [50/106 (47%)]\tTrain Loss: 0.000757\n",
      "Train Epoch: 29 [60/106 (57%)]\tTrain Loss: 0.001545\n",
      "Train Epoch: 29 [70/106 (66%)]\tTrain Loss: 0.005004\n",
      "Train Epoch: 29 [80/106 (75%)]\tTrain Loss: 0.024805\n",
      "Train Epoch: 29 [90/106 (85%)]\tTrain Loss: 0.001245\n",
      "Train Epoch: 29 [100/106 (94%)]\tTrain Loss: 0.006004\n",
      "\n",
      "Train set: Average loss: 0.0001, Accuracy: 395/424 (93%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [4.33562847e-04 2.09273398e-03 2.65359529e-04 6.05745707e-04\n",
      " 3.84321879e-03 2.25845302e-04 2.56816694e-03 3.80716240e-03\n",
      " 3.33170727e-04 8.69823911e-04 1.08580070e-03 1.06739986e-04\n",
      " 8.14474944e-04 1.28505076e-03 9.95202269e-03 2.85323360e-04\n",
      " 3.74496303e-04 8.96396814e-04 2.26175209e-04 4.99561010e-03\n",
      " 8.63939407e-04 7.98819354e-04 2.62845680e-03 1.49815574e-01\n",
      " 7.18035561e-04 3.84266954e-03 9.92879808e-01 3.20436637e-04\n",
      " 2.70553428e-04 4.14756098e-04 1.85707957e-02 1.22236330e-02\n",
      " 1.28455218e-02 1.11767273e-04 4.07979154e-04 1.24523038e-04\n",
      " 3.46644345e-04 1.68826804e-03 1.22280745e-03 2.16432163e-04\n",
      " 2.87984178e-04 1.81122872e-04 3.05206561e-03 1.34407729e-03\n",
      " 7.17472425e-03 3.21520027e-03 1.54596763e-02 2.24047694e-02\n",
      " 7.97903001e-01 1.41339362e-04 8.84898186e-01 6.46130589e-04\n",
      " 7.05954328e-04 7.36349612e-04 5.54878847e-04 7.03914033e-04\n",
      " 7.81849143e-04 3.65519605e-04 5.73503436e-04 1.87085953e-03\n",
      " 9.26261425e-01 6.38826251e-01 9.89569664e-01 9.93663192e-01\n",
      " 2.70565003e-01 4.02982496e-02 5.04096132e-03 2.90233672e-01\n",
      " 3.26160155e-02 2.90768355e-01 6.09371671e-03 8.62699281e-03\n",
      " 2.68559664e-01 9.32294428e-01 8.82387638e-01 1.25206351e-01\n",
      " 4.61252183e-01 2.74053693e-01 9.91254926e-01 1.43854655e-02\n",
      " 5.13481647e-02 8.26691277e-03 9.91308391e-01 9.24669728e-02\n",
      " 6.38531114e-04 1.04103819e-01 5.95244858e-03 6.59598038e-03\n",
      " 6.45385648e-04 7.68120226e-04 3.92255560e-02 8.52580881e-04\n",
      " 2.40310119e-03 7.29180574e-02 6.20459265e-04 1.85029814e-04\n",
      " 4.48733947e-04 4.28621331e-03 4.43326164e-04 1.43565107e-02\n",
      " 7.05418934e-04 1.89893171e-02 6.02020067e-04 7.40720239e-03\n",
      " 4.73855203e-03 5.85913565e-03 7.71658197e-02 3.22187808e-03\n",
      " 1.16189141e-04 1.09529181e-03 5.27185330e-04 3.33874079e-04\n",
      " 5.36532782e-04 4.01677244e-04 8.42233712e-04 5.90362586e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 30 [0/106 (0%)]\tTrain Loss: 0.043103\n",
      "Train Epoch: 30 [10/106 (9%)]\tTrain Loss: 0.005204\n",
      "Train Epoch: 30 [20/106 (19%)]\tTrain Loss: 0.004815\n",
      "Train Epoch: 30 [30/106 (28%)]\tTrain Loss: 0.000733\n",
      "Train Epoch: 30 [40/106 (38%)]\tTrain Loss: 0.000496\n",
      "Train Epoch: 30 [50/106 (47%)]\tTrain Loss: 0.009526\n",
      "Train Epoch: 30 [60/106 (57%)]\tTrain Loss: 0.000515\n",
      "Train Epoch: 30 [70/106 (66%)]\tTrain Loss: 0.003315\n",
      "Train Epoch: 30 [80/106 (75%)]\tTrain Loss: 0.000466\n",
      "Train Epoch: 30 [90/106 (85%)]\tTrain Loss: 0.098765\n",
      "Train Epoch: 30 [100/106 (94%)]\tTrain Loss: 0.015446\n",
      "\n",
      "Train set: Average loss: 0.0012, Accuracy: 397/424 (94%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.42853974e-04 1.22742262e-03 1.21303834e-04 3.06624046e-04\n",
      " 3.16950539e-03 9.54911011e-05 1.42946152e-03 3.37318331e-03\n",
      " 1.15878895e-04 4.14423557e-04 4.41456534e-04 3.94826311e-05\n",
      " 3.49189155e-04 8.40669440e-04 5.09519456e-03 9.14803022e-05\n",
      " 1.56181210e-04 3.44164786e-04 6.42043960e-05 1.39498874e-03\n",
      " 3.50970688e-04 2.98926170e-04 1.36416301e-03 2.36894786e-01\n",
      " 2.59407680e-04 1.47180399e-03 9.91901457e-01 1.05283500e-04\n",
      " 9.03734108e-05 1.72831904e-04 1.51005434e-02 9.29378904e-03\n",
      " 4.58672084e-03 3.51320014e-05 1.93934844e-04 4.43897297e-05\n",
      " 1.53962639e-04 8.49829288e-04 6.06433023e-04 6.39338032e-05\n",
      " 8.81983142e-05 4.74353801e-05 1.69408217e-03 5.11088583e-04\n",
      " 4.85361740e-03 1.76742009e-03 1.27886049e-02 1.97178666e-02\n",
      " 9.21789169e-01 5.22545597e-05 9.57532406e-01 3.03829904e-04\n",
      " 2.97304388e-04 3.05491616e-04 2.37826564e-04 2.93129851e-04\n",
      " 2.53845676e-04 1.37095558e-04 2.21178983e-04 8.88928596e-04\n",
      " 9.73419964e-01 7.45654702e-01 9.89774704e-01 9.93284941e-01\n",
      " 3.17183763e-01 4.89498861e-02 3.08404560e-03 2.99077958e-01\n",
      " 2.85090413e-02 3.35175127e-01 6.11659558e-03 7.79151823e-03\n",
      " 3.45420927e-01 9.78470087e-01 9.33353484e-01 9.73520279e-02\n",
      " 8.44047785e-01 3.72238129e-01 9.88254666e-01 1.01643978e-02\n",
      " 5.99799082e-02 6.35613641e-03 9.91171777e-01 7.49439001e-02\n",
      " 3.13485507e-04 1.37258798e-01 4.36189119e-03 3.97882704e-03\n",
      " 2.91066914e-04 3.13415600e-04 2.69578006e-02 2.93882069e-04\n",
      " 9.76103474e-04 2.25310639e-01 2.15884575e-04 5.52674937e-05\n",
      " 2.04745724e-04 1.37062953e-03 2.07784789e-04 9.51999426e-03\n",
      " 2.66250107e-04 3.22146118e-02 2.45286676e-04 3.84658156e-03\n",
      " 2.60741287e-03 3.36592295e-03 9.18144807e-02 1.37765566e-03\n",
      " 3.41774357e-05 5.30497171e-04 2.31592334e-04 1.38327261e-04\n",
      " 2.82475434e-04 1.43244964e-04 3.65991029e-04 1.82680902e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 9 TN= 57 FN= 47 FP= 3\n",
      "TP+FP 12\n",
      "precision 0.75\n",
      "recall 0.16071428571428573\n",
      "F1 0.26470588235294124\n",
      "acc 0.5689655172413793\n",
      "AUCp 0.5553571428571429\n",
      "AUC 0.7464285714285713\n",
      "\n",
      " The epoch is 30, average recall: 0.1607, average precision: 0.7500,average F1: 0.2647, average accuracy: 0.5690, average AUC: 0.7464\n",
      "Train Epoch: 31 [0/106 (0%)]\tTrain Loss: 0.090522\n",
      "Train Epoch: 31 [10/106 (9%)]\tTrain Loss: 0.005482\n",
      "Train Epoch: 31 [20/106 (19%)]\tTrain Loss: 0.000394\n",
      "Train Epoch: 31 [30/106 (28%)]\tTrain Loss: 0.000961\n",
      "Train Epoch: 31 [40/106 (38%)]\tTrain Loss: 0.003155\n",
      "Train Epoch: 31 [50/106 (47%)]\tTrain Loss: 0.116210\n",
      "Train Epoch: 31 [60/106 (57%)]\tTrain Loss: 0.000544\n",
      "Train Epoch: 31 [70/106 (66%)]\tTrain Loss: 0.001501\n",
      "Train Epoch: 31 [80/106 (75%)]\tTrain Loss: 0.001757\n",
      "Train Epoch: 31 [90/106 (85%)]\tTrain Loss: 0.006615\n",
      "Train Epoch: 31 [100/106 (94%)]\tTrain Loss: 0.000482\n",
      "\n",
      "Train set: Average loss: 0.0009, Accuracy: 396/424 (93%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.28265645e-03 7.84163084e-03 8.41485686e-04 2.08620657e-03\n",
      " 1.22417482e-02 6.06090471e-04 1.04045384e-02 1.77110061e-02\n",
      " 8.38378270e-04 5.07491315e-03 3.12646199e-03 3.82067665e-04\n",
      " 2.39582104e-03 5.59464283e-03 8.59422237e-02 1.24682498e-03\n",
      " 1.60503644e-03 4.89900913e-03 8.86149297e-04 2.48000845e-02\n",
      " 2.71831313e-03 3.69466026e-03 2.70854495e-02 8.37190926e-01\n",
      " 3.40695539e-03 1.49734924e-02 9.95618880e-01 1.19489443e-03\n",
      " 8.62256915e-04 1.23554794e-03 6.08763508e-02 5.49847297e-02\n",
      " 1.24341309e-01 4.23020101e-04 1.25622307e-03 4.59938339e-04\n",
      " 1.24433113e-03 5.50127495e-03 4.22587665e-03 8.10967525e-04\n",
      " 1.15257583e-03 6.64569496e-04 8.95780325e-03 4.05962439e-03\n",
      " 2.20364369e-02 1.38235874e-02 4.43773493e-02 6.09412082e-02\n",
      " 9.84007776e-01 4.61713003e-04 9.82810616e-01 1.48771855e-03\n",
      " 2.67509022e-03 2.01235898e-03 3.44611867e-03 2.46910658e-03\n",
      " 4.19333205e-03 1.21062470e-03 1.59739144e-03 6.34714076e-03\n",
      " 9.88082409e-01 9.68355358e-01 9.94562447e-01 9.95634615e-01\n",
      " 7.16393769e-01 3.13069463e-01 2.44934689e-02 8.46639395e-01\n",
      " 2.67280906e-01 9.75602508e-01 3.77316773e-02 6.32479489e-02\n",
      " 8.98949385e-01 9.92895782e-01 9.49810743e-01 8.90249074e-01\n",
      " 9.63680327e-01 8.75095189e-01 9.96565640e-01 8.19860995e-02\n",
      " 5.57443380e-01 5.66149019e-02 9.96535778e-01 2.93855876e-01\n",
      " 3.71176028e-03 5.23631454e-01 3.64192128e-02 4.62953225e-02\n",
      " 4.33172192e-03 3.89382034e-03 8.08144569e-01 4.93421732e-03\n",
      " 1.01460638e-02 8.83343518e-01 3.13576288e-03 7.45637284e-04\n",
      " 1.39539537e-03 8.92755315e-02 2.07062974e-03 2.02143848e-01\n",
      " 3.97942821e-03 1.76981986e-01 1.94444309e-03 4.78395671e-02\n",
      " 2.27803290e-02 2.29800995e-02 7.38853514e-01 1.44445794e-02\n",
      " 5.32133679e-04 3.41032259e-03 2.30305805e-03 1.95385120e-03\n",
      " 2.76117167e-03 1.56191282e-03 4.48891940e-03 3.06181260e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 32 [0/106 (0%)]\tTrain Loss: 0.001724\n",
      "Train Epoch: 32 [10/106 (9%)]\tTrain Loss: 0.322136\n",
      "Train Epoch: 32 [20/106 (19%)]\tTrain Loss: 0.005904\n",
      "Train Epoch: 32 [30/106 (28%)]\tTrain Loss: 0.000445\n",
      "Train Epoch: 32 [40/106 (38%)]\tTrain Loss: 0.000492\n",
      "Train Epoch: 32 [50/106 (47%)]\tTrain Loss: 0.009451\n",
      "Train Epoch: 32 [60/106 (57%)]\tTrain Loss: 0.000478\n",
      "Train Epoch: 32 [70/106 (66%)]\tTrain Loss: 0.005417\n",
      "Train Epoch: 32 [80/106 (75%)]\tTrain Loss: 0.005585\n",
      "Train Epoch: 32 [90/106 (85%)]\tTrain Loss: 0.000943\n",
      "Train Epoch: 32 [100/106 (94%)]\tTrain Loss: 0.000484\n",
      "\n",
      "Train set: Average loss: 0.0003, Accuracy: 402/424 (95%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [7.95111817e-04 3.87211191e-03 4.89066704e-04 1.23018073e-03\n",
      " 6.77957945e-03 4.16960247e-04 4.06190427e-03 1.15759112e-02\n",
      " 6.19149068e-04 3.12625710e-03 1.83019170e-03 2.55891500e-04\n",
      " 1.68216962e-03 3.11057060e-03 5.04285693e-02 7.13347516e-04\n",
      " 7.93309067e-04 3.83285480e-03 6.40318729e-04 1.16140544e-02\n",
      " 2.12718477e-03 2.04460090e-03 1.25735188e-02 8.15316260e-01\n",
      " 2.23076367e-03 8.29471182e-03 9.97026384e-01 1.00460974e-03\n",
      " 6.85715466e-04 8.65153677e-04 5.84996566e-02 7.14145526e-02\n",
      " 4.45596203e-02 2.17690904e-04 6.74700539e-04 2.94268975e-04\n",
      " 6.31978444e-04 3.58777470e-03 3.76645126e-03 8.01559945e-04\n",
      " 1.07981253e-03 6.22161140e-04 7.06394576e-03 2.19136593e-03\n",
      " 1.57140009e-02 7.16574676e-03 2.61311363e-02 4.81612459e-02\n",
      " 9.75293338e-01 3.57134704e-04 9.77366984e-01 7.69768958e-04\n",
      " 1.77983113e-03 1.27024262e-03 2.89086695e-03 1.35565153e-03\n",
      " 4.83507337e-03 7.61065399e-04 1.23950781e-03 3.57192219e-03\n",
      " 9.89998281e-01 9.64298248e-01 9.96460497e-01 9.96767402e-01\n",
      " 5.86193621e-01 5.37007809e-01 2.15464421e-02 8.92337918e-01\n",
      " 3.13702822e-01 9.52390492e-01 8.31062272e-02 1.90984264e-01\n",
      " 8.48180294e-01 9.92629290e-01 9.54727709e-01 9.18385923e-01\n",
      " 9.50201511e-01 7.48009026e-01 9.95893478e-01 6.89307600e-02\n",
      " 6.13922596e-01 3.49063687e-02 9.98015046e-01 5.82181692e-01\n",
      " 2.17653764e-03 5.07571101e-01 5.23083843e-02 1.09518006e-01\n",
      " 3.77593702e-03 1.80494902e-03 8.58743548e-01 3.06764920e-03\n",
      " 7.48152612e-03 3.89855653e-01 1.33119000e-03 5.32571867e-04\n",
      " 8.68841482e-04 1.66655973e-01 1.83881936e-03 7.53586590e-02\n",
      " 4.48152516e-03 3.44395965e-01 1.68156356e-03 4.38987203e-02\n",
      " 1.41300382e-02 1.81538537e-02 4.33299661e-01 7.07000121e-03\n",
      " 4.21198085e-04 2.35599070e-03 1.91569387e-03 1.33981567e-03\n",
      " 1.51830062e-03 1.09714328e-03 3.99004528e-03 2.91499845e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 33 [0/106 (0%)]\tTrain Loss: 0.000492\n",
      "Train Epoch: 33 [10/106 (9%)]\tTrain Loss: 0.004647\n",
      "Train Epoch: 33 [20/106 (19%)]\tTrain Loss: 0.000845\n",
      "Train Epoch: 33 [30/106 (28%)]\tTrain Loss: 0.000679\n",
      "Train Epoch: 33 [40/106 (38%)]\tTrain Loss: 0.003497\n",
      "Train Epoch: 33 [50/106 (47%)]\tTrain Loss: 0.181042\n",
      "Train Epoch: 33 [60/106 (57%)]\tTrain Loss: 0.185470\n",
      "Train Epoch: 33 [70/106 (66%)]\tTrain Loss: 0.065036\n",
      "Train Epoch: 33 [80/106 (75%)]\tTrain Loss: 0.000571\n",
      "Train Epoch: 33 [90/106 (85%)]\tTrain Loss: 0.255773\n",
      "Train Epoch: 33 [100/106 (94%)]\tTrain Loss: 0.002363\n",
      "\n",
      "Train set: Average loss: 0.0001, Accuracy: 398/424 (94%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.65810149e-04 1.83550350e-03 3.03999113e-04 6.20490639e-04\n",
      " 4.04928485e-03 2.85638002e-04 1.81850302e-03 5.80279622e-03\n",
      " 6.07809634e-04 1.24360900e-03 7.47351034e-04 1.77407332e-04\n",
      " 5.29023469e-04 1.78939267e-03 1.70015600e-02 3.08213144e-04\n",
      " 5.57507505e-04 1.50160398e-03 3.62936233e-04 3.41137312e-03\n",
      " 1.41113915e-03 9.16757388e-04 3.12466291e-03 6.13528676e-02\n",
      " 6.86125597e-04 2.31509726e-03 9.88866270e-01 4.48476087e-04\n",
      " 3.38371465e-04 3.65678832e-04 8.65213294e-03 6.12673163e-03\n",
      " 7.40789995e-03 1.19207587e-04 3.71236412e-04 1.65830570e-04\n",
      " 3.41256440e-04 1.77126646e-03 1.57274737e-03 2.44638795e-04\n",
      " 3.53723270e-04 2.10146813e-04 3.82546801e-03 1.26807264e-03\n",
      " 8.96113180e-03 2.34603812e-03 1.25795798e-02 1.88322645e-02\n",
      " 8.32565784e-01 1.97740272e-04 8.19455147e-01 5.70764882e-04\n",
      " 7.51187152e-04 6.86305808e-04 1.18691812e-03 6.99817843e-04\n",
      " 1.24352670e-03 3.22531821e-04 1.03832257e-03 1.63109996e-03\n",
      " 9.80781078e-01 8.59669089e-01 9.81415868e-01 9.91380274e-01\n",
      " 1.06751867e-01 1.66710261e-02 4.96900221e-03 1.07723646e-01\n",
      " 2.84892768e-02 3.30204189e-01 9.27102193e-03 1.27445525e-02\n",
      " 1.73300728e-01 8.93987477e-01 1.55605972e-01 1.18287891e-01\n",
      " 5.61691046e-01 4.64589417e-01 9.92193580e-01 8.60562362e-03\n",
      " 3.26815844e-02 1.34319812e-02 9.78630781e-01 3.44249606e-02\n",
      " 6.56200864e-04 1.08507000e-01 9.67094209e-03 9.03402269e-03\n",
      " 1.29922503e-03 8.23102542e-04 5.21947369e-02 6.66217995e-04\n",
      " 3.00846761e-03 9.38723236e-03 4.94462030e-04 2.10178681e-04\n",
      " 5.51623583e-04 5.09247836e-03 1.10122806e-03 6.20561605e-03\n",
      " 1.11694285e-03 1.98543966e-02 6.67679298e-04 4.67861257e-03\n",
      " 4.72075026e-03 7.73601746e-03 1.85830016e-02 2.74753687e-03\n",
      " 2.26417513e-04 1.02746766e-03 7.87409721e-04 4.71212523e-04\n",
      " 8.40229040e-04 2.90779542e-04 1.53423555e-03 8.93085613e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 34 [0/106 (0%)]\tTrain Loss: 0.001927\n",
      "Train Epoch: 34 [10/106 (9%)]\tTrain Loss: 0.017446\n",
      "Train Epoch: 34 [20/106 (19%)]\tTrain Loss: 0.001704\n",
      "Train Epoch: 34 [30/106 (28%)]\tTrain Loss: 0.000919\n",
      "Train Epoch: 34 [40/106 (38%)]\tTrain Loss: 0.000390\n",
      "Train Epoch: 34 [50/106 (47%)]\tTrain Loss: 0.003449\n",
      "Train Epoch: 34 [60/106 (57%)]\tTrain Loss: 0.124230\n",
      "Train Epoch: 34 [70/106 (66%)]\tTrain Loss: 0.000427\n",
      "Train Epoch: 34 [80/106 (75%)]\tTrain Loss: 0.000574\n",
      "Train Epoch: 34 [90/106 (85%)]\tTrain Loss: 0.007023\n",
      "Train Epoch: 34 [100/106 (94%)]\tTrain Loss: 0.002169\n",
      "\n",
      "Train set: Average loss: 0.0020, Accuracy: 396/424 (93%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.74038887e-05 2.13716921e-04 2.36242868e-05 1.80914085e-05\n",
      " 8.30020290e-05 7.85135853e-06 1.88930178e-04 1.15138944e-04\n",
      " 3.95846437e-05 5.19372516e-05 5.73237085e-05 6.99761586e-06\n",
      " 2.04581502e-05 4.36887640e-05 1.68455276e-03 8.43826729e-06\n",
      " 3.33095159e-05 1.70313360e-04 2.33061255e-05 2.70619150e-03\n",
      " 1.39748503e-04 4.97252076e-05 1.97557063e-04 5.57919964e-04\n",
      " 3.30640833e-05 4.62111493e-04 9.10364747e-01 3.91705253e-05\n",
      " 2.76876253e-05 2.26721804e-05 6.21363474e-03 1.52764504e-03\n",
      " 4.47806297e-03 6.48109744e-06 1.44527094e-05 6.48420701e-06\n",
      " 1.19224178e-05 1.15551622e-04 9.30353126e-05 9.64894207e-06\n",
      " 1.21185540e-05 8.21311369e-06 3.39022052e-04 7.61085030e-05\n",
      " 1.04974303e-03 7.62568452e-05 9.07754176e-04 2.33610696e-03\n",
      " 3.37068379e-01 4.79916162e-06 4.33586657e-01 9.56730128e-06\n",
      " 4.77624671e-05 2.41187990e-05 7.05662897e-05 4.97357760e-05\n",
      " 1.94376174e-04 1.83268767e-05 9.24424530e-05 1.32604982e-04\n",
      " 9.01152432e-01 7.87974298e-01 9.87968683e-01 9.89678144e-01\n",
      " 4.03755039e-01 9.26435832e-03 8.64314497e-04 5.15848041e-01\n",
      " 1.63288489e-02 6.40521109e-01 4.42573772e-04 3.38903803e-04\n",
      " 1.46938618e-02 6.34709537e-01 4.40986827e-02 7.78549314e-01\n",
      " 9.42236722e-01 6.94458187e-01 9.84083593e-01 4.55920259e-03\n",
      " 6.48195948e-03 1.07000163e-03 9.84138191e-01 1.43791232e-02\n",
      " 3.89184534e-05 1.76204033e-02 1.38082332e-03 8.98841070e-04\n",
      " 5.34973442e-05 5.25576652e-05 6.79504708e-04 2.64157698e-05\n",
      " 2.78449064e-04 6.64014893e-04 2.49615659e-05 1.05815043e-05\n",
      " 3.45123590e-05 2.17109406e-03 4.78358597e-05 7.14466034e-04\n",
      " 7.33644629e-05 7.17999460e-03 2.21161099e-05 1.49412733e-03\n",
      " 7.58436217e-04 5.14156418e-04 4.67951223e-03 1.41179626e-04\n",
      " 6.31255762e-06 6.91485329e-05 4.07724219e-05 1.78063801e-05\n",
      " 6.68396824e-05 2.15717082e-05 7.16439827e-05 5.85610978e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 35 [0/106 (0%)]\tTrain Loss: 0.000345\n",
      "Train Epoch: 35 [10/106 (9%)]\tTrain Loss: 0.000389\n",
      "Train Epoch: 35 [20/106 (19%)]\tTrain Loss: 0.002465\n",
      "Train Epoch: 35 [30/106 (28%)]\tTrain Loss: 0.001449\n",
      "Train Epoch: 35 [40/106 (38%)]\tTrain Loss: 0.001003\n",
      "Train Epoch: 35 [50/106 (47%)]\tTrain Loss: 0.000350\n",
      "Train Epoch: 35 [60/106 (57%)]\tTrain Loss: 0.000869\n",
      "Train Epoch: 35 [70/106 (66%)]\tTrain Loss: 0.234811\n",
      "Train Epoch: 35 [80/106 (75%)]\tTrain Loss: 0.007978\n",
      "Train Epoch: 35 [90/106 (85%)]\tTrain Loss: 0.000875\n",
      "Train Epoch: 35 [100/106 (94%)]\tTrain Loss: 0.157133\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 403/424 (95%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [5.82109424e-05 2.54546088e-04 6.01683605e-05 1.09263870e-04\n",
      " 4.22289246e-04 2.16013341e-05 1.87880883e-04 5.94583573e-04\n",
      " 6.99937445e-05 1.40952965e-04 1.27218809e-04 1.63782370e-05\n",
      " 7.92314459e-05 1.51659624e-04 6.22978841e-04 3.45303015e-05\n",
      " 5.13521263e-05 1.21750803e-04 2.58418841e-05 5.18748770e-04\n",
      " 1.69906241e-04 5.54112448e-05 8.84170804e-05 1.53420566e-04\n",
      " 5.62858368e-05 1.14771945e-04 1.74153864e-01 6.32819720e-05\n",
      " 1.07448293e-04 4.82019241e-05 7.57921301e-03 1.81089959e-03\n",
      " 1.58191717e-03 1.93018295e-05 4.84700977e-05 1.81298019e-05\n",
      " 2.48051856e-05 2.60979490e-04 1.38079529e-04 3.12849515e-05\n",
      " 3.57948084e-05 3.55331031e-05 7.39735377e-04 1.22334313e-04\n",
      " 5.90956479e-04 3.46927671e-04 3.12136137e-03 1.06909135e-02\n",
      " 3.87982368e-01 9.65632444e-06 1.21887088e-01 9.06592904e-05\n",
      " 5.31371625e-05 1.37452123e-04 9.24922788e-05 1.49055471e-04\n",
      " 2.36231950e-04 7.57127491e-05 1.33962429e-04 1.26059793e-04\n",
      " 5.11361659e-01 6.85943244e-03 9.80190814e-01 9.72364247e-01\n",
      " 4.73290635e-03 1.73775464e-01 3.24469246e-03 4.87308279e-02\n",
      " 1.79643575e-02 6.43280335e-03 1.52936683e-03 1.79816969e-03\n",
      " 9.29555111e-03 2.84523629e-02 1.27625093e-02 8.37062299e-03\n",
      " 9.91594970e-01 9.79436219e-01 9.26779449e-01 2.34451517e-03\n",
      " 3.60173732e-03 8.19299486e-04 2.72108853e-01 1.46151548e-02\n",
      " 4.44381403e-05 4.45683906e-03 1.48662875e-04 1.64663637e-04\n",
      " 2.87651328e-05 6.90032830e-05 1.02868980e-04 9.90932822e-05\n",
      " 5.72324498e-04 4.48319246e-04 4.46834129e-05 3.17365048e-05\n",
      " 5.22208356e-05 2.13184307e-04 4.44485224e-04 4.51962027e-04\n",
      " 9.34109921e-05 1.36639759e-01 1.49023530e-04 1.42186473e-03\n",
      " 7.95103435e-04 6.74430223e-04 3.29339085e-03 3.69258749e-04\n",
      " 1.34743614e-05 1.73863475e-04 1.40997334e-04 3.17325103e-05\n",
      " 9.38783778e-05 4.30155633e-05 2.10006794e-04 5.83515503e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 36 [0/106 (0%)]\tTrain Loss: 0.000275\n",
      "Train Epoch: 36 [10/106 (9%)]\tTrain Loss: 0.006415\n",
      "Train Epoch: 36 [20/106 (19%)]\tTrain Loss: 0.000300\n",
      "Train Epoch: 36 [30/106 (28%)]\tTrain Loss: 0.012014\n",
      "Train Epoch: 36 [40/106 (38%)]\tTrain Loss: 0.002294\n",
      "Train Epoch: 36 [50/106 (47%)]\tTrain Loss: 0.292129\n",
      "Train Epoch: 36 [60/106 (57%)]\tTrain Loss: 0.073782\n",
      "Train Epoch: 36 [70/106 (66%)]\tTrain Loss: 0.095946\n",
      "Train Epoch: 36 [80/106 (75%)]\tTrain Loss: 0.000635\n",
      "Train Epoch: 36 [90/106 (85%)]\tTrain Loss: 0.243961\n",
      "Train Epoch: 36 [100/106 (94%)]\tTrain Loss: 0.011198\n",
      "\n",
      "Train set: Average loss: 0.0001, Accuracy: 387/424 (91%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.89249666e-04 2.81738564e-02 1.87115162e-04 1.29976170e-03\n",
      " 5.38330339e-03 1.05938285e-04 3.43809240e-02 2.72734258e-02\n",
      " 3.85758723e-03 2.67068118e-01 3.99430189e-03 8.04869051e-04\n",
      " 3.07613588e-03 1.63546596e-02 5.58775425e-01 2.34832242e-03\n",
      " 1.18282449e-03 4.77665305e-01 4.68430546e-04 9.97482717e-01\n",
      " 1.11149348e-01 6.08743576e-04 6.04225099e-01 6.77422106e-01\n",
      " 1.16504915e-03 9.98506844e-01 9.93105471e-01 2.29487056e-03\n",
      " 4.58277063e-04 1.29582849e-03 9.87395287e-01 7.97481000e-01\n",
      " 9.31897640e-01 1.41036871e-04 3.07582610e-04 1.12863549e-04\n",
      " 1.76615416e-04 2.84051104e-03 1.25126087e-03 6.44118525e-04\n",
      " 8.72777833e-04 3.43940104e-04 5.97023815e-02 3.29803396e-03\n",
      " 1.08776586e-02 1.19622680e-03 2.13721171e-02 2.98313871e-02\n",
      " 9.76016819e-01 8.76386184e-05 9.91264999e-01 8.29134020e-04\n",
      " 6.50448166e-03 1.41253474e-03 8.06145836e-03 3.71208903e-03\n",
      " 5.98000765e-01 4.67639271e-04 2.09467281e-02 5.26585691e-02\n",
      " 9.05089378e-01 4.95096773e-01 9.90792274e-01 9.91172373e-01\n",
      " 9.95500386e-01 9.63310957e-01 1.87392175e-01 9.80175912e-01\n",
      " 9.55827296e-01 9.78388965e-01 2.13899072e-02 4.17308956e-02\n",
      " 9.71111834e-01 9.94485915e-01 9.96045411e-01 9.93236482e-01\n",
      " 9.98582482e-01 9.99895334e-01 9.82877731e-01 8.43307197e-01\n",
      " 8.82255256e-01 9.94934857e-01 9.95789587e-01 9.82820749e-01\n",
      " 6.64440682e-04 8.88358057e-01 7.79626295e-02 4.14781496e-02\n",
      " 1.41159026e-02 3.25892214e-03 9.85464990e-01 2.16493662e-03\n",
      " 5.80918975e-03 9.85762835e-01 1.60852307e-03 7.59479823e-04\n",
      " 4.26413055e-04 4.54386473e-02 1.92800048e-03 3.80824178e-01\n",
      " 8.29585246e-04 3.37118089e-01 1.35371287e-03 7.60680377e-01\n",
      " 1.72812734e-02 5.39411485e-01 5.77748656e-01 2.52051279e-03\n",
      " 4.00972902e-04 1.59802334e-03 1.50194869e-03 7.61673029e-04\n",
      " 7.95549303e-02 1.17088407e-02 5.11113415e-03 7.95961649e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 37 [0/106 (0%)]\tTrain Loss: 0.005876\n",
      "Train Epoch: 37 [10/106 (9%)]\tTrain Loss: 0.008886\n",
      "Train Epoch: 37 [20/106 (19%)]\tTrain Loss: 0.000501\n",
      "Train Epoch: 37 [30/106 (28%)]\tTrain Loss: 0.002153\n",
      "Train Epoch: 37 [40/106 (38%)]\tTrain Loss: 0.006893\n",
      "Train Epoch: 37 [50/106 (47%)]\tTrain Loss: 0.002351\n",
      "Train Epoch: 37 [60/106 (57%)]\tTrain Loss: 0.018329\n",
      "Train Epoch: 37 [70/106 (66%)]\tTrain Loss: 0.003815\n",
      "Train Epoch: 37 [80/106 (75%)]\tTrain Loss: 0.001365\n",
      "Train Epoch: 37 [90/106 (85%)]\tTrain Loss: 0.004736\n",
      "Train Epoch: 37 [100/106 (94%)]\tTrain Loss: 0.005090\n",
      "\n",
      "Train set: Average loss: 0.0001, Accuracy: 395/424 (93%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.61018921e-04 9.56138596e-03 2.18644651e-04 7.85120763e-04\n",
      " 3.55382916e-03 4.19162934e-05 1.22710261e-02 6.33351132e-03\n",
      " 4.32578672e-04 3.50017543e-03 3.47879366e-04 1.74627887e-04\n",
      " 5.00543800e-04 5.06143691e-03 1.27490580e-01 6.00682048e-04\n",
      " 1.97043133e-04 2.84144990e-02 1.95803674e-04 6.15546200e-03\n",
      " 4.63791948e-04 3.90325236e-04 3.17340228e-03 2.98012421e-03\n",
      " 3.81685386e-04 3.00204428e-03 9.32556629e-01 6.93024194e-04\n",
      " 6.19614788e-04 7.30453583e-04 2.18655959e-01 1.22876531e-02\n",
      " 2.12127976e-02 8.35802348e-05 2.67863070e-04 2.19019363e-04\n",
      " 1.80930234e-04 1.21718354e-03 1.98419509e-03 3.31832387e-04\n",
      " 3.53513227e-04 1.89521073e-04 2.18968140e-03 1.58352824e-03\n",
      " 3.21548618e-03 9.46971937e-04 1.03267431e-02 1.33844465e-02\n",
      " 7.54575968e-01 1.62405442e-04 6.62693262e-01 2.47917167e-04\n",
      " 4.96986555e-04 8.71853204e-04 1.21429248e-03 5.50371886e-04\n",
      " 6.34540617e-03 2.60224420e-04 1.05014071e-03 7.00691424e-04\n",
      " 8.80664468e-01 4.29279029e-01 9.68048751e-01 9.62338209e-01\n",
      " 8.83212090e-01 3.63552920e-03 6.85313391e-03 8.31695557e-01\n",
      " 2.76121587e-01 8.66689347e-03 1.29945222e-02 9.16677713e-03\n",
      " 2.62369096e-01 9.03212011e-01 6.36054158e-01 8.89513075e-01\n",
      " 9.96310413e-01 9.84862626e-01 9.99469101e-01 3.22337672e-02\n",
      " 1.13397747e-01 9.94495675e-02 9.64199603e-01 6.16233647e-01\n",
      " 7.37695256e-04 2.04070881e-01 1.44775528e-02 4.45153378e-03\n",
      " 4.82079166e-04 1.68357149e-03 2.59135780e-03 2.32829305e-04\n",
      " 7.49577070e-04 1.38934073e-03 2.30220307e-04 8.17082109e-05\n",
      " 1.78416187e-04 1.64780964e-03 1.11875194e-03 8.41054425e-04\n",
      " 8.54754762e-04 5.25112711e-02 1.05578429e-03 1.68337754e-03\n",
      " 4.29271813e-03 4.90679406e-02 3.31912283e-03 1.05545041e-03\n",
      " 2.68511270e-04 7.40427233e-04 5.02251962e-04 1.76994057e-04\n",
      " 3.14576784e-04 1.03670958e-04 3.87003040e-03 9.16413788e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 38 [0/106 (0%)]\tTrain Loss: 0.014157\n",
      "Train Epoch: 38 [10/106 (9%)]\tTrain Loss: 0.000834\n",
      "Train Epoch: 38 [20/106 (19%)]\tTrain Loss: 0.284798\n",
      "Train Epoch: 38 [30/106 (28%)]\tTrain Loss: 0.000241\n",
      "Train Epoch: 38 [40/106 (38%)]\tTrain Loss: 0.002461\n",
      "Train Epoch: 38 [50/106 (47%)]\tTrain Loss: 0.000205\n",
      "Train Epoch: 38 [60/106 (57%)]\tTrain Loss: 0.000602\n",
      "Train Epoch: 38 [70/106 (66%)]\tTrain Loss: 0.007244\n",
      "Train Epoch: 38 [80/106 (75%)]\tTrain Loss: 0.000785\n",
      "Train Epoch: 38 [90/106 (85%)]\tTrain Loss: 0.008034\n",
      "Train Epoch: 38 [100/106 (94%)]\tTrain Loss: 0.004324\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 393/424 (93%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [8.72306373e-06 8.79366344e-05 4.65909034e-06 1.27703761e-05\n",
      " 6.19952552e-05 5.02888224e-06 5.70098127e-05 2.26286531e-04\n",
      " 1.40505304e-04 4.84874297e-04 2.86655086e-05 2.64028768e-05\n",
      " 1.29328409e-05 4.00396275e-05 4.36752383e-03 1.62883280e-05\n",
      " 1.01532996e-05 1.04139827e-03 2.28047793e-06 2.44534031e-01\n",
      " 5.49337274e-05 2.23659008e-05 6.44258223e-04 1.45689794e-03\n",
      " 8.47767114e-06 1.01373373e-02 9.96578038e-01 9.72705602e-05\n",
      " 8.37114112e-06 1.86478173e-05 5.95462462e-03 9.72105248e-04\n",
      " 9.39581805e-05 9.20115508e-06 3.24896464e-05 3.36017661e-06\n",
      " 5.23309018e-06 1.52041132e-04 2.40700901e-04 4.99691560e-05\n",
      " 3.65038031e-05 2.04939461e-05 9.22771433e-05 3.00233642e-05\n",
      " 7.98749024e-05 2.92655623e-05 4.58977855e-04 1.32604816e-03\n",
      " 5.68170287e-03 2.01291368e-05 3.85662355e-03 1.19345677e-05\n",
      " 8.77610510e-05 3.03166871e-05 6.21909494e-05 5.18984889e-05\n",
      " 4.74938170e-05 9.07613776e-06 1.70211104e-04 1.04693929e-04\n",
      " 7.50678300e-04 3.18907289e-04 4.64224815e-01 3.76822323e-01\n",
      " 3.33296746e-01 5.90670679e-04 1.59009505e-04 6.85827911e-01\n",
      " 1.53489481e-03 1.67020387e-03 5.67050010e-04 3.41342733e-04\n",
      " 3.39999199e-02 3.79562885e-01 7.80951023e-01 1.98356304e-02\n",
      " 9.89463568e-01 9.41151142e-01 9.61804748e-01 2.42568017e-03\n",
      " 4.50100703e-03 1.41974986e-02 8.03878248e-01 3.46682698e-01\n",
      " 1.28847068e-05 4.17792890e-03 1.37409923e-04 6.35525736e-04\n",
      " 5.84100089e-05 1.55856425e-04 8.03702220e-04 1.50902852e-05\n",
      " 9.89234322e-05 1.15735903e-01 5.00675123e-05 9.17278521e-06\n",
      " 8.69448468e-06 3.49579565e-03 1.73444751e-05 7.74532855e-01\n",
      " 8.56728220e-05 9.60844802e-04 7.20655880e-05 2.49088102e-04\n",
      " 4.86638193e-04 4.67486819e-03 2.49400735e-03 2.18262459e-04\n",
      " 2.10690378e-05 1.23511112e-04 1.68719664e-04 1.31236622e-04\n",
      " 4.78148431e-04 2.07790272e-05 1.10759785e-04 1.98019130e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 39 [0/106 (0%)]\tTrain Loss: 0.002715\n",
      "Train Epoch: 39 [10/106 (9%)]\tTrain Loss: 0.000597\n",
      "Train Epoch: 39 [20/106 (19%)]\tTrain Loss: 0.001447\n",
      "Train Epoch: 39 [30/106 (28%)]\tTrain Loss: 0.182656\n",
      "Train Epoch: 39 [40/106 (38%)]\tTrain Loss: 0.002269\n",
      "Train Epoch: 39 [50/106 (47%)]\tTrain Loss: 0.000373\n",
      "Train Epoch: 39 [60/106 (57%)]\tTrain Loss: 0.000412\n",
      "Train Epoch: 39 [70/106 (66%)]\tTrain Loss: 0.000857\n",
      "Train Epoch: 39 [80/106 (75%)]\tTrain Loss: 0.000343\n",
      "Train Epoch: 39 [90/106 (85%)]\tTrain Loss: 0.000755\n",
      "Train Epoch: 39 [100/106 (94%)]\tTrain Loss: 0.065926\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 403/424 (95%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.97660037e-04 1.59059954e-03 1.19597098e-04 2.86030961e-04\n",
      " 9.82620055e-04 5.93945406e-05 1.74493447e-03 2.97703012e-03\n",
      " 4.78578033e-03 1.83985487e-03 6.00402243e-03 7.59179311e-05\n",
      " 6.57025957e-03 1.04177441e-03 3.12204391e-01 5.88194060e-04\n",
      " 1.30242470e-03 7.48819718e-03 3.47648747e-04 2.71144629e-01\n",
      " 1.42001209e-03 7.17840972e-04 9.82534349e-01 9.40953910e-01\n",
      " 1.72310835e-03 9.57758904e-01 9.99201596e-01 2.47265468e-03\n",
      " 3.60102567e-04 8.04710260e-04 9.70427811e-01 5.66395707e-02\n",
      " 4.30959642e-01 2.49593984e-04 3.77434422e-04 9.41807375e-05\n",
      " 3.01984255e-04 9.00480081e-04 9.49034351e-04 3.34544166e-04\n",
      " 6.68826571e-04 4.27696476e-04 4.91341401e-04 1.71674450e-03\n",
      " 2.32044980e-03 7.73794344e-03 8.34661722e-03 4.60787565e-02\n",
      " 9.95053470e-01 5.03217627e-04 9.92474020e-01 6.33954696e-05\n",
      " 4.47477709e-04 1.41096290e-03 1.13016516e-02 1.00480113e-03\n",
      " 2.03076601e-01 2.70572025e-04 2.26539746e-03 1.33674464e-03\n",
      " 9.73782003e-01 1.52747840e-01 9.99179065e-01 9.97084916e-01\n",
      " 9.99561489e-01 9.92781579e-01 4.45658900e-03 9.96131539e-01\n",
      " 8.87664780e-02 9.90390778e-01 4.76271799e-03 2.74808705e-03\n",
      " 8.37872684e-01 9.99353826e-01 9.99447525e-01 9.98459697e-01\n",
      " 9.99802530e-01 9.98256505e-01 9.99958634e-01 8.50027613e-03\n",
      " 8.51563334e-01 2.45112509e-01 9.98642743e-01 9.95769262e-01\n",
      " 1.08074280e-03 5.43897271e-01 6.42094063e-03 4.27283980e-02\n",
      " 9.43909574e-04 2.70934124e-03 9.89632249e-01 1.21285189e-02\n",
      " 7.31577631e-04 9.99235392e-01 7.06259767e-03 6.46860688e-04\n",
      " 1.59362724e-04 5.10284789e-02 6.12715725e-04 9.98773396e-01\n",
      " 8.45988689e-04 1.23266920e-01 5.23152703e-04 7.09309354e-02\n",
      " 7.88831860e-02 8.89498889e-02 3.71139616e-01 5.00624835e-01\n",
      " 5.71224722e-04 9.79233831e-02 4.64251544e-03 9.04525165e-04\n",
      " 7.00438991e-02 6.58285394e-02 1.49507658e-03 3.60150158e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 40 [0/106 (0%)]\tTrain Loss: 0.103003\n",
      "Train Epoch: 40 [10/106 (9%)]\tTrain Loss: 0.003081\n",
      "Train Epoch: 40 [20/106 (19%)]\tTrain Loss: 0.001134\n",
      "Train Epoch: 40 [30/106 (28%)]\tTrain Loss: 0.086620\n",
      "Train Epoch: 40 [40/106 (38%)]\tTrain Loss: 0.235935\n",
      "Train Epoch: 40 [50/106 (47%)]\tTrain Loss: 0.015992\n",
      "Train Epoch: 40 [60/106 (57%)]\tTrain Loss: 0.000204\n",
      "Train Epoch: 40 [70/106 (66%)]\tTrain Loss: 0.000276\n",
      "Train Epoch: 40 [80/106 (75%)]\tTrain Loss: 0.002949\n",
      "Train Epoch: 40 [90/106 (85%)]\tTrain Loss: 0.189268\n",
      "Train Epoch: 40 [100/106 (94%)]\tTrain Loss: 0.024521\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 399/424 (94%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [7.23030025e-05 5.86583337e-04 3.02666831e-05 8.56221886e-05\n",
      " 2.68196047e-04 3.19318315e-05 7.53145374e-04 4.80416988e-04\n",
      " 1.61551137e-03 1.96353043e-03 3.10392905e-04 3.64721018e-05\n",
      " 2.87317496e-04 1.07964850e-03 4.53450456e-02 6.61169121e-04\n",
      " 6.51062117e-04 9.42943811e-01 1.39994430e-04 9.22942638e-01\n",
      " 1.77185331e-03 4.08369640e-04 9.85771179e-01 9.97965336e-01\n",
      " 6.69121800e-04 9.95197713e-01 9.97365057e-01 3.28165363e-03\n",
      " 1.01358746e-04 1.05392384e-04 9.41278815e-01 7.12280869e-02\n",
      " 2.88442448e-02 1.26319891e-03 2.17270368e-04 5.21262555e-05\n",
      " 1.45513186e-04 5.45414572e-04 1.52101880e-03 5.31236641e-03\n",
      " 1.03422478e-02 3.38817411e-03 6.62080303e-04 9.52227158e-04\n",
      " 2.44405179e-04 2.66017654e-04 1.31742819e-03 2.56957719e-03\n",
      " 9.81642008e-01 4.99238260e-04 9.76111889e-01 2.56073345e-05\n",
      " 7.86211313e-05 1.63706805e-04 1.03295604e-02 2.61159439e-04\n",
      " 1.40587971e-01 6.29347414e-05 4.10106790e-04 1.45885249e-04\n",
      " 9.90673125e-01 9.92856741e-01 9.95631218e-01 9.95824575e-01\n",
      " 9.82394278e-01 9.65281188e-01 2.07823049e-03 9.91185963e-01\n",
      " 2.58493155e-01 9.95293200e-01 8.28621864e-01 8.88574481e-01\n",
      " 7.20877588e-01 9.96617615e-01 9.92252529e-01 7.88611591e-01\n",
      " 5.36285698e-01 2.17291936e-01 9.89781022e-01 1.07916500e-02\n",
      " 9.74843264e-01 9.53252971e-01 9.98400986e-01 9.85827029e-01\n",
      " 1.35579717e-03 8.41213405e-01 1.04087107e-01 8.53890087e-03\n",
      " 5.80348400e-03 3.48911621e-03 9.90706146e-01 4.17380454e-03\n",
      " 4.01547266e-04 9.97822046e-01 9.41196270e-03 4.39902040e-04\n",
      " 5.76695711e-05 9.78984535e-01 6.88282948e-04 9.94735777e-01\n",
      " 9.20345843e-01 9.83362794e-01 5.63467434e-03 4.39067781e-02\n",
      " 4.29619044e-01 6.11040890e-01 9.85208750e-01 6.39637589e-01\n",
      " 9.85924527e-03 8.47401768e-02 3.65342140e-01 1.03001855e-02\n",
      " 9.91997421e-01 1.73525084e-02 3.91747057e-03 8.76630936e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 13 TN= 57 FN= 43 FP= 3\n",
      "TP+FP 16\n",
      "precision 0.8125\n",
      "recall 0.23214285714285715\n",
      "F1 0.3611111111111111\n",
      "acc 0.603448275862069\n",
      "AUCp 0.5910714285714286\n",
      "AUC 0.7821428571428571\n",
      "\n",
      " The epoch is 40, average recall: 0.2321, average precision: 0.8125,average F1: 0.3611, average accuracy: 0.6034, average AUC: 0.7821\n",
      "Train Epoch: 41 [0/106 (0%)]\tTrain Loss: 0.000736\n",
      "Train Epoch: 41 [10/106 (9%)]\tTrain Loss: 0.002655\n",
      "Train Epoch: 41 [20/106 (19%)]\tTrain Loss: 0.000894\n",
      "Train Epoch: 41 [30/106 (28%)]\tTrain Loss: 0.274441\n",
      "Train Epoch: 41 [40/106 (38%)]\tTrain Loss: 0.000368\n",
      "Train Epoch: 41 [50/106 (47%)]\tTrain Loss: 0.001867\n",
      "Train Epoch: 41 [60/106 (57%)]\tTrain Loss: 0.002476\n",
      "Train Epoch: 41 [70/106 (66%)]\tTrain Loss: 0.065764\n",
      "Train Epoch: 41 [80/106 (75%)]\tTrain Loss: 0.054947\n",
      "Train Epoch: 41 [90/106 (85%)]\tTrain Loss: 0.003210\n",
      "Train Epoch: 41 [100/106 (94%)]\tTrain Loss: 0.162085\n",
      "\n",
      "Train set: Average loss: 0.0002, Accuracy: 393/424 (93%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.89937923e-05 2.09625490e-04 2.26105112e-05 2.41392081e-05\n",
      " 6.80437806e-05 7.65030836e-06 7.11576940e-05 6.35687975e-05\n",
      " 4.07725020e-05 8.50061697e-05 7.85476223e-05 3.64207608e-06\n",
      " 1.23068618e-04 1.32832793e-04 1.55624095e-02 1.15726289e-04\n",
      " 1.16300318e-04 1.22941658e-02 9.08979655e-06 9.47609425e-01\n",
      " 2.07962774e-04 1.27573905e-04 9.13624346e-01 9.48822916e-01\n",
      " 1.20758406e-04 5.37720509e-03 9.99650836e-01 1.67248625e-04\n",
      " 2.52150039e-05 2.76905557e-05 4.54737306e-01 2.92984862e-03\n",
      " 5.51656261e-03 3.78755576e-05 1.83423326e-05 8.51288132e-06\n",
      " 2.94093115e-05 1.48256600e-04 1.79383889e-04 1.05139893e-03\n",
      " 3.50028835e-02 1.33397611e-04 2.86589464e-04 6.02964137e-04\n",
      " 1.06887150e-04 1.47550891e-04 7.04809790e-04 2.17814348e-03\n",
      " 9.97206748e-01 1.97572317e-05 9.92965221e-01 8.74421039e-06\n",
      " 1.26996192e-05 2.28850386e-05 2.15134351e-04 1.33085559e-04\n",
      " 4.41097072e-04 9.37529512e-06 8.17807813e-05 4.17305164e-05\n",
      " 5.56126311e-02 1.20053091e-03 9.90051091e-01 9.83283043e-01\n",
      " 7.36113563e-02 9.25929904e-01 3.84398358e-04 8.73872399e-01\n",
      " 3.26486677e-02 9.98894989e-01 4.80890786e-03 1.39280826e-01\n",
      " 9.73503113e-01 9.99710739e-01 9.73096788e-01 1.56521108e-02\n",
      " 8.75302613e-01 9.71402109e-01 9.98396575e-01 3.45412060e-03\n",
      " 9.66266751e-01 2.11715683e-01 9.99655485e-01 8.93682420e-01\n",
      " 1.20262092e-04 3.24456692e-02 3.39609862e-04 4.17144707e-04\n",
      " 3.97520780e-05 3.10208037e-04 1.03216723e-03 9.96209928e-05\n",
      " 1.45872167e-04 9.98784363e-01 3.70385183e-04 2.33643877e-05\n",
      " 1.26298110e-05 7.03789201e-03 6.22670850e-05 9.97925162e-01\n",
      " 2.56653786e-01 9.95639801e-01 2.91242293e-04 2.67316611e-03\n",
      " 2.41919011e-02 4.08956641e-03 9.99350131e-01 9.83489990e-01\n",
      " 1.24371276e-04 1.73769682e-03 2.62463954e-03 3.74937867e-04\n",
      " 6.30644709e-03 1.38537297e-02 4.16532363e-04 4.05624422e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 42 [0/106 (0%)]\tTrain Loss: 0.000870\n",
      "Train Epoch: 42 [10/106 (9%)]\tTrain Loss: 0.058976\n",
      "Train Epoch: 42 [20/106 (19%)]\tTrain Loss: 0.046314\n",
      "Train Epoch: 42 [30/106 (28%)]\tTrain Loss: 0.003386\n",
      "Train Epoch: 42 [40/106 (38%)]\tTrain Loss: 0.000240\n",
      "Train Epoch: 42 [50/106 (47%)]\tTrain Loss: 0.000843\n",
      "Train Epoch: 42 [60/106 (57%)]\tTrain Loss: 0.000234\n",
      "Train Epoch: 42 [70/106 (66%)]\tTrain Loss: 0.000180\n",
      "Train Epoch: 42 [80/106 (75%)]\tTrain Loss: 0.000216\n",
      "Train Epoch: 42 [90/106 (85%)]\tTrain Loss: 0.291730\n",
      "Train Epoch: 42 [100/106 (94%)]\tTrain Loss: 0.044293\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 397/424 (94%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.61626592e-05 3.67543806e-04 2.79012347e-05 4.12139307e-05\n",
      " 1.55650734e-04 1.32500891e-05 6.53916359e-05 1.57353425e-04\n",
      " 5.45537296e-05 1.39673275e-03 3.40451661e-05 1.63834557e-05\n",
      " 2.24707474e-05 2.19857742e-04 1.64154395e-02 5.71218043e-05\n",
      " 5.17956469e-05 5.49847865e-03 3.27908638e-05 1.46952897e-01\n",
      " 6.65389642e-04 7.73356296e-05 2.95715760e-02 7.45109200e-01\n",
      " 9.13300319e-05 1.71956345e-02 9.93406057e-01 3.12760076e-03\n",
      " 4.13648122e-05 4.24752980e-05 9.11641836e-01 5.96439792e-03\n",
      " 1.91136918e-04 1.38855557e-05 2.15306445e-05 2.84218313e-05\n",
      " 3.00341890e-05 1.83800337e-04 3.03201406e-04 1.66195270e-04\n",
      " 3.50757997e-04 5.95020283e-05 6.87740045e-04 4.69229155e-04\n",
      " 8.81559317e-05 7.18010197e-05 5.00335591e-04 1.55108841e-03\n",
      " 9.85911012e-01 4.42517303e-05 9.65711415e-01 1.55504076e-05\n",
      " 6.73477625e-05 2.43440336e-05 1.19302647e-04 9.13362164e-05\n",
      " 2.79875123e-03 1.63859859e-05 1.43316720e-04 8.33398080e-05\n",
      " 2.86928564e-01 9.45890974e-03 9.88328278e-01 9.82624710e-01\n",
      " 9.63427663e-01 1.45548750e-02 1.42973848e-03 9.88462746e-01\n",
      " 2.31850564e-01 3.49743702e-02 3.09208815e-04 3.15823592e-04\n",
      " 8.79969895e-01 9.63843465e-01 9.74343002e-01 8.75681818e-01\n",
      " 9.82819498e-01 9.91863608e-01 9.85524118e-01 1.60676092e-02\n",
      " 3.13252397e-02 2.31209993e-01 9.97103751e-01 7.59117007e-01\n",
      " 3.38325262e-05 2.79689163e-01 3.82784219e-03 4.71921591e-03\n",
      " 1.27041465e-04 3.84878484e-04 7.49785453e-04 3.77353099e-05\n",
      " 3.37960082e-04 9.87084210e-01 3.60083504e-05 2.73919304e-05\n",
      " 2.98119830e-05 2.16302788e-03 1.23627891e-04 1.18645793e-03\n",
      " 1.77044538e-03 9.37999249e-01 9.04370318e-05 1.51497166e-04\n",
      " 2.71970202e-04 2.27598648e-04 2.62758695e-03 3.18783277e-04\n",
      " 5.52007077e-05 1.36574949e-04 3.87248292e-04 3.46747111e-04\n",
      " 2.10289916e-04 5.13535961e-05 1.35232913e-04 3.14175501e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 43 [0/106 (0%)]\tTrain Loss: 0.001830\n",
      "Train Epoch: 43 [10/106 (9%)]\tTrain Loss: 0.319053\n",
      "Train Epoch: 43 [20/106 (19%)]\tTrain Loss: 0.000238\n",
      "Train Epoch: 43 [30/106 (28%)]\tTrain Loss: 0.001392\n",
      "Train Epoch: 43 [40/106 (38%)]\tTrain Loss: 0.001249\n",
      "Train Epoch: 43 [50/106 (47%)]\tTrain Loss: 0.000892\n",
      "Train Epoch: 43 [60/106 (57%)]\tTrain Loss: 0.347891\n",
      "Train Epoch: 43 [70/106 (66%)]\tTrain Loss: 0.000280\n",
      "Train Epoch: 43 [80/106 (75%)]\tTrain Loss: 0.002710\n",
      "Train Epoch: 43 [90/106 (85%)]\tTrain Loss: 0.001825\n",
      "Train Epoch: 43 [100/106 (94%)]\tTrain Loss: 0.000122\n",
      "\n",
      "Train set: Average loss: 0.0022, Accuracy: 401/424 (95%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.81890350e-05 1.56817026e-04 2.99982639e-05 2.73107853e-05\n",
      " 7.13497138e-05 1.48297104e-05 6.61806407e-05 8.55852923e-05\n",
      " 3.23193744e-05 1.00865087e-04 2.71536119e-05 9.62685408e-06\n",
      " 2.11987299e-05 3.58694015e-05 1.06223300e-03 3.01266718e-05\n",
      " 3.90657369e-05 1.01504789e-04 2.64044611e-05 2.47019925e-03\n",
      " 7.98306646e-05 5.14835338e-05 4.72613238e-03 1.84718240e-03\n",
      " 5.08081721e-05 7.42090109e-04 9.94185388e-01 1.44624180e-04\n",
      " 4.72394640e-05 4.56758316e-05 9.27667737e-01 3.80464597e-03\n",
      " 7.67139602e-04 1.74689576e-05 2.21089449e-05 1.12792850e-05\n",
      " 2.17445449e-05 7.22336772e-05 9.66761509e-05 4.31684348e-05\n",
      " 8.86720809e-05 3.23027671e-05 1.55349451e-04 1.65806210e-04\n",
      " 6.01677893e-05 6.70078443e-05 4.99162183e-04 6.86660758e-04\n",
      " 7.52442122e-01 2.03065392e-05 3.60775441e-01 2.07963330e-05\n",
      " 2.23530660e-05 2.94980164e-05 4.61637901e-05 6.51972441e-05\n",
      " 1.68747822e-04 1.44317873e-05 4.57189708e-05 3.74364172e-05\n",
      " 2.90570945e-01 4.67108004e-03 9.91880834e-01 9.87159967e-01\n",
      " 5.42779565e-01 5.88779920e-04 1.31579829e-04 9.82628703e-01\n",
      " 4.30477643e-03 5.07701263e-02 2.06814322e-04 4.51442524e-04\n",
      " 1.96858570e-02 9.69661772e-01 9.86590147e-01 8.68443251e-01\n",
      " 8.13912213e-01 7.95261919e-01 9.98275161e-01 1.07903627e-03\n",
      " 5.90199642e-02 8.39060452e-03 9.98048425e-01 8.89082849e-01\n",
      " 3.74306001e-05 1.31772934e-02 5.91415795e-04 6.04055880e-04\n",
      " 3.95841125e-05 1.02076709e-04 1.69938008e-04 3.59064397e-05\n",
      " 8.44464303e-05 5.51418699e-02 2.93823450e-05 1.13205697e-05\n",
      " 1.49482867e-05 4.67373931e-04 7.90455742e-05 1.31070390e-02\n",
      " 1.17695541e-03 6.33616865e-01 7.59062750e-05 1.21961588e-04\n",
      " 2.28641147e-04 1.87051861e-04 3.53816012e-03 8.90166033e-03\n",
      " 2.97809220e-05 1.75786467e-04 1.01901067e-04 1.19397220e-04\n",
      " 1.73828506e-04 7.12253022e-05 1.54870257e-04 7.19399250e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 44 [0/106 (0%)]\tTrain Loss: 0.037763\n",
      "Train Epoch: 44 [10/106 (9%)]\tTrain Loss: 0.000372\n",
      "Train Epoch: 44 [20/106 (19%)]\tTrain Loss: 0.296833\n",
      "Train Epoch: 44 [30/106 (28%)]\tTrain Loss: 0.002514\n",
      "Train Epoch: 44 [40/106 (38%)]\tTrain Loss: 0.004340\n",
      "Train Epoch: 44 [50/106 (47%)]\tTrain Loss: 0.032006\n",
      "Train Epoch: 44 [60/106 (57%)]\tTrain Loss: 0.003994\n",
      "Train Epoch: 44 [70/106 (66%)]\tTrain Loss: 0.001771\n",
      "Train Epoch: 44 [80/106 (75%)]\tTrain Loss: 0.001595\n",
      "Train Epoch: 44 [90/106 (85%)]\tTrain Loss: 0.001375\n",
      "Train Epoch: 44 [100/106 (94%)]\tTrain Loss: 0.343787\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 401/424 (95%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.28192158e-04 1.65374862e-04 5.10307109e-05 9.61660335e-05\n",
      " 3.58766498e-04 5.01306167e-05 7.48262464e-05 7.54755863e-04\n",
      " 1.64915662e-04 7.36622978e-03 5.41122317e-05 7.85988668e-05\n",
      " 7.81639319e-05 9.57553493e-05 9.66117904e-03 2.47205287e-04\n",
      " 1.46211940e-04 5.40014450e-03 8.45818213e-05 9.44335520e-01\n",
      " 4.32538043e-04 1.00160243e-04 9.71281171e-01 9.60801363e-01\n",
      " 1.36707575e-04 8.88976336e-01 9.99572456e-01 3.58573685e-04\n",
      " 1.02665348e-04 9.30759634e-05 9.68932390e-01 3.12928110e-03\n",
      " 2.56836843e-02 8.78044011e-05 6.16887046e-05 2.89137261e-05\n",
      " 9.21903411e-05 9.12243384e-04 4.09665081e-04 4.23419930e-04\n",
      " 3.90733732e-03 1.09687302e-04 7.10879569e-04 2.07199599e-03\n",
      " 2.21451221e-04 1.88786827e-04 4.18135431e-03 3.49943410e-03\n",
      " 9.78671670e-01 4.38528747e-04 9.90610421e-01 9.46770670e-05\n",
      " 1.73265857e-04 9.72946727e-05 1.35387876e-04 2.45878618e-04\n",
      " 5.69467724e-04 3.77398101e-05 1.80772608e-04 2.10396422e-04\n",
      " 6.51151175e-03 1.12671393e-03 9.87249196e-01 9.85643387e-01\n",
      " 7.44097590e-01 5.01902699e-02 8.79576022e-04 9.91228580e-01\n",
      " 1.14648759e-01 5.69439717e-02 7.63241434e-04 2.01304653e-03\n",
      " 6.79077387e-01 9.94866252e-01 9.96431708e-01 9.12651181e-01\n",
      " 9.79298234e-01 9.64509487e-01 9.97282028e-01 7.28280051e-03\n",
      " 4.00121063e-02 3.34294164e-03 9.99311447e-01 9.59675848e-01\n",
      " 1.00769146e-04 4.18856293e-02 2.21741349e-02 2.88722031e-02\n",
      " 2.47593096e-04 5.67533658e-04 2.07916480e-02 1.20180252e-04\n",
      " 5.73913567e-04 9.96892691e-01 8.91768432e-04 6.53334209e-05\n",
      " 6.36199693e-05 1.11876905e-03 3.51415481e-04 3.22408974e-01\n",
      " 4.41187434e-03 6.92130327e-01 1.96648223e-04 1.06861570e-03\n",
      " 2.15296540e-03 7.81033712e-04 9.79030550e-01 8.74615252e-01\n",
      " 7.35508584e-05 5.81728353e-04 1.93557888e-03 2.98434083e-04\n",
      " 9.54488001e-04 1.13484944e-04 2.40353038e-04 6.26815192e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 45 [0/106 (0%)]\tTrain Loss: 0.002071\n",
      "Train Epoch: 45 [10/106 (9%)]\tTrain Loss: 0.007721\n",
      "Train Epoch: 45 [20/106 (19%)]\tTrain Loss: 0.003874\n",
      "Train Epoch: 45 [30/106 (28%)]\tTrain Loss: 0.000829\n",
      "Train Epoch: 45 [40/106 (38%)]\tTrain Loss: 0.004271\n",
      "Train Epoch: 45 [50/106 (47%)]\tTrain Loss: 0.000500\n",
      "Train Epoch: 45 [60/106 (57%)]\tTrain Loss: 0.005274\n",
      "Train Epoch: 45 [70/106 (66%)]\tTrain Loss: 0.020532\n",
      "Train Epoch: 45 [80/106 (75%)]\tTrain Loss: 0.000216\n",
      "Train Epoch: 45 [90/106 (85%)]\tTrain Loss: 0.002717\n",
      "Train Epoch: 45 [100/106 (94%)]\tTrain Loss: 0.000435\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 407/424 (96%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [4.48390529e-05 2.18980131e-04 5.12349216e-05 3.97569274e-05\n",
      " 7.06392675e-05 1.68485676e-05 1.00011915e-04 9.85697479e-05\n",
      " 3.10782525e-05 1.17261894e-04 3.41012492e-05 1.54530844e-05\n",
      " 2.88859737e-05 5.08364283e-05 1.61228108e-03 6.12625154e-05\n",
      " 6.65805710e-05 1.16350548e-03 5.33215316e-05 3.56362271e-03\n",
      " 1.51321452e-04 7.65257282e-05 8.22459348e-03 2.47319173e-02\n",
      " 7.62613563e-05 1.12131126e-02 9.96976495e-01 1.66258993e-04\n",
      " 5.95187121e-05 5.88882031e-05 6.94684029e-01 1.33928738e-03\n",
      " 1.61470391e-03 2.82848505e-05 3.55414377e-05 1.74707402e-05\n",
      " 3.97940239e-05 2.38979017e-04 1.69685736e-04 1.09643363e-04\n",
      " 3.17358354e-04 8.01324713e-05 3.31474148e-04 4.15134447e-04\n",
      " 9.80928598e-05 6.80563026e-05 7.06834835e-04 1.04982208e-03\n",
      " 4.84768867e-01 3.74544543e-05 3.02610695e-01 3.18750608e-05\n",
      " 4.61987511e-05 4.00650679e-05 7.98141409e-05 8.34593375e-05\n",
      " 1.93341271e-04 2.37360255e-05 5.71139099e-05 7.89213809e-05\n",
      " 2.18876861e-02 9.60984849e-04 9.82061148e-01 9.67237473e-01\n",
      " 3.92105937e-01 1.63868838e-03 1.64064651e-04 9.88645077e-01\n",
      " 1.01997880e-02 7.01847859e-03 2.92596873e-04 4.01870639e-04\n",
      " 2.93249940e-03 9.47189093e-01 9.80552495e-01 5.95453620e-01\n",
      " 9.10904229e-01 8.50459099e-01 9.97324347e-01 2.45700171e-03\n",
      " 2.91654025e-03 1.33582170e-03 9.97993588e-01 4.51064736e-01\n",
      " 7.48492239e-05 2.10196283e-02 1.48055637e-02 1.04268370e-02\n",
      " 1.31380104e-04 2.30543417e-04 2.13254258e-04 5.62973255e-05\n",
      " 2.67393043e-04 7.39277661e-01 4.98555382e-05 2.30951191e-05\n",
      " 3.06585425e-05 4.25526872e-04 1.14949595e-04 1.69804168e-03\n",
      " 4.13935719e-04 8.11202466e-01 8.02109062e-05 1.67425082e-04\n",
      " 2.63280002e-04 2.39098896e-04 6.37326809e-03 5.64492904e-02\n",
      " 4.04537168e-05 1.29381893e-04 9.62726481e-05 1.82564414e-04\n",
      " 2.08669677e-04 5.20618123e-05 1.23801408e-04 7.19502452e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 46 [0/106 (0%)]\tTrain Loss: 0.000521\n",
      "Train Epoch: 46 [10/106 (9%)]\tTrain Loss: 0.000248\n",
      "Train Epoch: 46 [20/106 (19%)]\tTrain Loss: 0.000169\n",
      "Train Epoch: 46 [30/106 (28%)]\tTrain Loss: 0.134609\n",
      "Train Epoch: 46 [40/106 (38%)]\tTrain Loss: 0.000174\n",
      "Train Epoch: 46 [50/106 (47%)]\tTrain Loss: 0.034100\n",
      "Train Epoch: 46 [60/106 (57%)]\tTrain Loss: 0.000158\n",
      "Train Epoch: 46 [70/106 (66%)]\tTrain Loss: 0.087291\n",
      "Train Epoch: 46 [80/106 (75%)]\tTrain Loss: 0.003328\n",
      "Train Epoch: 46 [90/106 (85%)]\tTrain Loss: 0.000195\n",
      "Train Epoch: 46 [100/106 (94%)]\tTrain Loss: 0.000158\n",
      "\n",
      "Train set: Average loss: 0.0012, Accuracy: 404/424 (95%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.66238693e-04 7.18781142e-04 1.47242245e-04 1.38898264e-04\n",
      " 2.80338456e-04 7.61502451e-05 2.58584158e-04 4.39820491e-04\n",
      " 1.30346772e-04 1.22823531e-03 1.25520746e-04 7.12388937e-05\n",
      " 1.27773805e-04 2.40934198e-04 8.31953529e-03 3.20148392e-04\n",
      " 2.24449119e-04 9.64959804e-03 2.27750395e-04 6.83308691e-02\n",
      " 5.40783687e-04 2.92272074e-04 1.75595749e-02 5.84503531e-01\n",
      " 3.61117069e-04 9.72701516e-03 9.99621272e-01 6.99494791e-04\n",
      " 1.93676125e-04 1.81306386e-04 9.16076303e-01 7.14126695e-03\n",
      " 7.87627324e-03 1.00010577e-04 1.17217067e-04 6.77661737e-05\n",
      " 1.58498762e-04 1.12135336e-03 7.98168418e-04 4.47132741e-04\n",
      " 4.00427589e-03 2.85807473e-04 1.54137262e-03 2.10732175e-03\n",
      " 3.89438064e-04 2.70540564e-04 2.71604420e-03 3.96923954e-03\n",
      " 9.80019450e-01 3.60423321e-04 9.75724638e-01 1.32849120e-04\n",
      " 1.83456475e-04 1.48811218e-04 3.53130919e-04 3.35997640e-04\n",
      " 1.26140635e-03 7.63942662e-05 2.27896147e-04 3.40631435e-04\n",
      " 8.44134867e-01 1.47530604e-02 9.95993018e-01 9.93903100e-01\n",
      " 8.30171585e-01 2.39925552e-02 7.85013719e-04 9.96166527e-01\n",
      " 3.69680196e-01 4.60979879e-01 1.49473688e-03 1.99813093e-03\n",
      " 2.58593559e-01 9.97548163e-01 9.96467590e-01 9.65546250e-01\n",
      " 9.89744604e-01 9.89498734e-01 9.99065340e-01 9.03204270e-03\n",
      " 2.02317759e-02 3.68980668e-03 9.99820769e-01 9.11243916e-01\n",
      " 3.67113942e-04 3.47907960e-01 7.08716959e-02 4.35778163e-02\n",
      " 4.09795175e-04 1.16818422e-03 7.14277441e-04 2.29627301e-04\n",
      " 8.95492674e-04 9.91987228e-01 2.44446215e-04 8.18026310e-05\n",
      " 1.16382311e-04 1.68119697e-03 4.98062815e-04 9.26335622e-03\n",
      " 2.72535207e-03 9.88263607e-01 2.87273637e-04 7.06333201e-04\n",
      " 1.13798107e-03 8.28489487e-04 1.94359466e-01 4.92096514e-01\n",
      " 2.00802446e-04 4.89316182e-04 9.04032320e-04 6.38934027e-04\n",
      " 6.23295025e-04 1.84085511e-04 4.88235353e-04 2.18614633e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 47 [0/106 (0%)]\tTrain Loss: 0.001194\n",
      "Train Epoch: 47 [10/106 (9%)]\tTrain Loss: 0.001711\n",
      "Train Epoch: 47 [20/106 (19%)]\tTrain Loss: 0.000426\n",
      "Train Epoch: 47 [30/106 (28%)]\tTrain Loss: 0.000400\n",
      "Train Epoch: 47 [40/106 (38%)]\tTrain Loss: 0.050427\n",
      "Train Epoch: 47 [50/106 (47%)]\tTrain Loss: 0.000309\n",
      "Train Epoch: 47 [60/106 (57%)]\tTrain Loss: 0.034504\n",
      "Train Epoch: 47 [70/106 (66%)]\tTrain Loss: 0.064163\n",
      "Train Epoch: 47 [80/106 (75%)]\tTrain Loss: 0.001933\n",
      "Train Epoch: 47 [90/106 (85%)]\tTrain Loss: 0.001518\n",
      "Train Epoch: 47 [100/106 (94%)]\tTrain Loss: 0.000494\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 408/424 (96%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [7.23321064e-05 3.31519987e-04 7.22890836e-05 6.82465252e-05\n",
      " 1.20087461e-04 2.55198411e-05 1.28874948e-04 2.29264813e-04\n",
      " 4.57977840e-05 5.33557322e-04 6.23439773e-05 3.17913946e-05\n",
      " 7.29468666e-05 1.04876366e-04 3.95162310e-03 1.44389516e-04\n",
      " 1.07371161e-04 1.83832238e-03 1.08252520e-04 1.09550087e-02\n",
      " 2.52797239e-04 1.35210750e-04 2.85245921e-03 2.17372447e-01\n",
      " 1.53477202e-04 7.47997267e-03 9.99600589e-01 2.41320886e-04\n",
      " 9.15519413e-05 9.06140049e-05 4.99952227e-01 1.69528904e-03\n",
      " 1.14440185e-03 4.55133413e-05 5.76920174e-05 3.38286518e-05\n",
      " 8.29242490e-05 4.06770210e-04 3.49476730e-04 1.81345444e-04\n",
      " 1.00420578e-03 1.27157691e-04 6.48335845e-04 6.58484641e-04\n",
      " 1.82168675e-04 1.29668784e-04 1.34771504e-03 2.00099708e-03\n",
      " 8.45132470e-01 1.44515405e-04 8.77793789e-01 6.19648708e-05\n",
      " 1.06046027e-04 7.25836217e-05 1.64391924e-04 1.35382070e-04\n",
      " 6.37943507e-04 3.66910790e-05 1.09655288e-04 1.84880613e-04\n",
      " 2.22661719e-01 2.36697937e-03 9.94429886e-01 9.91314471e-01\n",
      " 6.23579264e-01 1.09996237e-02 2.92925397e-04 9.90985036e-01\n",
      " 1.06164016e-01 2.18117964e-02 5.13627368e-04 6.15859113e-04\n",
      " 2.98036784e-02 9.93912101e-01 9.95262504e-01 8.71674716e-01\n",
      " 9.06507194e-01 8.94257069e-01 9.98849154e-01 3.63282883e-03\n",
      " 2.97454023e-03 1.37759745e-03 9.99538302e-01 7.37245023e-01\n",
      " 1.27215419e-04 4.01844457e-02 1.27576068e-02 4.41953028e-03\n",
      " 1.76531074e-04 4.56563459e-04 3.38237936e-04 8.85848640e-05\n",
      " 3.66545166e-04 9.79537129e-01 1.01568890e-04 3.62037827e-05\n",
      " 5.74263504e-05 5.11109247e-04 2.09559148e-04 1.97727536e-03\n",
      " 5.13430859e-04 9.31467175e-01 1.18061871e-04 2.79953034e-04\n",
      " 4.56416426e-04 4.27037885e-04 1.75430737e-02 2.60365337e-01\n",
      " 8.87590504e-05 1.87847749e-04 3.99401499e-04 2.64890405e-04\n",
      " 2.91187898e-04 9.55935029e-05 1.95391141e-04 7.55039218e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 48 [0/106 (0%)]\tTrain Loss: 0.295949\n",
      "Train Epoch: 48 [10/106 (9%)]\tTrain Loss: 0.000196\n",
      "Train Epoch: 48 [20/106 (19%)]\tTrain Loss: 0.001446\n",
      "Train Epoch: 48 [30/106 (28%)]\tTrain Loss: 0.005325\n",
      "Train Epoch: 48 [40/106 (38%)]\tTrain Loss: 0.000152\n",
      "Train Epoch: 48 [50/106 (47%)]\tTrain Loss: 0.001454\n",
      "Train Epoch: 48 [60/106 (57%)]\tTrain Loss: 0.000211\n",
      "Train Epoch: 48 [70/106 (66%)]\tTrain Loss: 0.005538\n",
      "Train Epoch: 48 [80/106 (75%)]\tTrain Loss: 0.000441\n",
      "Train Epoch: 48 [90/106 (85%)]\tTrain Loss: 0.000497\n",
      "Train Epoch: 48 [100/106 (94%)]\tTrain Loss: 0.000347\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 415/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [9.09073133e-05 4.92711144e-04 9.74032810e-05 7.41065523e-05\n",
      " 1.36459988e-04 2.96925300e-05 1.90955674e-04 2.24394127e-04\n",
      " 5.58839638e-05 2.37641172e-04 7.13180125e-05 2.87455496e-05\n",
      " 7.26737926e-05 1.04143779e-04 2.42144009e-03 1.27777093e-04\n",
      " 1.20042911e-04 1.40603352e-03 1.07093139e-04 2.62774574e-03\n",
      " 2.52367376e-04 1.27879481e-04 3.73179675e-03 4.59664464e-02\n",
      " 1.35211012e-04 1.42577384e-03 9.98200059e-01 2.29259123e-04\n",
      " 1.20758988e-04 1.19472970e-04 1.09054305e-01 1.93137745e-03\n",
      " 4.72545717e-03 5.05314456e-05 6.48380737e-05 3.43727042e-05\n",
      " 7.32281624e-05 5.98457176e-04 3.87715903e-04 1.49216474e-04\n",
      " 6.91689493e-04 1.21600206e-04 7.29913590e-04 9.44295956e-04\n",
      " 2.49178585e-04 1.49435320e-04 1.54651387e-03 1.89096446e-03\n",
      " 3.65176499e-01 9.91495181e-05 3.98146600e-01 7.41081749e-05\n",
      " 9.66153166e-05 7.89647456e-05 1.52467677e-04 1.65357502e-04\n",
      " 4.56397247e-04 3.94562048e-05 1.07611209e-04 1.60470532e-04\n",
      " 1.99667122e-02 7.87841214e-04 9.66430843e-01 8.40335429e-01\n",
      " 2.86226660e-01 3.64713580e-03 3.07075999e-04 9.80906785e-01\n",
      " 6.91098347e-02 8.95297527e-02 6.24768261e-04 7.20891578e-04\n",
      " 9.30714607e-03 9.85133052e-01 9.84953761e-01 7.85527885e-01\n",
      " 7.37824202e-01 6.14907444e-01 9.95615244e-01 3.35495803e-03\n",
      " 2.22976226e-03 8.78238585e-04 9.98450756e-01 4.02912349e-01\n",
      " 1.46295497e-04 1.87003352e-02 1.05924942e-02 6.32150983e-03\n",
      " 1.83723794e-04 4.98758571e-04 2.62232847e-04 1.10953864e-04\n",
      " 4.82863543e-04 9.09885168e-01 1.05795341e-04 3.80081292e-05\n",
      " 6.36384793e-05 7.14555907e-04 2.20249480e-04 2.18822155e-03\n",
      " 5.75340877e-04 8.70607972e-01 1.46549297e-04 3.36921978e-04\n",
      " 6.03445282e-04 5.25174197e-04 1.86667368e-02 1.06533445e-01\n",
      " 8.69681026e-05 2.25993732e-04 2.39551169e-04 2.44499330e-04\n",
      " 3.22339387e-04 9.08677102e-05 2.34223146e-04 7.35646754e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 49 [0/106 (0%)]\tTrain Loss: 0.000139\n",
      "Train Epoch: 49 [10/106 (9%)]\tTrain Loss: 0.001170\n",
      "Train Epoch: 49 [20/106 (19%)]\tTrain Loss: 0.000468\n",
      "Train Epoch: 49 [30/106 (28%)]\tTrain Loss: 0.000295\n",
      "Train Epoch: 49 [40/106 (38%)]\tTrain Loss: 0.000688\n",
      "Train Epoch: 49 [50/106 (47%)]\tTrain Loss: 0.001410\n",
      "Train Epoch: 49 [60/106 (57%)]\tTrain Loss: 0.000517\n",
      "Train Epoch: 49 [70/106 (66%)]\tTrain Loss: 0.000140\n",
      "Train Epoch: 49 [80/106 (75%)]\tTrain Loss: 0.001098\n",
      "Train Epoch: 49 [90/106 (85%)]\tTrain Loss: 0.000863\n",
      "Train Epoch: 49 [100/106 (94%)]\tTrain Loss: 0.000176\n",
      "\n",
      "Train set: Average loss: 0.0001, Accuracy: 406/424 (96%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.03911288e-04 8.81750195e-04 2.17122084e-04 1.94794382e-04\n",
      " 3.40131373e-04 7.83955620e-05 3.72137845e-04 5.35275671e-04\n",
      " 1.38389805e-04 8.57873238e-04 1.88226040e-04 8.95357807e-05\n",
      " 2.01007017e-04 2.99144274e-04 6.99761324e-03 3.73027608e-04\n",
      " 3.26498644e-04 6.56937249e-03 2.75698578e-04 1.26451505e-02\n",
      " 6.25133689e-04 3.57618672e-04 9.46875755e-03 1.10283211e-01\n",
      " 4.45458951e-04 4.54002619e-03 9.99496937e-01 7.35693786e-04\n",
      " 2.75831349e-04 2.76953651e-04 3.66395980e-01 3.65588279e-03\n",
      " 3.40249436e-03 1.41884899e-04 1.62772412e-04 9.64494466e-05\n",
      " 2.06803277e-04 1.06075546e-03 9.20557068e-04 4.79175447e-04\n",
      " 1.97613123e-03 3.03807290e-04 1.44432101e-03 1.81104348e-03\n",
      " 5.31251368e-04 3.46742134e-04 2.95574334e-03 3.50535195e-03\n",
      " 7.25894094e-01 2.74536054e-04 7.77659237e-01 1.82865464e-04\n",
      " 2.93706136e-04 2.07921345e-04 3.59635626e-04 4.06387117e-04\n",
      " 1.18159631e-03 1.05839237e-04 2.79508182e-04 4.73198714e-04\n",
      " 4.03724730e-01 8.55880417e-03 9.90028083e-01 9.83357310e-01\n",
      " 4.63383526e-01 2.87262965e-02 7.37152412e-04 9.89205360e-01\n",
      " 1.39202535e-01 9.76738557e-02 1.19775976e-03 1.38721743e-03\n",
      " 3.73147763e-02 9.92007494e-01 9.90130544e-01 9.38458145e-01\n",
      " 9.76546645e-01 9.73040462e-01 9.99310613e-01 6.06823713e-03\n",
      " 4.98224143e-03 2.03011651e-03 9.99530315e-01 7.01050222e-01\n",
      " 4.52207430e-04 6.28838912e-02 2.23600250e-02 1.31739695e-02\n",
      " 4.50763298e-04 1.17950770e-03 7.81941460e-04 2.49067903e-04\n",
      " 8.06359458e-04 9.60676730e-01 2.93770019e-04 1.12890775e-04\n",
      " 1.71454274e-04 1.27260271e-03 5.44957642e-04 5.64776454e-03\n",
      " 1.70355337e-03 9.50874925e-01 3.45358218e-04 7.85531476e-04\n",
      " 1.30247558e-03 1.18125265e-03 2.41114236e-02 2.89705753e-01\n",
      " 2.35033382e-04 5.15202642e-04 7.02823512e-04 5.62688161e-04\n",
      " 7.27709092e-04 2.45190342e-04 5.67232666e-04 1.69178692e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 50 [0/106 (0%)]\tTrain Loss: 0.001574\n",
      "Train Epoch: 50 [10/106 (9%)]\tTrain Loss: 0.273842\n",
      "Train Epoch: 50 [20/106 (19%)]\tTrain Loss: 0.246863\n",
      "Train Epoch: 50 [30/106 (28%)]\tTrain Loss: 0.000580\n",
      "Train Epoch: 50 [40/106 (38%)]\tTrain Loss: 0.000223\n",
      "Train Epoch: 50 [50/106 (47%)]\tTrain Loss: 0.003069\n",
      "Train Epoch: 50 [60/106 (57%)]\tTrain Loss: 0.053448\n",
      "Train Epoch: 50 [70/106 (66%)]\tTrain Loss: 0.001466\n",
      "Train Epoch: 50 [80/106 (75%)]\tTrain Loss: 0.000404\n",
      "Train Epoch: 50 [90/106 (85%)]\tTrain Loss: 0.068401\n",
      "Train Epoch: 50 [100/106 (94%)]\tTrain Loss: 0.000638\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 399/424 (94%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.13300848e-05 1.00422672e-04 1.61197840e-05 1.03311240e-05\n",
      " 2.62709636e-05 3.56918736e-06 3.07435657e-05 4.74707231e-05\n",
      " 8.01367514e-06 5.81112145e-05 9.16898898e-06 3.76948879e-06\n",
      " 9.09371647e-06 1.84132095e-05 1.32627483e-03 2.61271525e-05\n",
      " 1.94459790e-05 5.08820289e-04 1.77363854e-05 5.50683006e-04\n",
      " 4.44616235e-05 2.08732472e-05 1.25691190e-03 6.41159015e-03\n",
      " 2.60783108e-05 6.37922785e-04 9.98423696e-01 3.95233110e-05\n",
      " 1.70847816e-05 1.60667587e-05 3.06736797e-01 1.06250984e-03\n",
      " 1.30811962e-03 7.06430546e-06 1.24703183e-05 4.62851494e-06\n",
      " 1.32895993e-05 1.32191897e-04 8.29943456e-05 2.27826295e-05\n",
      " 9.56586009e-05 1.84559995e-05 2.08101817e-04 1.70376574e-04\n",
      " 3.87974287e-05 1.97587196e-05 5.05041855e-04 7.27655075e-04\n",
      " 3.35350037e-01 9.41852431e-06 1.04311496e-01 1.12800481e-05\n",
      " 1.37701545e-05 1.33142339e-05 2.78106763e-05 3.15627149e-05\n",
      " 9.25745117e-05 5.58704369e-06 1.92520456e-05 2.50479825e-05\n",
      " 1.20223388e-02 1.62197306e-04 9.88560021e-01 9.69627261e-01\n",
      " 4.50296253e-01 1.27028010e-03 9.58743185e-05 9.92524564e-01\n",
      " 3.41439694e-02 8.86547565e-02 1.24871789e-04 1.71657128e-04\n",
      " 6.62531471e-03 9.86653030e-01 9.91744459e-01 9.68958139e-01\n",
      " 8.14282596e-01 6.67626619e-01 9.95805860e-01 2.15587043e-03\n",
      " 2.06385972e-03 4.20104188e-04 9.97954845e-01 5.90824664e-01\n",
      " 2.37878721e-05 2.08514575e-02 5.35524590e-03 3.83705366e-03\n",
      " 2.67072683e-05 1.23117788e-04 5.77769097e-05 1.47154160e-05\n",
      " 8.07027027e-05 9.42069411e-01 1.66911686e-05 5.60388526e-06\n",
      " 8.95790799e-06 1.58105729e-04 4.11397923e-05 1.02338265e-03\n",
      " 1.62124168e-04 9.52878952e-01 2.56129279e-05 8.39537606e-05\n",
      " 1.82058066e-04 1.43614248e-04 2.56684585e-03 3.45744379e-02\n",
      " 1.45909062e-05 4.52650238e-05 2.87691382e-05 5.07323493e-05\n",
      " 6.66510314e-05 1.42143754e-05 5.71680685e-05 3.62946623e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 13 TN= 57 FN= 43 FP= 3\n",
      "TP+FP 16\n",
      "precision 0.8125\n",
      "recall 0.23214285714285715\n",
      "F1 0.3611111111111111\n",
      "acc 0.603448275862069\n",
      "AUCp 0.5910714285714286\n",
      "AUC 0.7714285714285714\n",
      "\n",
      " The epoch is 50, average recall: 0.2321, average precision: 0.8125,average F1: 0.3611, average accuracy: 0.6034, average AUC: 0.7714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 51 [0/106 (0%)]\tTrain Loss: 0.001431\n",
      "Train Epoch: 51 [10/106 (9%)]\tTrain Loss: 0.000141\n",
      "Train Epoch: 51 [20/106 (19%)]\tTrain Loss: 0.002152\n",
      "Train Epoch: 51 [30/106 (28%)]\tTrain Loss: 0.001350\n",
      "Train Epoch: 51 [40/106 (38%)]\tTrain Loss: 0.000236\n",
      "Train Epoch: 51 [50/106 (47%)]\tTrain Loss: 0.002510\n",
      "Train Epoch: 51 [60/106 (57%)]\tTrain Loss: 0.000445\n",
      "Train Epoch: 51 [70/106 (66%)]\tTrain Loss: 0.000213\n",
      "Train Epoch: 51 [80/106 (75%)]\tTrain Loss: 0.000217\n",
      "Train Epoch: 51 [90/106 (85%)]\tTrain Loss: 0.000141\n",
      "Train Epoch: 51 [100/106 (94%)]\tTrain Loss: 0.000254\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 415/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.92527181e-05 2.07914622e-04 4.19422831e-05 3.45744884e-05\n",
      " 6.09207164e-05 1.32956957e-05 7.51917541e-05 1.18363059e-04\n",
      " 2.48534416e-05 1.98090682e-04 3.53809410e-05 1.60240670e-05\n",
      " 3.59891928e-05 4.87924117e-05 2.18935660e-03 7.59965187e-05\n",
      " 7.02308971e-05 1.44456187e-03 5.55668230e-05 1.19809620e-02\n",
      " 1.25800303e-04 6.66639971e-05 9.40220896e-03 1.32388338e-01\n",
      " 8.51780278e-05 2.97566620e-03 9.98936236e-01 1.23533508e-04\n",
      " 4.93363768e-05 4.74521185e-05 2.53912896e-01 1.28206715e-03\n",
      " 1.28009240e-03 2.54632942e-05 3.75169038e-05 1.59967676e-05\n",
      " 4.07469197e-05 3.71592439e-04 1.88254940e-04 7.73309803e-05\n",
      " 5.01940376e-04 5.72409008e-05 4.89192607e-04 4.53069719e-04\n",
      " 1.15444112e-04 6.43423045e-05 9.48195637e-04 1.15735689e-03\n",
      " 4.28130478e-01 5.44594914e-05 3.19049865e-01 3.60096528e-05\n",
      " 4.75564011e-05 3.83088882e-05 6.95683266e-05 8.32817313e-05\n",
      " 2.49293371e-04 1.75167188e-05 5.20272297e-05 8.88710128e-05\n",
      " 2.56788284e-01 3.82392365e-03 9.92581844e-01 9.86380756e-01\n",
      " 1.43745556e-01 3.07352236e-03 1.87917598e-04 9.90655005e-01\n",
      " 5.64449243e-02 1.21357955e-01 4.43311437e-04 6.04749424e-04\n",
      " 2.65728217e-02 9.88850296e-01 9.92026150e-01 9.00938034e-01\n",
      " 8.92416000e-01 8.33141267e-01 9.96442974e-01 2.75556697e-03\n",
      " 3.41710984e-03 6.33438351e-04 9.98737514e-01 6.82732105e-01\n",
      " 7.28059313e-05 3.29180844e-02 1.21213626e-02 5.24465740e-03\n",
      " 9.42087645e-05 3.24460241e-04 1.72858621e-04 4.60171141e-05\n",
      " 2.34704887e-04 9.63071585e-01 6.13985540e-05 1.90049795e-05\n",
      " 3.08329109e-05 4.39263880e-04 1.26210973e-04 3.49965855e-03\n",
      " 7.15623435e-04 9.38515782e-01 7.51962070e-05 1.89051629e-04\n",
      " 4.16959258e-04 2.84364738e-04 1.61398668e-02 2.49493569e-01\n",
      " 4.94323540e-05 1.26704050e-04 1.62672324e-04 1.44933816e-04\n",
      " 2.00903363e-04 5.31481128e-05 1.31043300e-04 5.41350164e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 52 [0/106 (0%)]\tTrain Loss: 0.287890\n",
      "Train Epoch: 52 [10/106 (9%)]\tTrain Loss: 0.057746\n",
      "Train Epoch: 52 [20/106 (19%)]\tTrain Loss: 0.030162\n",
      "Train Epoch: 52 [30/106 (28%)]\tTrain Loss: 0.000169\n",
      "Train Epoch: 52 [40/106 (38%)]\tTrain Loss: 0.000583\n",
      "Train Epoch: 52 [50/106 (47%)]\tTrain Loss: 0.032630\n",
      "Train Epoch: 52 [60/106 (57%)]\tTrain Loss: 0.003936\n",
      "Train Epoch: 52 [70/106 (66%)]\tTrain Loss: 0.001291\n",
      "Train Epoch: 52 [80/106 (75%)]\tTrain Loss: 0.312198\n",
      "Train Epoch: 52 [90/106 (85%)]\tTrain Loss: 0.000186\n",
      "Train Epoch: 52 [100/106 (94%)]\tTrain Loss: 0.000365\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 401/424 (95%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.89220841e-05 3.03055451e-04 4.01553407e-05 3.57754907e-05\n",
      " 7.59297400e-05 1.33639469e-05 1.01137659e-04 1.73103879e-04\n",
      " 1.95088160e-05 1.14951190e-03 3.50953742e-05 1.73606913e-05\n",
      " 3.78857803e-05 8.64793474e-05 1.05601409e-02 8.97646460e-05\n",
      " 5.97949038e-05 7.88396876e-03 7.87655954e-05 2.43875593e-01\n",
      " 3.29023431e-04 6.62797975e-05 1.40426069e-01 9.82052863e-01\n",
      " 8.67938725e-05 8.58630911e-02 9.99282062e-01 1.46998995e-04\n",
      " 5.01362119e-05 5.13261766e-05 8.39248359e-01 5.95808821e-03\n",
      " 1.02500978e-03 1.80071329e-05 2.82418841e-05 1.15681787e-05\n",
      " 3.02926146e-05 3.84972809e-04 3.23021668e-04 7.36572620e-05\n",
      " 4.49461950e-04 4.69884908e-05 8.97382852e-04 8.33602680e-04\n",
      " 1.33309761e-04 5.84837471e-05 2.43595615e-03 6.69198157e-03\n",
      " 9.87606049e-01 1.50939159e-04 9.72733796e-01 2.55910982e-05\n",
      " 1.02231679e-04 4.39360228e-05 1.05702165e-04 7.29840976e-05\n",
      " 4.21898090e-04 1.44474170e-05 7.37705777e-05 1.28880856e-04\n",
      " 9.88646269e-01 3.54338408e-01 9.98344898e-01 9.98251140e-01\n",
      " 8.83983076e-01 1.14175044e-02 4.96473687e-04 9.97154593e-01\n",
      " 4.55913395e-01 7.66614497e-01 1.05536799e-03 9.76465293e-04\n",
      " 5.47942340e-01 9.97402191e-01 9.97858942e-01 9.92679834e-01\n",
      " 9.94766593e-01 9.96459305e-01 9.98042107e-01 1.33496327e-02\n",
      " 3.89437042e-02 8.24012328e-03 9.99401093e-01 9.82665777e-01\n",
      " 9.24953638e-05 8.04253817e-01 9.14190933e-02 2.53752246e-02\n",
      " 3.66335007e-04 9.71020490e-04 1.35075988e-03 5.70998563e-05\n",
      " 2.86786846e-04 9.97567415e-01 7.18478623e-05 1.92069710e-05\n",
      " 3.23085296e-05 2.47155549e-03 1.54804409e-04 5.89734763e-02\n",
      " 1.15829706e-03 9.96681631e-01 9.10689487e-05 2.49237753e-04\n",
      " 9.89660388e-04 5.62580070e-04 2.05874041e-01 1.59050658e-01\n",
      " 5.02530092e-05 2.15091277e-04 4.33906680e-04 2.48624245e-04\n",
      " 3.93800379e-04 4.62050084e-05 2.57007428e-04 3.00294487e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 53 [0/106 (0%)]\tTrain Loss: 0.002036\n",
      "Train Epoch: 53 [10/106 (9%)]\tTrain Loss: 0.036360\n",
      "Train Epoch: 53 [20/106 (19%)]\tTrain Loss: 0.000143\n",
      "Train Epoch: 53 [30/106 (28%)]\tTrain Loss: 0.000275\n",
      "Train Epoch: 53 [40/106 (38%)]\tTrain Loss: 0.000372\n",
      "Train Epoch: 53 [50/106 (47%)]\tTrain Loss: 0.003665\n",
      "Train Epoch: 53 [60/106 (57%)]\tTrain Loss: 0.299219\n",
      "Train Epoch: 53 [70/106 (66%)]\tTrain Loss: 0.000457\n",
      "Train Epoch: 53 [80/106 (75%)]\tTrain Loss: 0.001901\n",
      "Train Epoch: 53 [90/106 (85%)]\tTrain Loss: 0.001585\n",
      "Train Epoch: 53 [100/106 (94%)]\tTrain Loss: 0.000265\n",
      "\n",
      "Train set: Average loss: 0.0001, Accuracy: 399/424 (94%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.48487481e-04 1.37376646e-03 1.33590336e-04 1.38310526e-04\n",
      " 2.38923414e-04 8.23903174e-05 4.56267648e-04 4.17822914e-04\n",
      " 1.09178887e-04 3.14610801e-03 1.50194450e-04 7.93737709e-05\n",
      " 1.91887244e-04 6.47741370e-04 6.68036193e-02 2.37966524e-04\n",
      " 2.69177166e-04 2.82054930e-03 1.76440037e-04 9.72504973e-01\n",
      " 1.07491796e-03 2.40686903e-04 9.18737173e-01 9.51578319e-01\n",
      " 2.47739459e-04 6.24970973e-01 9.99652743e-01 7.35782785e-04\n",
      " 1.98649403e-04 2.29638026e-04 6.80613741e-02 1.44279720e-02\n",
      " 4.28791856e-03 1.10158981e-04 8.73977551e-05 4.96414868e-05\n",
      " 9.78408061e-05 9.09726194e-04 7.99350848e-04 1.93792477e-03\n",
      " 1.21715687e-01 7.31490727e-04 1.27216533e-03 2.48418283e-03\n",
      " 4.52430570e-04 2.19473426e-04 2.07605422e-03 2.25589192e-03\n",
      " 7.57804513e-01 2.41475506e-03 9.55897212e-01 9.62661361e-05\n",
      " 3.08957155e-04 1.38998017e-04 3.90199857e-04 2.73753249e-04\n",
      " 7.04406388e-03 5.40312794e-05 2.34893378e-04 3.18390259e-04\n",
      " 9.93488312e-01 9.84499276e-01 9.98500586e-01 9.96967614e-01\n",
      " 4.29778904e-01 1.89840831e-02 9.94029338e-04 9.97006357e-01\n",
      " 4.65749860e-01 5.34191847e-01 5.60988719e-03 7.23123690e-03\n",
      " 6.46103740e-01 9.99167204e-01 9.94567037e-01 9.82360959e-01\n",
      " 9.98154581e-01 9.96166766e-01 9.99566615e-01 6.19768165e-03\n",
      " 2.76740883e-02 1.48602575e-03 9.99733031e-01 9.94646609e-01\n",
      " 2.25040782e-03 6.44126713e-01 1.68223411e-01 2.30248701e-02\n",
      " 1.44382417e-02 3.75151588e-03 9.53372896e-01 2.24065079e-04\n",
      " 6.73682778e-04 9.97755110e-01 3.63994419e-04 1.04866274e-04\n",
      " 1.15032279e-04 4.81768772e-02 3.69019079e-04 9.27438200e-01\n",
      " 7.43248835e-02 9.90562379e-01 3.19603074e-04 5.92365686e-04\n",
      " 2.10708333e-03 8.19343957e-04 6.41957700e-01 9.56226826e-01\n",
      " 2.00799579e-04 1.87832362e-03 2.04872917e-02 6.61755737e-04\n",
      " 7.76411325e-04 8.93452787e-04 6.00596424e-04 9.58454311e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 54 [0/106 (0%)]\tTrain Loss: 0.000231\n",
      "Train Epoch: 54 [10/106 (9%)]\tTrain Loss: 0.002802\n",
      "Train Epoch: 54 [20/106 (19%)]\tTrain Loss: 0.000278\n",
      "Train Epoch: 54 [30/106 (28%)]\tTrain Loss: 0.001273\n",
      "Train Epoch: 54 [40/106 (38%)]\tTrain Loss: 0.006742\n",
      "Train Epoch: 54 [50/106 (47%)]\tTrain Loss: 0.001280\n",
      "Train Epoch: 54 [60/106 (57%)]\tTrain Loss: 0.000355\n",
      "Train Epoch: 54 [70/106 (66%)]\tTrain Loss: 0.000194\n",
      "Train Epoch: 54 [80/106 (75%)]\tTrain Loss: 0.414260\n",
      "Train Epoch: 54 [90/106 (85%)]\tTrain Loss: 0.000524\n",
      "Train Epoch: 54 [100/106 (94%)]\tTrain Loss: 0.000344\n",
      "\n",
      "Train set: Average loss: 0.0067, Accuracy: 408/424 (96%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [9.37502700e-05 1.09796459e-03 1.29031832e-04 1.25778111e-04\n",
      " 2.54047831e-04 3.32710042e-05 1.81077645e-04 5.63656271e-04\n",
      " 7.44319841e-05 1.02684423e-02 1.54108071e-04 6.97733631e-05\n",
      " 1.20498102e-04 8.39132816e-04 4.70160782e-01 1.06606320e-04\n",
      " 1.24980666e-04 4.18533534e-01 1.28817439e-04 9.94926929e-01\n",
      " 1.55108015e-03 1.57388829e-04 9.70026851e-01 9.76385236e-01\n",
      " 1.79520619e-04 9.96394336e-01 9.99953032e-01 4.11779666e-03\n",
      " 1.76880698e-04 1.09943008e-04 9.88208592e-01 8.95157635e-01\n",
      " 2.17522740e-01 8.72173405e-05 5.71285636e-05 3.26392692e-05\n",
      " 8.79362997e-05 2.73143919e-03 1.03091158e-03 3.53883728e-02\n",
      " 8.00681055e-01 1.26641232e-03 7.82328472e-03 2.99658300e-03\n",
      " 7.15308706e-04 2.79095169e-04 2.37472309e-03 5.97932143e-03\n",
      " 9.91839886e-01 3.86033859e-03 9.96812284e-01 3.68670808e-05\n",
      " 7.11861765e-03 1.12723508e-04 6.95392489e-04 3.10850737e-04\n",
      " 5.49851298e-01 3.72223949e-05 3.91924434e-04 2.12959922e-03\n",
      " 9.88901973e-01 7.87172318e-01 9.99096751e-01 9.98213887e-01\n",
      " 9.98800159e-01 9.11684811e-01 7.68598169e-03 9.99750674e-01\n",
      " 9.14149523e-01 8.76372039e-01 7.39537273e-03 5.94193907e-03\n",
      " 9.97157097e-01 9.99902606e-01 9.99880552e-01 9.99446929e-01\n",
      " 9.99876618e-01 9.99925971e-01 9.99825060e-01 2.27690399e-01\n",
      " 9.65661883e-01 8.74499917e-01 9.99973059e-01 9.99301910e-01\n",
      " 1.01491367e-03 9.74142373e-01 4.95634824e-01 4.11979347e-01\n",
      " 9.24160145e-03 2.44319160e-03 9.13370132e-01 7.88153557e-04\n",
      " 1.45904510e-03 9.99839783e-01 3.94128641e-04 1.64083409e-04\n",
      " 2.01715695e-04 2.24297903e-02 3.91199545e-04 9.63981986e-01\n",
      " 1.89879742e-02 9.97555971e-01 1.81873605e-04 6.77013828e-04\n",
      " 3.66909429e-03 1.64028141e-03 8.73218477e-01 9.40101743e-01\n",
      " 8.76247432e-05 1.18428830e-03 3.00720544e-03 4.65953315e-04\n",
      " 5.48489625e-04 3.51879280e-04 2.95002130e-04 3.03372651e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 55 [0/106 (0%)]\tTrain Loss: 0.000151\n",
      "Train Epoch: 55 [10/106 (9%)]\tTrain Loss: 0.000307\n",
      "Train Epoch: 55 [20/106 (19%)]\tTrain Loss: 0.008370\n",
      "Train Epoch: 55 [30/106 (28%)]\tTrain Loss: 0.000201\n",
      "Train Epoch: 55 [40/106 (38%)]\tTrain Loss: 0.007134\n",
      "Train Epoch: 55 [50/106 (47%)]\tTrain Loss: 0.003206\n",
      "Train Epoch: 55 [60/106 (57%)]\tTrain Loss: 0.004470\n",
      "Train Epoch: 55 [70/106 (66%)]\tTrain Loss: 0.001173\n",
      "Train Epoch: 55 [80/106 (75%)]\tTrain Loss: 0.000220\n",
      "Train Epoch: 55 [90/106 (85%)]\tTrain Loss: 0.002469\n",
      "Train Epoch: 55 [100/106 (94%)]\tTrain Loss: 0.001860\n",
      "\n",
      "Train set: Average loss: 0.0017, Accuracy: 408/424 (96%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [9.52017217e-06 9.25967819e-04 1.34461470e-05 7.95050073e-06\n",
      " 1.58324292e-05 2.29160969e-06 5.42873495e-05 2.18630030e-05\n",
      " 8.14932355e-06 6.58963982e-04 3.62391220e-05 2.75361504e-06\n",
      " 1.79375311e-05 1.86436555e-05 2.65379879e-03 1.07282294e-05\n",
      " 1.77119600e-05 2.00410951e-02 1.27338672e-05 9.76180375e-01\n",
      " 8.70285076e-05 1.19500337e-05 8.71780336e-01 5.50652621e-03\n",
      " 6.72765646e-06 6.91756189e-01 9.97980535e-01 5.40265355e-05\n",
      " 1.43813113e-05 9.40585142e-06 3.22418025e-04 5.23083960e-04\n",
      " 2.04943368e-04 5.37161532e-06 5.12990118e-06 2.68237045e-06\n",
      " 5.17145236e-06 4.68142352e-05 3.67359971e-05 1.76401663e-04\n",
      " 6.19984984e-01 1.06076579e-04 6.86606683e-04 5.19780333e-05\n",
      " 9.80102050e-05 2.35237967e-05 5.20697446e-04 1.01110258e-03\n",
      " 4.24108595e-01 6.12807984e-04 7.33924806e-01 2.74084027e-06\n",
      " 1.67029993e-05 4.32409615e-06 6.03082954e-05 2.90516709e-05\n",
      " 6.25152432e-04 3.06448146e-06 2.58953733e-05 3.09993993e-05\n",
      " 8.89013037e-02 1.11245422e-03 9.92104411e-01 8.68926048e-01\n",
      " 9.92447138e-01 4.90275343e-05 3.11383919e-05 9.84678626e-01\n",
      " 6.44668818e-01 3.50763083e-01 1.46881742e-02 3.60996872e-02\n",
      " 9.43819284e-01 9.96021330e-01 9.97421503e-01 5.95964640e-02\n",
      " 7.97560394e-01 8.39379191e-01 9.91913438e-01 8.21321330e-04\n",
      " 9.76459235e-02 6.59493264e-04 9.97360289e-01 8.00078571e-01\n",
      " 1.65683396e-05 1.34173944e-03 1.03446878e-02 7.58597802e-04\n",
      " 2.98331375e-04 2.56321975e-04 8.65617068e-04 9.81649191e-06\n",
      " 6.66734632e-05 9.94244039e-01 2.06825080e-05 6.00137673e-06\n",
      " 5.67405277e-06 2.00585826e-04 2.89218606e-05 9.64898527e-01\n",
      " 9.01629333e-04 6.86135173e-01 1.36205472e-05 4.44681064e-05\n",
      " 1.99143818e-04 1.67131278e-04 5.62627539e-02 9.64152932e-01\n",
      " 1.81818996e-05 1.31965950e-04 9.76110023e-05 9.13810800e-05\n",
      " 1.19242148e-04 5.99511768e-05 3.30467774e-05 9.29114467e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 56 [0/106 (0%)]\tTrain Loss: 0.000150\n",
      "Train Epoch: 56 [10/106 (9%)]\tTrain Loss: 0.000167\n",
      "Train Epoch: 56 [20/106 (19%)]\tTrain Loss: 0.037759\n",
      "Train Epoch: 56 [30/106 (28%)]\tTrain Loss: 0.270976\n",
      "Train Epoch: 56 [40/106 (38%)]\tTrain Loss: 0.004011\n",
      "Train Epoch: 56 [50/106 (47%)]\tTrain Loss: 0.000473\n",
      "Train Epoch: 56 [60/106 (57%)]\tTrain Loss: 0.000381\n",
      "Train Epoch: 56 [70/106 (66%)]\tTrain Loss: 0.002928\n",
      "Train Epoch: 56 [80/106 (75%)]\tTrain Loss: 0.000745\n",
      "Train Epoch: 56 [90/106 (85%)]\tTrain Loss: 0.000162\n",
      "Train Epoch: 56 [100/106 (94%)]\tTrain Loss: 0.388397\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 410/424 (97%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.28392924e-06 2.92032819e-05 2.46142122e-06 1.23346467e-06\n",
      " 2.48060087e-06 3.45658805e-07 5.77762557e-06 4.56415910e-06\n",
      " 7.89425144e-07 1.11404715e-05 1.59554543e-06 5.72809938e-07\n",
      " 6.89052115e-07 1.17097363e-06 2.40798181e-05 2.39835208e-06\n",
      " 2.85410215e-06 1.00162142e-05 1.34510992e-06 6.16887398e-02\n",
      " 7.52658252e-06 2.06785057e-06 7.32174958e-04 4.64912057e-02\n",
      " 1.62551635e-06 4.99208532e-02 9.99804437e-01 2.38303778e-06\n",
      " 1.79723133e-06 1.51307165e-06 3.05346548e-05 3.50452392e-05\n",
      " 7.99042209e-06 1.13250837e-06 1.21187838e-06 1.02252943e-06\n",
      " 7.99065845e-07 2.16489070e-06 3.60268768e-06 1.40883378e-06\n",
      " 3.55559109e-06 1.13457531e-06 1.43818588e-05 3.32683021e-06\n",
      " 3.73711691e-06 5.04425225e-06 1.89215018e-04 7.42336910e-04\n",
      " 9.23948288e-01 3.75595373e-05 8.89172137e-01 3.66284638e-07\n",
      " 4.67328118e-06 1.61336493e-06 7.63422304e-06 4.95555969e-06\n",
      " 3.34420110e-05 7.69451503e-07 2.31869990e-06 1.07791184e-05\n",
      " 9.81585472e-05 4.57735769e-06 6.91953242e-01 2.42646933e-01\n",
      " 9.79805171e-01 4.55950430e-06 2.25038298e-06 1.02697359e-02\n",
      " 8.55741557e-04 1.23625055e-01 1.42569630e-03 1.78556913e-03\n",
      " 7.36765444e-01 9.86176312e-01 8.40657949e-01 5.72653897e-02\n",
      " 9.98015404e-01 9.99127567e-01 9.95090604e-01 6.91391033e-05\n",
      " 2.35889897e-01 2.99747189e-04 9.99476016e-01 8.20268033e-05\n",
      " 5.20717037e-07 1.22985541e-04 3.90367677e-05 1.07759415e-05\n",
      " 4.20567221e-06 5.24407096e-06 5.07466733e-01 1.17825152e-06\n",
      " 2.39199971e-06 9.97347355e-01 2.40155168e-05 4.15050317e-06\n",
      " 1.02371780e-06 1.25927663e-05 8.68162533e-06 2.24331707e-05\n",
      " 5.89493948e-06 2.79596578e-02 1.77146478e-06 1.46214943e-05\n",
      " 8.41841611e-05 2.51803776e-05 3.93626481e-01 2.35532007e-05\n",
      " 7.30787235e-07 6.62078992e-06 1.11351392e-05 5.94557196e-06\n",
      " 3.44198015e-05 1.13972430e-06 5.51053790e-06 9.03926957e-06]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 57 [0/106 (0%)]\tTrain Loss: 0.008425\n",
      "Train Epoch: 57 [10/106 (9%)]\tTrain Loss: 0.604186\n",
      "Train Epoch: 57 [20/106 (19%)]\tTrain Loss: 0.092216\n",
      "Train Epoch: 57 [30/106 (28%)]\tTrain Loss: 0.000198\n",
      "Train Epoch: 57 [40/106 (38%)]\tTrain Loss: 0.000218\n",
      "Train Epoch: 57 [50/106 (47%)]\tTrain Loss: 0.000269\n",
      "Train Epoch: 57 [60/106 (57%)]\tTrain Loss: 0.000093\n",
      "Train Epoch: 57 [70/106 (66%)]\tTrain Loss: 0.000077\n",
      "Train Epoch: 57 [80/106 (75%)]\tTrain Loss: 0.000192\n",
      "Train Epoch: 57 [90/106 (85%)]\tTrain Loss: 0.000077\n",
      "Train Epoch: 57 [100/106 (94%)]\tTrain Loss: 0.001201\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 407/424 (96%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [8.53390588e-07 1.81300842e-04 4.69489714e-06 2.74410422e-06\n",
      " 2.87920989e-06 1.21121360e-07 2.32721031e-05 8.05083346e-06\n",
      " 1.94548056e-06 9.60161196e-06 1.23465188e-05 7.47981801e-07\n",
      " 3.19108876e-05 1.53937253e-05 4.44855262e-03 6.59131547e-05\n",
      " 3.18626589e-05 1.00349728e-02 6.61538843e-06 9.39753830e-01\n",
      " 2.34516963e-04 1.13031365e-05 4.56534445e-01 1.61868423e-01\n",
      " 5.88290959e-06 9.94047046e-01 9.98537898e-01 9.29544331e-04\n",
      " 8.90356932e-06 5.22912887e-06 9.22555685e-01 9.67247546e-01\n",
      " 8.99896175e-02 3.25341148e-06 3.15750367e-06 2.12523400e-06\n",
      " 3.63728577e-06 7.09513042e-05 4.04886414e-05 2.04882980e-03\n",
      " 3.58389206e-02 3.66250548e-04 3.09832030e-05 2.83375484e-05\n",
      " 4.66519232e-05 1.19619599e-05 9.64117644e-04 1.66530423e-02\n",
      " 9.04837489e-01 3.50013179e-05 7.89395988e-01 8.62443528e-07\n",
      " 4.73900809e-06 2.80635618e-06 8.65286961e-03 4.74064427e-06\n",
      " 3.07257622e-02 2.57607326e-06 6.70748932e-06 2.07101803e-05\n",
      " 9.20273662e-01 7.91431516e-02 9.97089982e-01 9.94660676e-01\n",
      " 9.99411702e-01 4.38126782e-03 2.87074981e-05 9.97983813e-01\n",
      " 7.18063787e-02 9.77648020e-01 1.35710612e-01 4.97234434e-01\n",
      " 7.98620939e-01 9.89840329e-01 9.96970177e-01 9.96156633e-01\n",
      " 9.53230858e-01 8.22951496e-01 9.99440968e-01 3.95831168e-01\n",
      " 9.92160499e-01 7.99686074e-01 9.99677300e-01 9.61653054e-01\n",
      " 1.56685273e-05 4.57821935e-01 9.98350978e-01 9.50846910e-01\n",
      " 9.66610212e-04 3.44508735e-05 9.73947406e-01 8.47087449e-06\n",
      " 1.65383135e-05 9.95792747e-01 4.84190059e-05 4.96486882e-06\n",
      " 4.91689752e-06 3.16987485e-01 9.89007895e-05 2.78021008e-01\n",
      " 3.25406715e-03 9.26269114e-01 1.30922390e-05 7.87329336e-04\n",
      " 2.93594807e-01 8.43212256e-05 9.80014682e-01 1.98471799e-01\n",
      " 1.44036558e-05 1.91547293e-02 2.33248575e-03 5.74185760e-05\n",
      " 2.54561892e-04 6.14967896e-04 9.21982573e-05 9.04806256e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "Train Epoch: 58 [0/106 (0%)]\tTrain Loss: 0.000104\n",
      "Train Epoch: 58 [10/106 (9%)]\tTrain Loss: 0.000104\n",
      "Train Epoch: 58 [20/106 (19%)]\tTrain Loss: 0.000132\n",
      "Train Epoch: 58 [30/106 (28%)]\tTrain Loss: 0.001068\n",
      "Train Epoch: 58 [40/106 (38%)]\tTrain Loss: 0.002222\n",
      "Train Epoch: 58 [50/106 (47%)]\tTrain Loss: 0.008033\n",
      "Train Epoch: 58 [60/106 (57%)]\tTrain Loss: 0.000303\n",
      "Train Epoch: 58 [70/106 (66%)]\tTrain Loss: 0.139530\n",
      "Train Epoch: 58 [80/106 (75%)]\tTrain Loss: 0.000153\n",
      "Train Epoch: 58 [90/106 (85%)]\tTrain Loss: 0.000163\n",
      "Train Epoch: 58 [100/106 (94%)]\tTrain Loss: 0.000168\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 409/424 (96%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [6.90360685e-05 1.64213314e-04 6.05249807e-05 5.20483154e-05\n",
      " 5.24303578e-05 1.04111286e-05 6.85240739e-05 6.37438570e-05\n",
      " 4.20333781e-05 1.85564531e-05 7.80318151e-05 8.39608583e-06\n",
      " 5.58609463e-05 6.95185954e-05 2.68139556e-04 3.47495152e-05\n",
      " 3.22360211e-05 3.41225896e-05 3.94170129e-05 8.87194299e-04\n",
      " 8.82832974e-05 2.55640734e-05 3.07443348e-04 6.28652751e-01\n",
      " 2.99113108e-05 9.20189137e-04 9.92027223e-01 4.93709231e-05\n",
      " 5.60808512e-05 4.06182335e-05 2.22545743e-04 4.66638303e-04\n",
      " 1.82667153e-03 3.67396424e-05 2.68176773e-05 2.06975037e-05\n",
      " 2.62914145e-05 4.32446614e-05 8.10874844e-05 1.32751113e-04\n",
      " 2.24097050e-03 8.31963262e-05 7.04202757e-05 1.19037766e-04\n",
      " 1.26310537e-04 7.55193614e-05 8.71602737e-04 1.69417565e-03\n",
      " 6.21605851e-02 1.95179586e-04 6.97783334e-03 1.27231488e-05\n",
      " 2.77229228e-05 3.05942849e-05 1.03848470e-04 5.60571098e-05\n",
      " 3.51947558e-04 3.41490268e-05 3.47286805e-05 4.77728536e-05\n",
      " 1.86034665e-02 2.58195912e-04 9.78126466e-01 9.41697478e-01\n",
      " 2.19384149e-01 3.29988054e-03 1.44653794e-04 9.37115669e-01\n",
      " 8.00842643e-02 9.37903583e-01 2.39323312e-03 1.14278207e-02\n",
      " 2.48177461e-02 9.85183179e-01 8.98769438e-01 1.26500987e-03\n",
      " 4.57471907e-01 3.90642174e-02 9.08426940e-01 2.68421398e-04\n",
      " 7.70995219e-04 2.68038071e-04 9.99931693e-01 2.50492315e-03\n",
      " 4.29801403e-05 1.04075065e-03 4.06832114e-04 1.90093517e-04\n",
      " 6.06395442e-05 6.25089888e-05 1.71708819e-02 6.88132495e-05\n",
      " 4.25390135e-05 9.71798658e-01 1.22327314e-04 6.37129997e-05\n",
      " 1.69939613e-05 1.16991870e-01 1.28783315e-04 2.35825125e-03\n",
      " 1.44225610e-02 2.05692440e-01 2.08640427e-04 2.46794079e-04\n",
      " 4.37441544e-04 2.22159913e-04 9.67080712e-01 9.55376089e-01\n",
      " 6.56174598e-05 1.57078221e-01 5.49079701e-02 3.98252159e-04\n",
      " 2.02392301e-04 4.99446469e-04 1.03947343e-04 5.01876893e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 59 [0/106 (0%)]\tTrain Loss: 0.000169\n",
      "Train Epoch: 59 [10/106 (9%)]\tTrain Loss: 0.000777\n",
      "Train Epoch: 59 [20/106 (19%)]\tTrain Loss: 0.001637\n",
      "Train Epoch: 59 [30/106 (28%)]\tTrain Loss: 0.000118\n",
      "Train Epoch: 59 [40/106 (38%)]\tTrain Loss: 0.001089\n",
      "Train Epoch: 59 [50/106 (47%)]\tTrain Loss: 0.000322\n",
      "Train Epoch: 59 [60/106 (57%)]\tTrain Loss: 0.000792\n",
      "Train Epoch: 59 [70/106 (66%)]\tTrain Loss: 0.000166\n",
      "Train Epoch: 59 [80/106 (75%)]\tTrain Loss: 0.134536\n",
      "Train Epoch: 59 [90/106 (85%)]\tTrain Loss: 0.000544\n",
      "Train Epoch: 59 [100/106 (94%)]\tTrain Loss: 0.000563\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 401/424 (95%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.13564911e-05 2.59181979e-04 3.14807112e-05 3.08870585e-05\n",
      " 4.91394494e-05 1.82920894e-06 5.45572693e-05 7.13207410e-05\n",
      " 6.12492749e-06 7.69371691e-05 1.68810744e-04 9.58920464e-06\n",
      " 3.23162152e-04 5.30089164e-05 1.41672237e-04 6.76746713e-05\n",
      " 2.68218544e-04 1.47811981e-04 8.36658073e-05 4.06173646e-01\n",
      " 2.27042503e-04 7.25530263e-05 4.56299400e-04 9.52922255e-02\n",
      " 1.45104874e-04 6.27546489e-01 9.95547295e-01 6.22263688e-05\n",
      " 3.76447715e-05 3.55649718e-05 6.57350384e-03 3.31229065e-04\n",
      " 3.57512478e-03 4.11288893e-05 4.26645129e-05 8.41563087e-05\n",
      " 7.70990373e-05 4.15011709e-05 1.15903647e-04 3.36423982e-04\n",
      " 1.94394663e-02 1.15334318e-04 7.65516379e-05 2.03898453e-04\n",
      " 1.88168968e-04 4.24589707e-05 6.65857748e-04 2.51024542e-03\n",
      " 9.25377071e-01 6.07138965e-03 7.95308053e-01 1.62270881e-05\n",
      " 5.72857789e-05 2.64628688e-05 2.27224824e-04 6.45934342e-05\n",
      " 3.57040059e-04 6.79503428e-05 5.24312600e-05 7.47160084e-05\n",
      " 9.40324664e-01 1.59678014e-03 9.96528208e-01 9.93643522e-01\n",
      " 9.85995233e-01 5.93911946e-01 1.31408358e-03 5.31407595e-01\n",
      " 4.30258483e-01 2.31503174e-01 4.19384148e-03 4.24390717e-04\n",
      " 9.85463679e-01 9.94442165e-01 9.95062172e-01 8.89805853e-01\n",
      " 9.80179787e-01 9.74644125e-01 1.86898321e-01 9.92521993e-04\n",
      " 7.47383088e-02 4.61448170e-03 9.98121798e-01 9.29212391e-01\n",
      " 1.30553875e-04 4.21771454e-03 2.31782746e-04 8.23075461e-05\n",
      " 1.21209479e-04 3.82477447e-04 2.55158275e-01 1.34246206e-04\n",
      " 1.14240633e-04 9.95009661e-01 1.70654748e-04 1.04370651e-04\n",
      " 3.50361806e-05 1.66769400e-01 1.25923078e-04 1.01197073e-02\n",
      " 3.53331044e-02 9.61102009e-01 1.39290176e-04 1.25839884e-04\n",
      " 9.22732113e-04 5.57244697e-04 3.85904580e-01 3.07375044e-01\n",
      " 1.66307189e-04 9.84966522e-04 2.12968839e-03 3.30094510e-04\n",
      " 5.51683828e-04 1.36140529e-02 7.94948064e-05 8.05349482e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 60 [0/106 (0%)]\tTrain Loss: 0.000324\n",
      "Train Epoch: 60 [10/106 (9%)]\tTrain Loss: 0.102891\n",
      "Train Epoch: 60 [20/106 (19%)]\tTrain Loss: 0.013644\n",
      "Train Epoch: 60 [30/106 (28%)]\tTrain Loss: 0.025252\n",
      "Train Epoch: 60 [40/106 (38%)]\tTrain Loss: 0.349747\n",
      "Train Epoch: 60 [50/106 (47%)]\tTrain Loss: 0.000194\n",
      "Train Epoch: 60 [60/106 (57%)]\tTrain Loss: 0.008330\n",
      "Train Epoch: 60 [70/106 (66%)]\tTrain Loss: 0.002274\n",
      "Train Epoch: 60 [80/106 (75%)]\tTrain Loss: 0.025203\n",
      "Train Epoch: 60 [90/106 (85%)]\tTrain Loss: 0.000777\n",
      "Train Epoch: 60 [100/106 (94%)]\tTrain Loss: 0.000129\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 401/424 (95%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [4.48607789e-05 5.21748676e-04 9.03399778e-05 8.30358185e-05\n",
      " 7.88261532e-05 1.22806405e-05 1.22805082e-04 9.94529983e-05\n",
      " 2.30705082e-05 4.85413366e-05 1.19869845e-04 2.91403558e-05\n",
      " 7.86081728e-05 1.03485865e-04 2.46576325e-04 6.20527353e-05\n",
      " 9.70574547e-05 1.55705481e-03 1.08736072e-04 4.40685600e-01\n",
      " 7.91012775e-04 1.29486551e-04 5.95808029e-03 2.49115959e-01\n",
      " 1.19295604e-04 4.11947072e-03 7.85693526e-01 1.24273327e-04\n",
      " 1.35329668e-04 1.45196071e-04 1.96200606e-04 2.76140257e-04\n",
      " 2.77895277e-04 8.27765907e-05 7.30248867e-05 5.66392700e-05\n",
      " 5.32416561e-05 1.04140803e-04 1.13345865e-04 1.03535807e-04\n",
      " 1.73040491e-04 9.97990865e-05 1.52179942e-04 1.30885819e-04\n",
      " 1.51316985e-04 2.04167707e-04 4.96338587e-04 6.63541432e-04\n",
      " 1.29345676e-03 1.18604949e-04 4.07985557e-04 4.37864510e-05\n",
      " 6.67057102e-05 6.13473967e-05 1.85578770e-04 1.22839760e-04\n",
      " 3.34454206e-04 9.61236874e-05 5.57957283e-05 8.87961287e-05\n",
      " 8.82088304e-01 1.51742122e-03 9.92681563e-01 9.13000822e-01\n",
      " 4.15417850e-01 1.99799993e-04 1.28540152e-04 9.99202073e-01\n",
      " 3.39993507e-01 4.57905186e-03 5.64120128e-04 3.36878584e-04\n",
      " 1.76855717e-02 9.87315893e-01 8.96379590e-01 1.31334644e-03\n",
      " 9.81870294e-01 9.99548614e-01 9.99935389e-01 2.79235421e-04\n",
      " 6.51609432e-03 2.78888474e-04 9.99918938e-01 1.78218409e-01\n",
      " 1.84656514e-04 1.48224179e-03 7.54382636e-04 5.67629701e-04\n",
      " 1.40526477e-04 1.46003396e-04 7.39460707e-01 1.10927838e-04\n",
      " 1.00331534e-04 9.90810394e-01 1.29221342e-04 7.61381161e-05\n",
      " 5.36413063e-05 4.31902474e-04 2.10077676e-04 2.31889280e-04\n",
      " 1.33589434e-03 2.36292044e-03 1.44477934e-04 1.39933982e-04\n",
      " 3.57156852e-04 5.07310790e-04 1.52253464e-03 4.01241565e-03\n",
      " 8.35553728e-05 2.06087556e-04 3.57280805e-04 1.67600156e-04\n",
      " 1.75247100e-04 5.30547230e-04 1.24704486e-04 2.05260963e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 16 TN= 57 FN= 40 FP= 3\n",
      "TP+FP 19\n",
      "precision 0.8421052631578947\n",
      "recall 0.2857142857142857\n",
      "F1 0.4266666666666667\n",
      "acc 0.6293103448275862\n",
      "AUCp 0.6178571428571428\n",
      "AUC 0.7744047619047619\n",
      "\n",
      " The epoch is 60, average recall: 0.2857, average precision: 0.8421,average F1: 0.4267, average accuracy: 0.6293, average AUC: 0.7744\n",
      "Train Epoch: 61 [0/106 (0%)]\tTrain Loss: 0.002128\n",
      "Train Epoch: 61 [10/106 (9%)]\tTrain Loss: 0.126481\n",
      "Train Epoch: 61 [20/106 (19%)]\tTrain Loss: 0.001143\n",
      "Train Epoch: 61 [30/106 (28%)]\tTrain Loss: 0.000441\n",
      "Train Epoch: 61 [40/106 (38%)]\tTrain Loss: 0.000747\n",
      "Train Epoch: 61 [50/106 (47%)]\tTrain Loss: 0.000090\n",
      "Train Epoch: 61 [60/106 (57%)]\tTrain Loss: 0.002475\n",
      "Train Epoch: 61 [70/106 (66%)]\tTrain Loss: 0.094462\n",
      "Train Epoch: 61 [80/106 (75%)]\tTrain Loss: 0.360334\n",
      "Train Epoch: 61 [90/106 (85%)]\tTrain Loss: 0.008828\n",
      "Train Epoch: 61 [100/106 (94%)]\tTrain Loss: 0.000248\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 401/424 (95%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.27675266e-05 4.40386240e-04 2.84451180e-05 2.92584773e-05\n",
      " 2.55910982e-05 3.61443313e-06 3.84441482e-05 2.45324336e-05\n",
      " 4.43986346e-06 2.82779438e-05 4.24292193e-05 5.85199950e-06\n",
      " 2.24486012e-05 2.16897006e-05 6.22729058e-05 2.89742038e-05\n",
      " 7.55339788e-05 1.79827062e-03 5.25687901e-05 8.18110108e-01\n",
      " 2.08124256e-04 1.25763720e-04 1.86212733e-02 9.97241616e-01\n",
      " 1.55125046e-04 4.21253935e-04 9.99791324e-01 3.25057008e-05\n",
      " 3.26165209e-05 3.28173628e-05 3.57575154e-05 1.14413750e-04\n",
      " 8.89829462e-05 2.52516038e-05 1.29397731e-05 1.50679962e-05\n",
      " 2.31320410e-05 2.76145802e-05 2.34990875e-05 2.70848723e-05\n",
      " 4.70980376e-05 1.70647691e-05 5.40545225e-05 3.78020159e-05\n",
      " 3.36119156e-05 6.97609867e-05 3.16924474e-04 5.68055664e-04\n",
      " 7.11437222e-03 3.82634462e-04 5.73100755e-04 1.04095507e-05\n",
      " 1.96494530e-05 1.47403298e-05 5.08365229e-05 3.90352798e-05\n",
      " 7.12043329e-05 1.74927118e-05 9.93561571e-06 2.81908960e-05\n",
      " 9.92204070e-01 9.41554143e-04 9.98582959e-01 8.03365409e-01\n",
      " 9.70267653e-01 6.08747796e-05 3.26806621e-05 9.99660134e-01\n",
      " 9.06549096e-01 9.90406454e-01 3.79191304e-04 9.14354750e-05\n",
      " 2.21935496e-01 9.99370992e-01 2.19391271e-01 1.17940770e-03\n",
      " 9.83563185e-01 9.99568164e-01 9.99616742e-01 2.33434999e-04\n",
      " 4.04963590e-04 1.76605317e-04 9.99952555e-01 2.06051096e-02\n",
      " 5.69843687e-05 6.76613161e-03 2.07409915e-03 4.12586262e-04\n",
      " 8.61573426e-05 2.35943386e-04 9.87257898e-01 3.01752407e-05\n",
      " 2.60336856e-05 9.99893785e-01 6.33151794e-05 4.97414221e-05\n",
      " 1.72888722e-05 4.48721112e-04 5.60618173e-05 1.03819148e-04\n",
      " 2.97916657e-03 5.66627248e-04 3.76757307e-05 3.66641434e-05\n",
      " 1.59716106e-03 1.59308780e-04 5.34502454e-02 9.96249378e-01\n",
      " 5.82752284e-04 6.08192495e-05 2.54063606e-01 2.27236960e-04\n",
      " 9.77794407e-05 9.97033119e-01 3.74624906e-05 1.04534105e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 62 [0/106 (0%)]\tTrain Loss: 0.000169\n",
      "Train Epoch: 62 [10/106 (9%)]\tTrain Loss: 0.001513\n",
      "Train Epoch: 62 [20/106 (19%)]\tTrain Loss: 0.051051\n",
      "Train Epoch: 62 [30/106 (28%)]\tTrain Loss: 0.000183\n",
      "Train Epoch: 62 [40/106 (38%)]\tTrain Loss: 0.003026\n",
      "Train Epoch: 62 [50/106 (47%)]\tTrain Loss: 0.000086\n",
      "Train Epoch: 62 [60/106 (57%)]\tTrain Loss: 0.000333\n",
      "Train Epoch: 62 [70/106 (66%)]\tTrain Loss: 0.005942\n",
      "Train Epoch: 62 [80/106 (75%)]\tTrain Loss: 0.003755\n",
      "Train Epoch: 62 [90/106 (85%)]\tTrain Loss: 0.000669\n",
      "Train Epoch: 62 [100/106 (94%)]\tTrain Loss: 0.001728\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 402/424 (95%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [4.55905014e-04 3.00481450e-03 1.12040449e-04 4.89110593e-04\n",
      " 2.10743761e-04 1.07462423e-04 3.18777835e-04 4.16642753e-04\n",
      " 1.37268100e-04 9.90820350e-04 1.51128240e-03 6.03779226e-05\n",
      " 2.45956471e-04 3.70902097e-04 7.74453860e-04 2.70924415e-04\n",
      " 1.79034506e-03 6.50389679e-03 8.02824972e-04 9.90766644e-01\n",
      " 4.36942071e-01 5.30248508e-04 9.96245325e-01 9.94263947e-01\n",
      " 7.84471573e-04 9.23154831e-01 9.98956919e-01 1.96840687e-04\n",
      " 1.60876516e-04 1.36074988e-04 2.03612493e-03 1.46614411e-03\n",
      " 3.62000824e-03 1.73452834e-04 6.10495335e-05 7.68675745e-05\n",
      " 1.04644918e-04 5.51442499e-04 2.80143111e-04 6.21846877e-04\n",
      " 1.72178424e-03 3.41712759e-04 6.09310751e-04 2.72637903e-04\n",
      " 9.69140499e-04 5.23934257e-04 3.63529054e-03 5.82755171e-03\n",
      " 3.05412471e-01 1.14893308e-02 1.38253525e-01 6.30165305e-05\n",
      " 2.34755440e-04 6.39363643e-05 2.09724888e-01 4.55832458e-04\n",
      " 2.33155526e-02 7.82655188e-05 1.92789172e-04 3.67745175e-04\n",
      " 9.89066005e-01 9.26064372e-01 9.96820331e-01 9.94107485e-01\n",
      " 9.94755864e-01 4.58554663e-02 7.41057238e-03 9.93210256e-01\n",
      " 8.81488264e-01 9.97923970e-01 3.49690095e-02 3.00064944e-02\n",
      " 9.85255599e-01 9.99227405e-01 9.66803312e-01 5.32041192e-01\n",
      " 9.98584390e-01 9.85384643e-01 9.98693526e-01 1.00183086e-02\n",
      " 4.76119250e-01 1.79134868e-02 9.99999523e-01 4.17140514e-01\n",
      " 1.91659771e-03 5.07208526e-01 3.21769506e-01 3.21034789e-02\n",
      " 1.79901496e-01 4.91970358e-03 9.97306108e-01 3.40961677e-04\n",
      " 2.75635772e-04 9.99997020e-01 1.64974574e-02 4.84603969e-03\n",
      " 3.97424272e-04 4.76376086e-01 3.49847571e-04 9.30889487e-01\n",
      " 7.77729213e-01 5.82082450e-01 2.17949244e-04 5.97043079e-04\n",
      " 6.00271761e-01 2.20879633e-03 9.97158647e-01 7.11662233e-01\n",
      " 1.29364636e-02 2.32146159e-02 9.29611444e-01 2.65743658e-02\n",
      " 1.59595944e-02 8.57194364e-01 4.42192541e-04 2.18217916e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 63 [0/106 (0%)]\tTrain Loss: 0.000615\n",
      "Train Epoch: 63 [10/106 (9%)]\tTrain Loss: 0.000283\n",
      "Train Epoch: 63 [20/106 (19%)]\tTrain Loss: 0.348766\n",
      "Train Epoch: 63 [30/106 (28%)]\tTrain Loss: 0.000187\n",
      "Train Epoch: 63 [40/106 (38%)]\tTrain Loss: 0.001485\n",
      "Train Epoch: 63 [50/106 (47%)]\tTrain Loss: 0.000345\n",
      "Train Epoch: 63 [60/106 (57%)]\tTrain Loss: 0.001840\n",
      "Train Epoch: 63 [70/106 (66%)]\tTrain Loss: 0.000591\n",
      "Train Epoch: 63 [80/106 (75%)]\tTrain Loss: 0.035163\n",
      "Train Epoch: 63 [90/106 (85%)]\tTrain Loss: 0.002022\n",
      "Train Epoch: 63 [100/106 (94%)]\tTrain Loss: 0.001962\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 403/424 (95%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [5.89299998e-05 1.19200186e-03 4.32925226e-05 1.01916026e-04\n",
      " 4.21619661e-05 2.08271140e-05 1.31336885e-04 2.64421338e-04\n",
      " 2.86183622e-05 3.12753255e-04 1.17924377e-04 3.36071098e-05\n",
      " 5.09974379e-05 4.96273824e-05 1.90017774e-04 2.82853907e-05\n",
      " 7.78273752e-05 6.55948669e-02 8.18958142e-05 5.40345907e-01\n",
      " 1.26418239e-03 2.64481583e-04 8.01403344e-01 9.46306407e-01\n",
      " 1.80690957e-04 9.00701523e-01 9.98109579e-01 7.01349200e-05\n",
      " 4.96571520e-05 3.72987233e-05 6.84105381e-02 1.28572837e-01\n",
      " 3.18055972e-03 3.64623993e-05 1.63702734e-05 2.15271539e-05\n",
      " 2.68803051e-05 1.51470001e-03 1.25580365e-04 5.08346347e-05\n",
      " 8.58052808e-05 3.86820138e-05 3.13387776e-04 6.01455286e-05\n",
      " 2.24136442e-04 1.46661856e-04 1.11922109e-03 1.73839252e-03\n",
      " 1.96202785e-01 5.70912845e-04 1.07744135e-01 1.41815281e-05\n",
      " 5.86146198e-05 2.86976720e-05 2.43949573e-04 5.99220257e-05\n",
      " 3.73811345e-03 1.92070256e-05 6.46593835e-05 8.66028058e-05\n",
      " 9.97057676e-01 8.62819016e-01 9.98694241e-01 9.98279810e-01\n",
      " 9.98390198e-01 9.87624943e-01 7.35273287e-02 9.99336064e-01\n",
      " 9.78570402e-01 9.99717414e-01 1.41953281e-03 1.28712715e-03\n",
      " 9.96740162e-01 9.99653697e-01 9.98610973e-01 9.95882154e-01\n",
      " 9.99575794e-01 9.98072267e-01 9.97117400e-01 6.77462518e-01\n",
      " 9.82312620e-01 8.93897638e-02 9.99997139e-01 9.91625011e-01\n",
      " 7.02549238e-04 9.34416234e-01 7.99279660e-02 4.25138464e-03\n",
      " 1.80349604e-03 6.93859649e-04 9.88013864e-01 1.83346638e-04\n",
      " 1.40755801e-04 9.99201953e-01 4.44475038e-04 1.92629508e-04\n",
      " 6.95841154e-05 5.75339794e-01 5.43758433e-05 1.55145198e-01\n",
      " 2.26724334e-03 9.76490498e-01 4.70823252e-05 3.01281852e-03\n",
      " 9.88533139e-01 4.39224910e-04 9.94033992e-01 4.55324687e-02\n",
      " 2.86801544e-04 3.65986285e-04 1.04502223e-01 9.53869778e-04\n",
      " 5.91184653e-04 3.48874815e-02 7.47329686e-05 7.93093859e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 64 [0/106 (0%)]\tTrain Loss: 0.000239\n",
      "Train Epoch: 64 [10/106 (9%)]\tTrain Loss: 0.000454\n",
      "Train Epoch: 64 [20/106 (19%)]\tTrain Loss: 0.000051\n",
      "Train Epoch: 64 [30/106 (28%)]\tTrain Loss: 0.025281\n",
      "Train Epoch: 64 [40/106 (38%)]\tTrain Loss: 0.002684\n",
      "Train Epoch: 64 [50/106 (47%)]\tTrain Loss: 0.000068\n",
      "Train Epoch: 64 [60/106 (57%)]\tTrain Loss: 0.000809\n",
      "Train Epoch: 64 [70/106 (66%)]\tTrain Loss: 0.000774\n",
      "Train Epoch: 64 [80/106 (75%)]\tTrain Loss: 0.000048\n",
      "Train Epoch: 64 [90/106 (85%)]\tTrain Loss: 0.000522\n",
      "Train Epoch: 64 [100/106 (94%)]\tTrain Loss: 0.000089\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 415/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.86889411e-05 2.87786499e-02 1.00395582e-05 1.34987476e-05\n",
      " 6.87630882e-06 2.26140219e-06 4.42522796e-05 1.80627030e-05\n",
      " 3.33659113e-06 1.90195024e-05 4.45205151e-05 2.27462851e-06\n",
      " 1.79080471e-05 7.08416555e-05 2.77833809e-04 9.82855727e-06\n",
      " 7.86174787e-04 1.09889859e-03 9.78882235e-05 9.96848285e-01\n",
      " 9.60620522e-01 3.30532494e-04 9.99255478e-01 9.98952508e-01\n",
      " 5.75355953e-05 9.99505758e-01 9.99577105e-01 1.14612667e-05\n",
      " 1.59358915e-05 1.54343252e-05 8.87989707e-04 7.76727736e-01\n",
      " 1.48261204e-01 1.17327463e-05 2.92335699e-06 3.42962721e-06\n",
      " 5.82898474e-06 2.67977612e-05 2.96367416e-05 1.54112913e-05\n",
      " 1.55612739e-04 3.30661969e-05 2.49654222e-05 1.09121265e-05\n",
      " 5.32301274e-05 2.93251251e-05 3.85153806e-04 5.73971483e-04\n",
      " 3.33345026e-01 1.40309881e-03 2.34619789e-02 2.36033611e-06\n",
      " 8.89102830e-06 3.00730812e-06 9.96335983e-01 1.23685795e-05\n",
      " 1.16525656e-02 4.13081807e-06 7.40043151e-06 1.86774669e-05\n",
      " 9.99639511e-01 9.95397627e-01 9.99779284e-01 9.99659777e-01\n",
      " 9.99235272e-01 6.54882714e-02 2.16066209e-03 9.99550521e-01\n",
      " 9.94181097e-01 9.99923468e-01 1.89205222e-02 1.24366041e-02\n",
      " 9.99345005e-01 9.99833345e-01 9.91242766e-01 9.97744441e-01\n",
      " 9.99725401e-01 9.98454809e-01 9.99768555e-01 9.61661816e-01\n",
      " 9.99448717e-01 9.82289076e-01 9.99969482e-01 9.98027861e-01\n",
      " 2.92256922e-01 9.97668564e-01 9.75810170e-01 1.94962770e-02\n",
      " 9.46675718e-01 4.20083225e-01 9.98840630e-01 1.73775534e-05\n",
      " 1.92694588e-05 9.99697328e-01 2.45773017e-01 4.62338823e-04\n",
      " 1.68794122e-05 9.96907055e-01 6.18890481e-05 9.97184575e-01\n",
      " 1.27880991e-01 9.84378517e-01 1.25521456e-05 3.39083694e-04\n",
      " 9.98802543e-01 2.06318730e-03 9.99803364e-01 9.87510324e-01\n",
      " 6.82937920e-01 1.25132687e-03 9.82966661e-01 1.24998661e-02\n",
      " 1.60464063e-01 9.98038113e-01 3.89317393e-05 8.08034651e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 65 [0/106 (0%)]\tTrain Loss: 0.000085\n",
      "Train Epoch: 65 [10/106 (9%)]\tTrain Loss: 0.000079\n",
      "Train Epoch: 65 [20/106 (19%)]\tTrain Loss: 0.000170\n",
      "Train Epoch: 65 [30/106 (28%)]\tTrain Loss: 0.000408\n",
      "Train Epoch: 65 [40/106 (38%)]\tTrain Loss: 0.000192\n",
      "Train Epoch: 65 [50/106 (47%)]\tTrain Loss: 0.000102\n",
      "Train Epoch: 65 [60/106 (57%)]\tTrain Loss: 0.000086\n",
      "Train Epoch: 65 [70/106 (66%)]\tTrain Loss: 0.005181\n",
      "Train Epoch: 65 [80/106 (75%)]\tTrain Loss: 0.003121\n",
      "Train Epoch: 65 [90/106 (85%)]\tTrain Loss: 0.000109\n",
      "Train Epoch: 65 [100/106 (94%)]\tTrain Loss: 0.000717\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 402/424 (95%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.51460757e-04 1.08732812e-01 9.53651543e-05 1.24203929e-04\n",
      " 8.21738358e-05 2.85904534e-05 2.61301408e-04 1.37205803e-04\n",
      " 2.14307584e-05 8.58974527e-05 2.60612200e-04 2.05734086e-05\n",
      " 1.04994644e-04 1.01140540e-04 2.85768736e-04 1.23379636e-04\n",
      " 5.99236169e-04 8.97850376e-04 2.41682632e-04 9.98101532e-01\n",
      " 1.62695833e-02 6.36262703e-04 9.75286305e-01 9.99096394e-01\n",
      " 3.99325596e-04 9.93519127e-01 9.99796808e-01 7.63117714e-05\n",
      " 8.90810988e-05 1.00046833e-04 3.11198004e-04 4.58570709e-03\n",
      " 3.03427293e-03 1.26904037e-04 4.03899176e-05 4.04519815e-05\n",
      " 8.06977769e-05 1.53499728e-04 7.92155770e-05 6.33332966e-05\n",
      " 1.18496093e-04 5.20200847e-05 2.37485321e-04 9.92662535e-05\n",
      " 2.13792940e-04 1.82747637e-04 1.66679337e-03 1.86468475e-03\n",
      " 3.63226086e-01 7.29593623e-04 1.52698578e-02 3.66110398e-05\n",
      " 6.76514392e-05 3.57158024e-05 5.39645292e-02 1.05482468e-04\n",
      " 5.63082518e-04 4.45651131e-05 3.73296134e-05 1.11185916e-04\n",
      " 9.99540091e-01 9.86452222e-01 9.99878883e-01 9.99489784e-01\n",
      " 9.99656081e-01 1.10013026e-03 6.50953327e-04 9.99922156e-01\n",
      " 9.48907197e-01 9.99926329e-01 5.58001827e-03 1.23282650e-03\n",
      " 9.99069631e-01 9.99947429e-01 9.54332352e-01 9.57898796e-01\n",
      " 9.99967098e-01 9.99933004e-01 9.99559104e-01 1.36169270e-02\n",
      " 9.98612642e-01 4.80291955e-02 9.99999404e-01 9.86661732e-01\n",
      " 9.26114246e-03 9.83205616e-01 7.92667627e-01 4.00120160e-03\n",
      " 9.26798955e-02 4.70910966e-03 9.99222755e-01 8.84204564e-05\n",
      " 7.82356001e-05 9.99846697e-01 1.82605069e-03 2.11618826e-04\n",
      " 7.17645962e-05 2.70545304e-01 9.16377976e-05 9.87214446e-01\n",
      " 2.54195016e-02 2.65635252e-01 1.01425270e-04 2.17574066e-04\n",
      " 9.46820080e-01 5.63134265e-04 9.99557436e-01 9.95053589e-01\n",
      " 1.00924494e-02 1.86001067e-03 9.79625881e-01 3.46510881e-03\n",
      " 1.79115068e-02 9.98232722e-01 1.40845077e-04 2.92320532e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 66 [0/106 (0%)]\tTrain Loss: 0.001149\n",
      "Train Epoch: 66 [10/106 (9%)]\tTrain Loss: 0.002188\n",
      "Train Epoch: 66 [20/106 (19%)]\tTrain Loss: 0.000128\n",
      "Train Epoch: 66 [30/106 (28%)]\tTrain Loss: 0.000094\n",
      "Train Epoch: 66 [40/106 (38%)]\tTrain Loss: 0.000103\n",
      "Train Epoch: 66 [50/106 (47%)]\tTrain Loss: 0.022055\n",
      "Train Epoch: 66 [60/106 (57%)]\tTrain Loss: 0.014127\n",
      "Train Epoch: 66 [70/106 (66%)]\tTrain Loss: 0.000386\n",
      "Train Epoch: 66 [80/106 (75%)]\tTrain Loss: 0.000071\n",
      "Train Epoch: 66 [90/106 (85%)]\tTrain Loss: 0.000158\n",
      "Train Epoch: 66 [100/106 (94%)]\tTrain Loss: 0.000323\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 409/424 (96%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.76510875e-04 2.31589051e-03 2.74988470e-05 5.99949344e-05\n",
      " 3.51246999e-05 3.90760579e-05 5.14704734e-05 7.30694737e-05\n",
      " 1.71284846e-05 3.58500474e-05 7.12583933e-05 1.00077068e-05\n",
      " 4.57471033e-05 5.84929476e-05 1.23945050e-04 3.36460434e-05\n",
      " 2.73242156e-04 2.31926446e-04 6.54403557e-05 9.93313253e-01\n",
      " 7.99124653e-04 1.60597876e-04 9.82473373e-01 9.98540521e-01\n",
      " 9.78469689e-05 9.92142677e-01 9.99664903e-01 3.19634855e-05\n",
      " 2.96573799e-05 2.99412168e-05 1.62331911e-04 6.86463863e-02\n",
      " 6.40293257e-03 5.24998068e-05 1.60823947e-05 1.29644532e-05\n",
      " 2.50236299e-05 5.74616315e-05 3.28997703e-05 2.88864685e-05\n",
      " 2.10052051e-04 3.67811117e-05 6.03298140e-05 3.71007045e-05\n",
      " 5.98157239e-05 8.03045768e-05 4.63891018e-04 5.73542784e-04\n",
      " 1.01465374e-01 4.59900213e-04 4.94155474e-03 1.40480388e-05\n",
      " 2.22481340e-05 1.43865791e-05 8.76627211e-03 3.68086548e-05\n",
      " 2.91819306e-04 1.42693416e-05 2.56291514e-05 3.24807006e-05\n",
      " 9.99461114e-01 9.89121377e-01 9.99872088e-01 9.99494910e-01\n",
      " 9.98720825e-01 1.45106148e-02 2.34866678e-03 9.99827504e-01\n",
      " 9.43311810e-01 9.99902129e-01 5.48085608e-02 6.14904389e-02\n",
      " 9.98911738e-01 9.99893308e-01 9.50364411e-01 9.82756972e-01\n",
      " 9.99742806e-01 9.98909354e-01 9.99140620e-01 9.07285288e-02\n",
      " 9.99001086e-01 4.70551988e-03 9.99978185e-01 9.93329287e-01\n",
      " 1.77084166e-03 9.77442384e-01 5.12090743e-01 7.84030650e-04\n",
      " 2.38942429e-02 2.49471003e-03 9.99071002e-01 4.74011940e-05\n",
      " 3.01345754e-05 9.99155760e-01 6.88742206e-04 7.31793698e-05\n",
      " 2.19557933e-05 3.78711164e-01 3.26346599e-05 9.97679293e-01\n",
      " 2.81983882e-01 8.32683265e-01 6.79857330e-05 4.81296593e-04\n",
      " 9.96575534e-01 1.80875388e-04 9.99580085e-01 9.99236345e-01\n",
      " 1.02617234e-01 3.06500494e-02 9.84805048e-01 1.87714293e-03\n",
      " 1.43094556e-02 9.98972774e-01 4.94442356e-05 1.30018190e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 67 [0/106 (0%)]\tTrain Loss: 0.000076\n",
      "Train Epoch: 67 [10/106 (9%)]\tTrain Loss: 0.000059\n",
      "Train Epoch: 67 [20/106 (19%)]\tTrain Loss: 0.000063\n",
      "Train Epoch: 67 [30/106 (28%)]\tTrain Loss: 0.000046\n",
      "Train Epoch: 67 [40/106 (38%)]\tTrain Loss: 0.002240\n",
      "Train Epoch: 67 [50/106 (47%)]\tTrain Loss: 0.000125\n",
      "Train Epoch: 67 [60/106 (57%)]\tTrain Loss: 0.000158\n",
      "Train Epoch: 67 [70/106 (66%)]\tTrain Loss: 0.000291\n",
      "Train Epoch: 67 [80/106 (75%)]\tTrain Loss: 0.000061\n",
      "Train Epoch: 67 [90/106 (85%)]\tTrain Loss: 0.014056\n",
      "Train Epoch: 67 [100/106 (94%)]\tTrain Loss: 0.000143\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 386/424 (91%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [5.37842097e-05 2.31560436e-03 8.99368024e-06 1.57723625e-05\n",
      " 8.87779424e-06 6.62198408e-06 2.10382386e-05 2.23544503e-05\n",
      " 4.06658273e-06 1.08259701e-05 1.75881923e-05 1.62181402e-06\n",
      " 8.00244834e-06 2.30576861e-05 7.89302576e-05 7.30173952e-06\n",
      " 8.17551991e-05 3.33799922e-04 2.78296739e-05 9.81972396e-01\n",
      " 2.32728751e-04 6.77243879e-05 9.88788307e-01 9.95908260e-01\n",
      " 3.11579679e-05 9.92506206e-01 9.98566687e-01 8.50702145e-06\n",
      " 8.06360367e-06 7.27563156e-06 6.72740643e-05 5.31116799e-02\n",
      " 3.74457389e-02 1.41665778e-05 3.03567322e-06 3.03609590e-06\n",
      " 5.98432280e-06 1.79536000e-05 1.20692366e-05 5.62689138e-06\n",
      " 3.61191560e-05 9.86354371e-06 2.47769312e-05 8.12226153e-06\n",
      " 1.78886567e-05 3.11889999e-05 2.81659508e-04 4.44092177e-04\n",
      " 4.89325300e-02 9.21817336e-05 1.09589240e-03 2.80947847e-06\n",
      " 5.15183137e-06 2.69125167e-06 8.18986446e-04 1.18280150e-05\n",
      " 2.15653534e-04 2.58665364e-06 6.35192464e-06 7.95541564e-06\n",
      " 9.95701492e-01 9.55269337e-01 9.99236584e-01 9.97582316e-01\n",
      " 9.95971262e-01 8.48284806e-04 1.80903426e-03 9.99138594e-01\n",
      " 9.76898968e-01 9.99380231e-01 3.76665480e-02 1.88702084e-02\n",
      " 9.94530022e-01 9.99337614e-01 9.85013425e-01 9.92151260e-01\n",
      " 9.98565733e-01 9.95271504e-01 9.97733712e-01 4.62135077e-02\n",
      " 9.94856954e-01 1.33469258e-03 9.99670148e-01 9.92967129e-01\n",
      " 6.91508362e-03 9.85917747e-01 9.51862574e-01 1.52155268e-03\n",
      " 5.65608032e-02 3.23521579e-03 9.95038092e-01 1.58430630e-05\n",
      " 1.05443651e-05 9.94916201e-01 1.65288773e-04 2.39697292e-05\n",
      " 4.57659826e-06 5.91454685e-01 1.23506170e-05 9.94247794e-01\n",
      " 1.35222822e-01 9.17549849e-01 2.33285391e-05 6.54214600e-05\n",
      " 9.74597037e-01 1.42499368e-04 9.96200264e-01 9.88881171e-01\n",
      " 1.14219601e-03 1.04249576e-02 6.81302845e-01 4.00485063e-04\n",
      " 8.79851810e-04 9.95463669e-01 1.66196933e-05 1.13175658e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 68 [0/106 (0%)]\tTrain Loss: 0.299624\n",
      "Train Epoch: 68 [10/106 (9%)]\tTrain Loss: 0.000076\n",
      "Train Epoch: 68 [20/106 (19%)]\tTrain Loss: 0.000281\n",
      "Train Epoch: 68 [30/106 (28%)]\tTrain Loss: 0.000136\n",
      "Train Epoch: 68 [40/106 (38%)]\tTrain Loss: 0.252196\n",
      "Train Epoch: 68 [50/106 (47%)]\tTrain Loss: 0.000218\n",
      "Train Epoch: 68 [60/106 (57%)]\tTrain Loss: 0.001273\n",
      "Train Epoch: 68 [70/106 (66%)]\tTrain Loss: 0.000049\n",
      "Train Epoch: 68 [80/106 (75%)]\tTrain Loss: 0.000622\n",
      "Train Epoch: 68 [90/106 (85%)]\tTrain Loss: 0.000360\n",
      "Train Epoch: 68 [100/106 (94%)]\tTrain Loss: 0.000305\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 403/424 (95%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.39892625e-04 2.11157370e-03 3.50647279e-05 6.29791684e-05\n",
      " 3.61100283e-05 3.06299626e-05 6.92610629e-05 8.51133154e-05\n",
      " 1.76172343e-05 5.13950908e-05 8.06839162e-05 1.25586121e-05\n",
      " 7.02236575e-05 8.02171708e-05 1.81909330e-04 4.16775401e-05\n",
      " 2.04188735e-04 3.84117768e-04 9.65599756e-05 9.78338599e-01\n",
      " 4.68781189e-04 1.60573385e-04 9.87358987e-01 9.94997859e-01\n",
      " 1.11131973e-04 9.89470124e-01 9.99331594e-01 3.81817481e-05\n",
      " 3.95951429e-05 3.64197949e-05 1.58700073e-04 1.23639209e-02\n",
      " 4.06275690e-03 6.24659733e-05 1.80575207e-05 1.76566627e-05\n",
      " 2.71396584e-05 6.66949563e-05 4.36985138e-05 3.14087411e-05\n",
      " 1.32435918e-04 3.89406523e-05 6.97887954e-05 4.38296447e-05\n",
      " 9.50546601e-05 1.09712608e-04 5.77690487e-04 7.42546050e-04\n",
      " 4.19866145e-02 2.58771121e-04 2.00905767e-03 1.59814736e-05\n",
      " 2.66933912e-05 1.85114895e-05 3.55443708e-03 5.00695132e-05\n",
      " 3.91781941e-04 1.75796613e-05 2.93762369e-05 4.53945759e-05\n",
      " 9.99063313e-01 9.81368124e-01 9.99803722e-01 9.99214888e-01\n",
      " 9.98135686e-01 2.48870556e-03 2.10315990e-03 9.99767840e-01\n",
      " 9.16658521e-01 9.99766648e-01 2.89562866e-02 3.92824225e-02\n",
      " 9.98070657e-01 9.99783576e-01 9.88504350e-01 9.73310053e-01\n",
      " 9.99757230e-01 9.99154806e-01 9.98539448e-01 8.35220218e-02\n",
      " 9.98226941e-01 1.50203868e-03 9.99958754e-01 9.92147028e-01\n",
      " 5.58088860e-03 9.51836228e-01 2.48086601e-01 8.04082025e-04\n",
      " 1.86208971e-02 2.66059744e-03 9.98437107e-01 5.65157134e-05\n",
      " 4.24062418e-05 9.98423815e-01 2.53461418e-04 8.41371293e-05\n",
      " 3.23706081e-05 1.61011457e-01 4.28680578e-05 9.94755745e-01\n",
      " 2.82265216e-01 6.67909920e-01 8.14274899e-05 2.89129443e-04\n",
      " 9.85397279e-01 2.33986750e-04 9.98187959e-01 9.98374343e-01\n",
      " 2.67587253e-03 2.30843797e-02 6.23040915e-01 5.12375613e-04\n",
      " 2.55092466e-03 9.97934580e-01 6.09819181e-05 2.31374725e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 69 [0/106 (0%)]\tTrain Loss: 0.008742\n",
      "Train Epoch: 69 [10/106 (9%)]\tTrain Loss: 0.001900\n",
      "Train Epoch: 69 [20/106 (19%)]\tTrain Loss: 0.000068\n",
      "Train Epoch: 69 [30/106 (28%)]\tTrain Loss: 0.001084\n",
      "Train Epoch: 69 [40/106 (38%)]\tTrain Loss: 0.069428\n",
      "Train Epoch: 69 [50/106 (47%)]\tTrain Loss: 0.000134\n",
      "Train Epoch: 69 [60/106 (57%)]\tTrain Loss: 0.000059\n",
      "Train Epoch: 69 [70/106 (66%)]\tTrain Loss: 0.002896\n",
      "Train Epoch: 69 [80/106 (75%)]\tTrain Loss: 0.000119\n",
      "Train Epoch: 69 [90/106 (85%)]\tTrain Loss: 0.000750\n",
      "Train Epoch: 69 [100/106 (94%)]\tTrain Loss: 0.000206\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 406/424 (96%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.64946827e-05 1.51022652e-03 8.65104539e-06 1.46402190e-05\n",
      " 8.82441418e-06 7.74149794e-06 2.19299291e-05 2.27891069e-05\n",
      " 4.08350752e-06 1.17380287e-05 1.65780439e-05 2.44047828e-06\n",
      " 1.23184582e-05 1.74688084e-05 6.75963674e-05 1.03802877e-05\n",
      " 8.86355629e-05 9.80909826e-05 2.65973140e-05 9.91538286e-01\n",
      " 5.61790890e-04 4.30029722e-05 9.88542438e-01 9.97783840e-01\n",
      " 4.29598149e-05 9.95419741e-01 9.99416590e-01 7.66403718e-06\n",
      " 7.03227624e-06 6.23161714e-06 4.98954432e-05 2.46779695e-02\n",
      " 1.45561725e-03 1.55473535e-05 3.67934672e-06 3.44418686e-06\n",
      " 6.39231757e-06 1.79448853e-05 1.09990124e-05 8.20429887e-06\n",
      " 1.77954891e-04 1.84582514e-05 1.85984773e-05 9.80799541e-06\n",
      " 2.48761098e-05 2.11562801e-05 1.36994684e-04 1.69577033e-04\n",
      " 3.15689854e-02 1.27012871e-04 1.25320477e-03 3.20057507e-06\n",
      " 5.80789310e-06 3.35950358e-06 2.84370612e-02 1.01697942e-05\n",
      " 1.24345432e-04 3.25001247e-06 6.27473264e-06 1.06280668e-05\n",
      " 9.99307036e-01 9.93996859e-01 9.99710381e-01 9.99297023e-01\n",
      " 9.97823715e-01 2.85565131e-03 1.16360479e-03 9.99484539e-01\n",
      " 7.24501193e-01 9.99648809e-01 3.25672030e-02 3.09351217e-02\n",
      " 9.97722208e-01 9.99701560e-01 9.93182480e-01 9.73495305e-01\n",
      " 9.99306083e-01 9.97835815e-01 9.98368680e-01 4.35103364e-02\n",
      " 9.97817755e-01 3.91425472e-03 9.99877810e-01 9.94561732e-01\n",
      " 1.81998906e-03 9.78050053e-01 6.58095181e-01 4.04148304e-04\n",
      " 3.18317637e-02 1.65080582e-03 9.98206496e-01 1.33367776e-05\n",
      " 8.94242476e-06 9.98560607e-01 1.57034287e-04 2.74891463e-05\n",
      " 7.10905806e-06 5.11682391e-01 1.05600930e-05 9.97465730e-01\n",
      " 3.83303285e-01 9.44027781e-01 1.83887114e-05 1.33114518e-04\n",
      " 9.71400857e-01 5.89983138e-05 9.98454332e-01 9.97532487e-01\n",
      " 2.56387498e-02 3.60531211e-02 9.53500032e-01 6.91504276e-04\n",
      " 9.24592000e-03 9.97269690e-01 1.73000717e-05 9.02848697e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 70 [0/106 (0%)]\tTrain Loss: 0.000099\n",
      "Train Epoch: 70 [10/106 (9%)]\tTrain Loss: 0.000257\n",
      "Train Epoch: 70 [20/106 (19%)]\tTrain Loss: 0.000209\n",
      "Train Epoch: 70 [30/106 (28%)]\tTrain Loss: 0.000250\n",
      "Train Epoch: 70 [40/106 (38%)]\tTrain Loss: 0.000119\n",
      "Train Epoch: 70 [50/106 (47%)]\tTrain Loss: 0.000464\n",
      "Train Epoch: 70 [60/106 (57%)]\tTrain Loss: 0.001747\n",
      "Train Epoch: 70 [70/106 (66%)]\tTrain Loss: 0.000257\n",
      "Train Epoch: 70 [80/106 (75%)]\tTrain Loss: 0.000141\n",
      "Train Epoch: 70 [90/106 (85%)]\tTrain Loss: 0.020755\n",
      "Train Epoch: 70 [100/106 (94%)]\tTrain Loss: 0.000713\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 415/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.89066025e-04 1.37810670e-02 8.33538725e-05 1.44660837e-04\n",
      " 8.05057789e-05 7.32270419e-05 2.09144739e-04 2.01248462e-04\n",
      " 3.78523764e-05 1.27744192e-04 2.65775045e-04 3.53443829e-05\n",
      " 2.36902997e-04 1.68173414e-04 4.37419454e-04 1.43734796e-04\n",
      " 1.12539576e-03 5.57019084e-04 2.35547355e-04 9.96944368e-01\n",
      " 3.54171470e-02 5.60163520e-04 9.98420238e-01 9.99298573e-01\n",
      " 4.87789686e-04 9.98847365e-01 9.99843717e-01 8.72695891e-05\n",
      " 9.68649474e-05 9.23691405e-05 8.76232516e-04 5.12359142e-01\n",
      " 3.61587922e-03 1.28335858e-04 4.58083996e-05 3.84995874e-05\n",
      " 7.05243074e-05 2.30312318e-04 9.80070254e-05 8.64286558e-05\n",
      " 9.38004290e-04 1.75010791e-04 1.66897837e-04 1.15610907e-04\n",
      " 3.40784696e-04 2.42839611e-04 1.30998238e-03 1.67261646e-03\n",
      " 6.79660022e-01 1.74211443e-03 1.13885738e-01 3.57934114e-05\n",
      " 8.09254125e-05 4.14362294e-05 7.54155040e-01 9.60368197e-05\n",
      " 9.19644814e-03 4.73767941e-05 6.02611544e-05 1.46644656e-04\n",
      " 9.99728739e-01 9.95613813e-01 9.99921441e-01 9.99765456e-01\n",
      " 9.99591529e-01 7.47424483e-01 1.25061367e-02 9.99912977e-01\n",
      " 9.52119827e-01 9.99969244e-01 2.60712415e-01 5.31527162e-01\n",
      " 9.99453604e-01 9.99947786e-01 9.97468829e-01 9.94925976e-01\n",
      " 9.99945760e-01 9.99908328e-01 9.99644160e-01 3.45689893e-01\n",
      " 9.99718606e-01 3.74207526e-01 9.99999166e-01 9.97587562e-01\n",
      " 6.00592494e-02 9.87332404e-01 8.66306424e-01 3.25126084e-03\n",
      " 2.18038112e-01 3.74551839e-03 9.99618530e-01 1.40106349e-04\n",
      " 8.33247177e-05 9.99881506e-01 1.04884580e-02 6.68201596e-04\n",
      " 1.00056954e-04 9.61149454e-01 9.96615709e-05 9.98492956e-01\n",
      " 8.13304424e-01 9.86946702e-01 1.98980953e-04 5.04454970e-03\n",
      " 9.99068320e-01 5.60465851e-04 9.99766409e-01 9.99794424e-01\n",
      " 7.21595287e-01 3.19826156e-01 9.97559190e-01 1.29032377e-02\n",
      " 5.01716733e-01 9.99432743e-01 1.44579899e-04 8.77115526e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 29 TN= 55 FN= 27 FP= 5\n",
      "TP+FP 34\n",
      "precision 0.8529411764705882\n",
      "recall 0.5178571428571429\n",
      "F1 0.6444444444444445\n",
      "acc 0.7241379310344828\n",
      "AUCp 0.7172619047619047\n",
      "AUC 0.8416666666666667\n",
      "\n",
      " The epoch is 70, average recall: 0.5179, average precision: 0.8529,average F1: 0.6444, average accuracy: 0.7241, average AUC: 0.8417\n",
      "Train Epoch: 71 [0/106 (0%)]\tTrain Loss: 0.003961\n",
      "Train Epoch: 71 [10/106 (9%)]\tTrain Loss: 0.000755\n",
      "Train Epoch: 71 [20/106 (19%)]\tTrain Loss: 0.004695\n",
      "Train Epoch: 71 [30/106 (28%)]\tTrain Loss: 0.000179\n",
      "Train Epoch: 71 [40/106 (38%)]\tTrain Loss: 0.000118\n",
      "Train Epoch: 71 [50/106 (47%)]\tTrain Loss: 0.000048\n",
      "Train Epoch: 71 [60/106 (57%)]\tTrain Loss: 0.000103\n",
      "Train Epoch: 71 [70/106 (66%)]\tTrain Loss: 0.000611\n",
      "Train Epoch: 71 [80/106 (75%)]\tTrain Loss: 0.000089\n",
      "Train Epoch: 71 [90/106 (85%)]\tTrain Loss: 0.188265\n",
      "Train Epoch: 71 [100/106 (94%)]\tTrain Loss: 0.011252\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 412/424 (97%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [5.51353114e-05 2.06417800e-03 1.91856634e-05 3.85670173e-05\n",
      " 2.26152770e-05 1.79709350e-05 4.55639856e-05 5.37562664e-05\n",
      " 1.05118643e-05 3.01244036e-05 5.12037623e-05 6.91950208e-06\n",
      " 5.00942588e-05 4.99865055e-05 1.83414653e-04 3.12031916e-05\n",
      " 3.26653069e-04 5.18036308e-04 5.97450344e-05 9.94722843e-01\n",
      " 3.11442204e-02 1.35474009e-04 9.87884462e-01 9.97783124e-01\n",
      " 3.01167718e-04 9.94701505e-01 9.99473155e-01 2.33118171e-05\n",
      " 2.02077626e-05 1.94593158e-05 2.39113491e-04 3.72637324e-02\n",
      " 8.40061984e-04 3.67842003e-05 1.02958948e-05 9.75791318e-06\n",
      " 1.68527458e-05 5.83387591e-05 3.18707425e-05 2.95188929e-05\n",
      " 2.42744759e-03 5.70326520e-05 4.40408985e-05 2.41637827e-05\n",
      " 6.65669941e-05 5.10518366e-05 2.25775118e-04 2.70452845e-04\n",
      " 6.83590472e-02 3.71966365e-04 3.27641726e-03 9.10022300e-06\n",
      " 1.62059023e-05 9.54087773e-06 3.11173666e-02 2.33814626e-05\n",
      " 3.21733940e-04 9.35325534e-06 1.50146352e-05 2.85085589e-05\n",
      " 9.99333441e-01 9.93076444e-01 9.99679446e-01 9.99126256e-01\n",
      " 9.98574853e-01 3.36240023e-01 2.52216775e-03 9.99442041e-01\n",
      " 7.56708384e-01 9.99730766e-01 3.67697217e-02 6.35076538e-02\n",
      " 9.97927427e-01 9.99760091e-01 9.91689801e-01 9.58027899e-01\n",
      " 9.99533534e-01 9.98582006e-01 9.98457551e-01 2.35255156e-02\n",
      " 9.97218490e-01 4.08084458e-03 9.99943137e-01 9.95307386e-01\n",
      " 5.42366039e-03 9.27790761e-01 4.64018822e-01 7.34837493e-04\n",
      " 3.23403925e-02 8.41041654e-03 9.98494387e-01 3.61558596e-05\n",
      " 2.16584485e-05 9.99255836e-01 1.03597832e-03 1.23617167e-04\n",
      " 2.54519800e-05 8.95631015e-01 3.34332472e-05 9.97651637e-01\n",
      " 6.03104115e-01 9.67907548e-01 4.66469428e-05 1.43769570e-03\n",
      " 9.94588852e-01 1.31159803e-04 9.96579468e-01 9.99409318e-01\n",
      " 7.13757455e-01 9.56378952e-02 9.94327009e-01 1.82302855e-03\n",
      " 5.07607758e-02 9.96988356e-01 4.63996039e-05 2.52007885e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 72 [0/106 (0%)]\tTrain Loss: 0.000079\n",
      "Train Epoch: 72 [10/106 (9%)]\tTrain Loss: 0.001736\n",
      "Train Epoch: 72 [20/106 (19%)]\tTrain Loss: 0.000758\n",
      "Train Epoch: 72 [30/106 (28%)]\tTrain Loss: 0.012531\n",
      "Train Epoch: 72 [40/106 (38%)]\tTrain Loss: 0.000049\n",
      "Train Epoch: 72 [50/106 (47%)]\tTrain Loss: 0.000534\n",
      "Train Epoch: 72 [60/106 (57%)]\tTrain Loss: 0.000211\n",
      "Train Epoch: 72 [70/106 (66%)]\tTrain Loss: 0.000185\n",
      "Train Epoch: 72 [80/106 (75%)]\tTrain Loss: 0.000067\n",
      "Train Epoch: 72 [90/106 (85%)]\tTrain Loss: 0.002414\n",
      "Train Epoch: 72 [100/106 (94%)]\tTrain Loss: 0.000054\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 420/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.03567414e-04 1.87466375e-03 3.85166641e-05 6.74451221e-05\n",
      " 3.86670399e-05 2.69918910e-05 7.20655225e-05 8.43328089e-05\n",
      " 1.58167040e-05 4.26635743e-05 9.63052444e-05 1.35404816e-05\n",
      " 7.85158627e-05 8.04611045e-05 1.71439577e-04 4.31547269e-05\n",
      " 2.56023253e-04 7.54639332e-04 6.44984029e-05 9.65166986e-01\n",
      " 4.79144423e-04 4.93079249e-04 9.44258153e-01 9.88292038e-01\n",
      " 2.72468751e-04 9.94060934e-01 9.98904586e-01 4.22682024e-05\n",
      " 4.46404883e-05 4.36954324e-05 6.09164999e-04 8.15794319e-02\n",
      " 1.76982535e-03 5.43440146e-05 1.85462977e-05 1.93249834e-05\n",
      " 3.24727698e-05 7.70196784e-05 5.71048113e-05 5.80337983e-05\n",
      " 2.57792417e-03 1.06199754e-04 6.38284182e-05 4.80958588e-05\n",
      " 6.10011157e-05 8.03764415e-05 2.26315431e-04 3.15215351e-04\n",
      " 8.49160738e-03 1.63120916e-04 9.10206174e-04 1.90344708e-05\n",
      " 2.83028730e-05 1.84140517e-05 8.29721161e-04 3.79621633e-05\n",
      " 1.95938395e-03 1.86447760e-05 2.53121707e-05 4.84962657e-05\n",
      " 9.97929335e-01 9.53323185e-01 9.99663949e-01 9.98362243e-01\n",
      " 9.99273479e-01 5.01269568e-03 1.34670269e-03 9.99751985e-01\n",
      " 4.31225777e-01 9.99795258e-01 5.03701717e-03 6.81899861e-03\n",
      " 9.95553315e-01 9.99646187e-01 9.95955348e-01 9.89584386e-01\n",
      " 9.99735773e-01 9.98458147e-01 9.99452889e-01 9.19783860e-03\n",
      " 9.97074842e-01 1.36819133e-03 9.99989033e-01 9.92659032e-01\n",
      " 1.76251996e-02 5.48095703e-01 2.58689106e-01 5.05142263e-04\n",
      " 3.47302505e-03 1.26114918e-03 9.96817231e-01 5.25211362e-05\n",
      " 3.90882087e-05 9.97212946e-01 1.56024005e-04 6.01780011e-05\n",
      " 3.61596867e-05 8.24874878e-01 4.56123234e-05 9.92911041e-01\n",
      " 4.80846018e-01 9.63864803e-01 7.48977764e-05 3.56153789e-04\n",
      " 9.74294662e-01 1.57071743e-04 9.68320310e-01 9.94677186e-01\n",
      " 3.11242472e-02 5.94645599e-03 8.93122256e-01 4.13305679e-04\n",
      " 4.95307613e-04 9.93013084e-01 6.83258477e-05 7.05608691e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 73 [0/106 (0%)]\tTrain Loss: 0.000339\n",
      "Train Epoch: 73 [10/106 (9%)]\tTrain Loss: 0.001660\n",
      "Train Epoch: 73 [20/106 (19%)]\tTrain Loss: 0.000488\n",
      "Train Epoch: 73 [30/106 (28%)]\tTrain Loss: 0.000141\n",
      "Train Epoch: 73 [40/106 (38%)]\tTrain Loss: 0.000592\n",
      "Train Epoch: 73 [50/106 (47%)]\tTrain Loss: 0.000149\n",
      "Train Epoch: 73 [60/106 (57%)]\tTrain Loss: 0.000378\n",
      "Train Epoch: 73 [70/106 (66%)]\tTrain Loss: 0.000081\n",
      "Train Epoch: 73 [80/106 (75%)]\tTrain Loss: 0.000084\n",
      "Train Epoch: 73 [90/106 (85%)]\tTrain Loss: 0.010644\n",
      "Train Epoch: 73 [100/106 (94%)]\tTrain Loss: 0.001522\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 407/424 (96%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.24555071e-04 9.43054855e-02 7.97628672e-05 1.20158358e-04\n",
      " 5.93289842e-05 4.60701594e-05 1.85693294e-04 1.43639729e-04\n",
      " 2.63512065e-05 6.83907783e-05 3.08782270e-04 1.58754610e-05\n",
      " 6.81556470e-04 4.67196660e-04 1.92466890e-03 9.20870079e-05\n",
      " 1.39899319e-03 2.90349731e-03 1.51444139e-04 9.99448836e-01\n",
      " 3.10261385e-03 4.44519741e-04 9.97675121e-01 9.99746382e-01\n",
      " 1.91038343e-04 9.98859406e-01 9.99867439e-01 7.90884806e-05\n",
      " 8.10829952e-05 8.99469669e-05 1.18451240e-03 8.53952467e-01\n",
      " 1.69270504e-02 8.05036325e-05 2.38958273e-05 2.77648596e-05\n",
      " 5.72424287e-05 1.34649905e-04 8.83228786e-05 1.25971463e-04\n",
      " 5.39560290e-03 3.23274930e-04 1.75750087e-04 1.17186559e-04\n",
      " 2.39878878e-04 1.72427783e-04 7.16209528e-04 1.00909173e-03\n",
      " 4.57367152e-02 2.74121267e-04 2.03224691e-03 2.65983545e-05\n",
      " 5.50168734e-05 2.33797236e-05 6.12779148e-03 7.27433362e-05\n",
      " 5.60094602e-03 3.38728169e-05 4.64517980e-05 1.06875224e-04\n",
      " 9.99861598e-01 9.98707891e-01 9.99949098e-01 9.99863863e-01\n",
      " 9.99868989e-01 1.13115706e-01 3.64104286e-03 9.99934196e-01\n",
      " 9.89145100e-01 9.99962926e-01 2.19276547e-01 3.19388866e-01\n",
      " 9.99766648e-01 9.99972582e-01 9.98540878e-01 9.99235749e-01\n",
      " 9.99977827e-01 9.99958992e-01 9.99948978e-01 8.53472054e-01\n",
      " 9.99919415e-01 4.06586498e-01 9.99998927e-01 9.99190867e-01\n",
      " 1.21650331e-01 9.95318115e-01 9.89814401e-01 6.94393972e-03\n",
      " 8.21565449e-01 1.91920530e-02 9.99749124e-01 1.11946909e-04\n",
      " 7.06027713e-05 9.99891520e-01 2.30766414e-03 1.13583577e-04\n",
      " 5.71036690e-05 9.73497093e-01 6.29864953e-05 9.99585688e-01\n",
      " 9.52256322e-01 9.97328281e-01 1.18845528e-04 6.90542674e-03\n",
      " 9.95758235e-01 4.11734247e-04 9.99374807e-01 9.99718606e-01\n",
      " 1.63768791e-02 6.15330189e-02 9.94440079e-01 5.87232236e-04\n",
      " 1.66791480e-03 9.98541594e-01 1.06185878e-04 6.41929801e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 74 [0/106 (0%)]\tTrain Loss: 0.000266\n",
      "Train Epoch: 74 [10/106 (9%)]\tTrain Loss: 0.000145\n",
      "Train Epoch: 74 [20/106 (19%)]\tTrain Loss: 0.277305\n",
      "Train Epoch: 74 [30/106 (28%)]\tTrain Loss: 0.000102\n",
      "Train Epoch: 74 [40/106 (38%)]\tTrain Loss: 0.000700\n",
      "Train Epoch: 74 [50/106 (47%)]\tTrain Loss: 0.000547\n",
      "Train Epoch: 74 [60/106 (57%)]\tTrain Loss: 0.000104\n",
      "Train Epoch: 74 [70/106 (66%)]\tTrain Loss: 0.000557\n",
      "Train Epoch: 74 [80/106 (75%)]\tTrain Loss: 0.019187\n",
      "Train Epoch: 74 [90/106 (85%)]\tTrain Loss: 0.000047\n",
      "Train Epoch: 74 [100/106 (94%)]\tTrain Loss: 0.000054\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 401/424 (95%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [6.35660836e-04 3.56797245e-04 3.91846115e-05 1.02107268e-04\n",
      " 3.72958748e-05 6.83016769e-05 6.26691908e-05 1.54176261e-04\n",
      " 1.77229249e-05 8.26448741e-05 1.02086058e-04 2.27866713e-05\n",
      " 1.74856832e-04 4.88838821e-04 7.21838151e-04 4.04624770e-05\n",
      " 1.97379137e-04 4.94685548e-04 5.05771968e-05 5.25762467e-03\n",
      " 1.18727046e-04 1.39952535e-04 2.90567894e-03 9.90750492e-01\n",
      " 5.97877224e-05 4.81243789e-01 9.99456465e-01 3.94537237e-05\n",
      " 3.91997855e-05 4.54010260e-05 4.39842290e-04 3.26917499e-01\n",
      " 6.49334192e-02 3.91802423e-05 1.68057049e-05 2.06383374e-05\n",
      " 2.98438390e-05 2.04042371e-04 2.00739494e-04 2.25554133e-04\n",
      " 9.74297225e-02 8.10927153e-03 1.20576377e-04 8.67904819e-05\n",
      " 5.77775754e-05 9.04701592e-05 5.66346454e-04 1.02275459e-03\n",
      " 4.07711864e-02 1.07943175e-04 2.78995279e-03 1.60200325e-05\n",
      " 2.33895807e-05 1.53831716e-05 9.65565632e-05 2.58942855e-05\n",
      " 2.36348918e-04 1.49866237e-05 5.07887453e-05 4.06101390e-05\n",
      " 9.99782383e-01 9.96810615e-01 9.99951005e-01 9.99888539e-01\n",
      " 9.98864293e-01 9.23791230e-01 2.08751783e-02 9.99932528e-01\n",
      " 9.97230828e-01 9.99969721e-01 5.60511291e-01 3.83695096e-01\n",
      " 9.99539733e-01 9.99935627e-01 7.19167352e-01 1.39247011e-02\n",
      " 9.99941945e-01 9.99919534e-01 9.99387980e-01 7.64275610e-01\n",
      " 9.99595225e-01 2.18591085e-04 9.99991536e-01 9.99742448e-01\n",
      " 9.26347002e-02 9.91078019e-01 1.53946737e-02 3.01053456e-04\n",
      " 9.92660061e-04 4.02504433e-04 9.97707844e-01 2.08751488e-04\n",
      " 8.96533384e-05 9.39746022e-01 8.39232525e-05 3.48447575e-05\n",
      " 2.61224686e-05 9.27447915e-01 3.18977145e-05 9.99715626e-01\n",
      " 9.98991549e-01 9.99631643e-01 1.14437542e-04 4.30200162e-04\n",
      " 9.72511053e-01 1.80724033e-04 9.97738004e-01 9.99923706e-01\n",
      " 1.09609447e-01 4.19345528e-01 9.94603217e-01 7.69719714e-04\n",
      " 2.84771115e-04 9.91516471e-01 6.65994958e-05 2.85590912e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 75 [0/106 (0%)]\tTrain Loss: 0.000360\n",
      "Train Epoch: 75 [10/106 (9%)]\tTrain Loss: 0.000233\n",
      "Train Epoch: 75 [20/106 (19%)]\tTrain Loss: 0.000226\n",
      "Train Epoch: 75 [30/106 (28%)]\tTrain Loss: 0.000049\n",
      "Train Epoch: 75 [40/106 (38%)]\tTrain Loss: 0.000063\n",
      "Train Epoch: 75 [50/106 (47%)]\tTrain Loss: 0.000075\n",
      "Train Epoch: 75 [60/106 (57%)]\tTrain Loss: 0.000156\n",
      "Train Epoch: 75 [70/106 (66%)]\tTrain Loss: 0.000025\n",
      "Train Epoch: 75 [80/106 (75%)]\tTrain Loss: 0.000386\n",
      "Train Epoch: 75 [90/106 (85%)]\tTrain Loss: 0.000162\n",
      "Train Epoch: 75 [100/106 (94%)]\tTrain Loss: 0.000298\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 405/424 (96%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [9.18651131e-05 4.68680682e-03 1.95119610e-05 3.33540484e-05\n",
      " 1.97867412e-05 2.30888400e-05 4.36718146e-05 3.91334870e-05\n",
      " 8.71101747e-06 2.61363239e-05 6.22308216e-05 8.58799740e-06\n",
      " 6.17162441e-05 4.00332734e-04 1.76165986e-03 3.15034122e-05\n",
      " 2.32759383e-04 4.34744463e-04 4.94114247e-05 8.80216837e-01\n",
      " 9.47231165e-05 2.19561756e-04 9.84497547e-01 9.96503234e-01\n",
      " 4.66875281e-05 9.97880101e-01 9.99630570e-01 2.45313568e-05\n",
      " 1.46371194e-05 1.66414193e-05 9.07096313e-04 5.62300198e-02\n",
      " 7.57935690e-03 2.36969427e-05 1.01798851e-05 1.22432439e-05\n",
      " 2.50418416e-05 4.76234309e-05 4.44757825e-05 8.04555821e-05\n",
      " 4.18399787e-03 2.30253238e-04 5.89830670e-05 4.04930433e-05\n",
      " 2.13156382e-05 2.40783720e-05 1.41401921e-04 3.49475391e-04\n",
      " 4.00423352e-03 1.18272917e-04 1.06264791e-03 1.08482527e-05\n",
      " 1.58701623e-05 8.42651934e-06 1.24235277e-03 1.47619103e-05\n",
      " 5.81941055e-03 8.63916557e-06 1.86084853e-05 2.39548299e-05\n",
      " 9.98396099e-01 9.63157952e-01 9.99677777e-01 9.99380946e-01\n",
      " 9.98514950e-01 1.28601072e-03 3.21694114e-03 9.99555051e-01\n",
      " 9.90440249e-01 9.99901295e-01 2.41704006e-02 2.13988149e-03\n",
      " 9.95916069e-01 9.99687195e-01 9.97352600e-01 9.51247215e-01\n",
      " 9.99909639e-01 9.99886513e-01 9.99857187e-01 5.75381339e-01\n",
      " 9.97206628e-01 1.13266371e-02 9.99995470e-01 9.96630371e-01\n",
      " 4.36529703e-03 9.61213887e-01 9.23274279e-01 1.17340079e-03\n",
      " 6.07171297e-01 5.50895114e-04 9.98754144e-01 7.17711664e-05\n",
      " 2.53775252e-05 9.98286903e-01 4.70265877e-05 1.31432171e-05\n",
      " 1.45107133e-05 9.79051709e-01 2.00028189e-05 9.96460140e-01\n",
      " 9.97384012e-01 9.94036317e-01 3.14653153e-05 1.09346329e-04\n",
      " 2.14010966e-03 1.31899025e-04 9.99384642e-01 8.28403831e-01\n",
      " 1.78678241e-02 8.55397284e-02 9.66975570e-01 1.01906445e-03\n",
      " 4.48493403e-04 9.76054430e-01 4.49168547e-05 4.72344022e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 76 [0/106 (0%)]\tTrain Loss: 0.000041\n",
      "Train Epoch: 76 [10/106 (9%)]\tTrain Loss: 0.001165\n",
      "Train Epoch: 76 [20/106 (19%)]\tTrain Loss: 0.000152\n",
      "Train Epoch: 76 [30/106 (28%)]\tTrain Loss: 0.000028\n",
      "Train Epoch: 76 [40/106 (38%)]\tTrain Loss: 0.000192\n",
      "Train Epoch: 76 [50/106 (47%)]\tTrain Loss: 0.020194\n",
      "Train Epoch: 76 [60/106 (57%)]\tTrain Loss: 0.002181\n",
      "Train Epoch: 76 [70/106 (66%)]\tTrain Loss: 0.000100\n",
      "Train Epoch: 76 [80/106 (75%)]\tTrain Loss: 0.000234\n",
      "Train Epoch: 76 [90/106 (85%)]\tTrain Loss: 0.000445\n",
      "Train Epoch: 76 [100/106 (94%)]\tTrain Loss: 0.264154\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 401/424 (95%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.04641316e-04 1.99215651e-01 4.30481945e-04 2.94353697e-04\n",
      " 2.09951919e-04 6.34223834e-05 5.29536570e-04 2.38239634e-04\n",
      " 1.29991913e-05 2.04950789e-04 4.99703269e-03 1.69881605e-05\n",
      " 1.08419880e-02 4.19308525e-03 1.37310801e-02 1.92718208e-03\n",
      " 2.19501602e-03 3.12012457e-03 2.04354245e-03 9.98533726e-01\n",
      " 3.09950393e-03 9.19288606e-04 9.96218145e-01 9.98026550e-01\n",
      " 7.49928644e-04 4.68975514e-01 9.99692202e-01 9.29984308e-05\n",
      " 1.83680677e-04 6.23457832e-04 1.25981693e-04 1.10494355e-02\n",
      " 1.23728672e-02 2.14073603e-04 4.53152534e-05 2.81896966e-04\n",
      " 7.62947195e-04 2.87930918e-04 1.76292873e-04 3.42194699e-02\n",
      " 7.75087893e-01 3.44431967e-01 2.30891514e-03 3.43054556e-03\n",
      " 5.30082406e-03 2.58733606e-04 1.99513789e-02 5.88935427e-02\n",
      " 9.27835584e-01 5.26122097e-03 2.36580476e-01 5.93605619e-05\n",
      " 1.17350704e-04 7.33789348e-05 2.44831503e-03 2.97566410e-04\n",
      " 7.87168508e-04 1.45371727e-04 3.22384512e-05 5.93119650e-04\n",
      " 9.95530188e-01 8.37793529e-01 9.99027610e-01 9.98100340e-01\n",
      " 9.97621477e-01 5.55628957e-03 2.67725991e-04 9.98834550e-01\n",
      " 9.75897253e-01 5.85075736e-01 2.42393930e-02 6.21522497e-03\n",
      " 9.99507308e-01 9.99836206e-01 9.53120053e-01 9.37033072e-03\n",
      " 9.99772489e-01 9.99440134e-01 9.99284446e-01 8.29308480e-02\n",
      " 9.95098293e-01 9.97312546e-01 9.99999523e-01 2.12788507e-01\n",
      " 1.98917798e-04 8.70281875e-01 1.71303615e-01 2.58642193e-02\n",
      " 1.85191646e-01 6.32547634e-03 9.97907758e-01 3.37887119e-04\n",
      " 1.13265647e-03 9.91565824e-01 5.88131195e-04 1.13915696e-04\n",
      " 8.52862489e-04 4.28823242e-03 3.24243505e-04 9.37961817e-01\n",
      " 9.90874410e-01 3.23961210e-03 6.14052333e-05 1.27118058e-03\n",
      " 5.23549039e-03 2.26957863e-03 9.99478042e-01 9.99893069e-01\n",
      " 1.00872735e-03 5.84398047e-04 9.69425321e-01 7.80454138e-04\n",
      " 2.76240474e-03 3.42006758e-02 6.84475875e-04 2.96040857e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 77 [0/106 (0%)]\tTrain Loss: 0.000227\n",
      "Train Epoch: 77 [10/106 (9%)]\tTrain Loss: 0.000276\n",
      "Train Epoch: 77 [20/106 (19%)]\tTrain Loss: 0.000088\n",
      "Train Epoch: 77 [30/106 (28%)]\tTrain Loss: 0.000094\n",
      "Train Epoch: 77 [40/106 (38%)]\tTrain Loss: 0.000124\n",
      "Train Epoch: 77 [50/106 (47%)]\tTrain Loss: 0.000291\n",
      "Train Epoch: 77 [60/106 (57%)]\tTrain Loss: 0.000464\n",
      "Train Epoch: 77 [70/106 (66%)]\tTrain Loss: 0.000058\n",
      "Train Epoch: 77 [80/106 (75%)]\tTrain Loss: 0.020280\n",
      "Train Epoch: 77 [90/106 (85%)]\tTrain Loss: 0.000742\n",
      "Train Epoch: 77 [100/106 (94%)]\tTrain Loss: 0.002688\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 414/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.84183880e-05 3.50706442e-03 1.73079934e-05 3.53849937e-05\n",
      " 2.78883344e-05 1.23251330e-05 5.94448538e-05 7.10887252e-05\n",
      " 5.44619252e-06 6.51626178e-05 8.67572497e-04 6.94845903e-06\n",
      " 1.72554539e-03 2.24035862e-03 8.78741965e-03 3.41566512e-04\n",
      " 5.79084677e-04 1.87963080e-02 3.20760410e-05 9.64289367e-01\n",
      " 4.11474262e-04 9.19350889e-04 9.14594948e-01 9.99125779e-01\n",
      " 4.12733556e-04 3.26785058e-01 9.99958754e-01 4.28859676e-06\n",
      " 1.21435896e-05 1.81346422e-05 1.12375310e-05 6.07163412e-03\n",
      " 4.78761387e-04 7.56901545e-06 5.58859983e-06 7.71419946e-06\n",
      " 1.10437450e-05 6.50467991e-04 5.62568057e-05 2.46915594e-03\n",
      " 3.04419100e-01 9.07378912e-04 1.52064511e-03 2.77159124e-04\n",
      " 3.02820932e-04 2.73388250e-05 1.65347150e-03 7.31074950e-03\n",
      " 2.19233662e-01 8.91325908e-05 1.41728576e-03 1.44607202e-05\n",
      " 1.66691534e-05 1.02793729e-05 3.71278671e-04 7.90518425e-06\n",
      " 4.70144878e-05 3.44650334e-06 1.95752200e-05 5.02820585e-05\n",
      " 9.99914646e-01 9.99262869e-01 9.99950051e-01 9.99693036e-01\n",
      " 2.07385510e-01 2.62447093e-02 1.15714909e-03 9.99825180e-01\n",
      " 9.95940804e-01 1.68372300e-02 1.56365708e-02 9.46555985e-04\n",
      " 9.98367012e-01 9.99751389e-01 3.00135813e-04 5.77034545e-04\n",
      " 9.99866962e-01 9.99920368e-01 3.13791749e-03 1.68260671e-02\n",
      " 8.33567604e-03 2.30468926e-04 9.99992013e-01 9.96632040e-01\n",
      " 1.23074965e-03 8.80230367e-01 9.39883408e-04 1.21548263e-04\n",
      " 8.80418753e-04 4.77787573e-04 9.99425530e-01 3.19296698e-04\n",
      " 1.37524752e-04 9.90205050e-01 4.99279049e-05 1.85156041e-05\n",
      " 1.83162520e-05 3.03523894e-02 1.98760426e-05 2.40664519e-02\n",
      " 9.94958460e-01 5.12136035e-02 1.82448293e-05 8.59453168e-04\n",
      " 2.24849209e-03 2.35598098e-04 3.43967155e-02 6.89052511e-04\n",
      " 6.54793403e-04 3.92115617e-05 3.51655274e-03 2.92556681e-04\n",
      " 3.62213119e-04 4.30917600e-04 5.75330196e-05 2.01207804e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 78 [0/106 (0%)]\tTrain Loss: 0.000393\n",
      "Train Epoch: 78 [10/106 (9%)]\tTrain Loss: 0.022629\n",
      "Train Epoch: 78 [20/106 (19%)]\tTrain Loss: 0.000090\n",
      "Train Epoch: 78 [30/106 (28%)]\tTrain Loss: 0.267955\n",
      "Train Epoch: 78 [40/106 (38%)]\tTrain Loss: 0.001830\n",
      "Train Epoch: 78 [50/106 (47%)]\tTrain Loss: 0.000058\n",
      "Train Epoch: 78 [60/106 (57%)]\tTrain Loss: 0.000089\n",
      "Train Epoch: 78 [70/106 (66%)]\tTrain Loss: 0.000481\n",
      "Train Epoch: 78 [80/106 (75%)]\tTrain Loss: 0.000192\n",
      "Train Epoch: 78 [90/106 (85%)]\tTrain Loss: 0.001266\n",
      "Train Epoch: 78 [100/106 (94%)]\tTrain Loss: 0.000087\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 402/424 (95%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.97719123e-06 1.08820230e-01 1.35711334e-05 6.20290757e-06\n",
      " 2.34101867e-06 1.35419714e-06 7.85494194e-05 5.68226187e-06\n",
      " 5.35748768e-07 4.86304498e-06 1.21543650e-03 7.07130198e-07\n",
      " 3.66082741e-03 2.54049841e-02 8.87693703e-01 3.64218518e-04\n",
      " 9.96878386e-01 6.27474934e-02 6.25725734e-05 9.98209238e-01\n",
      " 9.96054173e-01 9.92967725e-01 9.98054266e-01 9.98801827e-01\n",
      " 3.31033277e-03 9.97032642e-01 9.99084353e-01 8.76896229e-06\n",
      " 2.03437339e-05 1.64688281e-05 4.54018969e-04 2.00339057e-03\n",
      " 1.14212302e-03 4.97663041e-06 1.21218477e-06 1.98692805e-06\n",
      " 2.36942287e-06 1.45560072e-03 6.94977789e-05 1.63406357e-02\n",
      " 9.92982149e-01 4.89428639e-04 1.16351453e-04 3.32613155e-04\n",
      " 5.41437221e-05 4.19101025e-06 1.48957464e-04 6.16433448e-04\n",
      " 7.10241616e-01 1.41668070e-05 4.72414307e-03 9.08376535e-07\n",
      " 7.68945465e-05 1.53466988e-06 1.47677108e-03 3.30526245e-06\n",
      " 4.04939085e-04 9.53899757e-07 1.57271711e-06 5.59254200e-04\n",
      " 9.98659730e-01 9.97256815e-01 9.98775542e-01 9.97805774e-01\n",
      " 9.95496988e-01 9.22376454e-01 3.03276262e-04 9.98761177e-01\n",
      " 9.12940741e-01 9.97460485e-01 5.74012816e-01 1.45550910e-03\n",
      " 9.81075764e-01 9.98281360e-01 9.96427119e-01 9.96576369e-01\n",
      " 9.98851776e-01 9.99372900e-01 9.98566568e-01 1.93287875e-03\n",
      " 9.96891797e-01 9.92082357e-01 9.99426484e-01 9.97094393e-01\n",
      " 1.10153062e-02 9.73813653e-01 9.39517379e-01 2.99482403e-04\n",
      " 9.96261656e-01 9.96763825e-01 9.98525679e-01 1.71747175e-04\n",
      " 5.10417158e-05 9.97084677e-01 2.56333360e-03 2.11695769e-05\n",
      " 2.33108385e-05 9.93274093e-01 3.33566859e-05 9.95799720e-01\n",
      " 9.97644484e-01 9.92713034e-01 1.71386801e-05 1.50862798e-01\n",
      " 9.47816074e-01 5.32706594e-03 9.98334706e-01 9.98990357e-01\n",
      " 9.86583054e-01 9.66528058e-02 3.29351634e-01 2.06739828e-02\n",
      " 9.92664099e-01 9.95516717e-01 1.66941754e-04 8.99439503e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0.]\n",
      "Train Epoch: 79 [0/106 (0%)]\tTrain Loss: 0.000316\n",
      "Train Epoch: 79 [10/106 (9%)]\tTrain Loss: 0.045700\n",
      "Train Epoch: 79 [20/106 (19%)]\tTrain Loss: 0.000084\n",
      "Train Epoch: 79 [30/106 (28%)]\tTrain Loss: 0.000243\n",
      "Train Epoch: 79 [40/106 (38%)]\tTrain Loss: 0.000121\n",
      "Train Epoch: 79 [50/106 (47%)]\tTrain Loss: 0.000199\n",
      "Train Epoch: 79 [60/106 (57%)]\tTrain Loss: 0.002268\n",
      "Train Epoch: 79 [70/106 (66%)]\tTrain Loss: 0.003645\n",
      "Train Epoch: 79 [80/106 (75%)]\tTrain Loss: 0.276865\n",
      "Train Epoch: 79 [90/106 (85%)]\tTrain Loss: 0.000272\n",
      "Train Epoch: 79 [100/106 (94%)]\tTrain Loss: 0.000019\n",
      "\n",
      "Train set: Average loss: 0.0029, Accuracy: 408/424 (96%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.89249313e-05 9.95495915e-01 5.77574647e-05 1.66217542e-05\n",
      " 1.02150689e-05 7.29615658e-06 8.53949487e-02 1.76282610e-05\n",
      " 1.34882498e-06 5.31578189e-06 3.04393470e-05 1.88850015e-06\n",
      " 4.15212453e-05 2.94801919e-03 5.43851815e-02 7.71390405e-05\n",
      " 9.88089263e-01 1.93482816e-01 1.40679590e-02 9.99999881e-01\n",
      " 9.99198616e-01 9.99367297e-01 9.99943256e-01 9.99915123e-01\n",
      " 9.95747745e-01 9.99933481e-01 9.99597251e-01 1.05685858e-05\n",
      " 7.24527708e-05 2.66041898e-05 1.90735955e-05 1.95279397e-04\n",
      " 9.57869488e-05 2.33801911e-05 5.65769142e-06 1.07930746e-05\n",
      " 2.21831779e-05 5.75272061e-05 1.91428589e-05 6.47514462e-05\n",
      " 9.77546930e-01 3.29637114e-05 3.26960908e-05 1.90201736e-05\n",
      " 1.47776018e-05 8.44313763e-06 3.65773267e-05 5.21337097e-05\n",
      " 2.48003408e-01 4.26734914e-06 4.64538782e-04 5.90824402e-06\n",
      " 2.68531676e-06 5.00119995e-06 1.54757610e-04 7.80954724e-06\n",
      " 1.56130263e-04 3.90943342e-06 2.08088341e-06 3.99681403e-06\n",
      " 9.99833345e-01 9.99156117e-01 9.99832869e-01 9.99442160e-01\n",
      " 9.99984503e-01 9.90347862e-01 1.91740453e-01 1.00000000e+00\n",
      " 9.88835037e-01 9.99917507e-01 9.33838606e-01 6.18632697e-02\n",
      " 8.96986902e-01 9.99684691e-01 9.99769509e-01 1.71902850e-02\n",
      " 9.96138990e-01 9.98262107e-01 9.99866843e-01 6.46909550e-02\n",
      " 9.99938965e-01 9.90716994e-01 9.99991894e-01 9.98760223e-01\n",
      " 9.77594018e-01 4.00530905e-01 3.21368803e-04 1.47744604e-05\n",
      " 9.99468744e-01 9.99818623e-01 9.99958277e-01 1.16358142e-05\n",
      " 6.46524995e-06 9.99376357e-01 1.20449156e-04 7.44624231e-06\n",
      " 4.97637393e-06 9.98497367e-01 7.86161050e-03 9.99965310e-01\n",
      " 9.99229431e-01 9.97827470e-01 6.60829537e-05 9.96658444e-01\n",
      " 9.90692317e-01 1.85506418e-04 1.96116164e-01 9.99983549e-01\n",
      " 9.94969666e-01 1.05935838e-02 9.99515176e-01 5.07630706e-01\n",
      " 8.32111895e-01 9.99867320e-01 6.21689309e-04 5.17634558e-04]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.]\n",
      "Train Epoch: 80 [0/106 (0%)]\tTrain Loss: 0.002109\n",
      "Train Epoch: 80 [10/106 (9%)]\tTrain Loss: 0.000077\n",
      "Train Epoch: 80 [20/106 (19%)]\tTrain Loss: 0.000039\n",
      "Train Epoch: 80 [30/106 (28%)]\tTrain Loss: 0.000079\n",
      "Train Epoch: 80 [40/106 (38%)]\tTrain Loss: 0.000275\n",
      "Train Epoch: 80 [50/106 (47%)]\tTrain Loss: 0.000027\n",
      "Train Epoch: 80 [60/106 (57%)]\tTrain Loss: 0.000110\n",
      "Train Epoch: 80 [70/106 (66%)]\tTrain Loss: 0.000536\n",
      "Train Epoch: 80 [80/106 (75%)]\tTrain Loss: 0.000106\n",
      "Train Epoch: 80 [90/106 (85%)]\tTrain Loss: 0.020618\n",
      "Train Epoch: 80 [100/106 (94%)]\tTrain Loss: 0.023003\n",
      "\n",
      "Train set: Average loss: 0.0100, Accuracy: 405/424 (96%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.42234998e-04 9.95786369e-01 2.13122927e-04 3.48695990e-04\n",
      " 4.94257511e-05 2.82250898e-04 5.92305623e-02 1.37392621e-04\n",
      " 4.14143869e-05 6.70507876e-03 2.39370333e-04 4.89195154e-05\n",
      " 1.11866539e-04 9.79001727e-03 4.93710861e-03 1.65489997e-04\n",
      " 9.57341015e-01 1.38683006e-01 4.55910689e-04 9.99997854e-01\n",
      " 9.98287022e-01 9.93268907e-01 9.99696374e-01 9.99975204e-01\n",
      " 9.89852726e-01 9.98671412e-01 9.99393225e-01 6.01469037e-05\n",
      " 2.53810344e-04 3.49243754e-04 1.47477476e-04 3.58650228e-04\n",
      " 3.52056162e-03 1.46742430e-04 4.98787485e-05 7.27071238e-05\n",
      " 1.49147920e-04 9.86579359e-01 1.60610420e-04 4.37059700e-01\n",
      " 9.98665333e-01 1.30369689e-03 4.39055555e-04 2.49093212e-03\n",
      " 2.19453970e-04 9.45266002e-05 3.09546886e-04 1.56512635e-03\n",
      " 9.44883108e-01 2.46974436e-04 1.09752744e-01 2.45264455e-05\n",
      " 1.62085198e-04 4.47367129e-05 6.13927492e-04 1.16231917e-04\n",
      " 6.46715809e-04 2.94048823e-05 6.63207757e-05 2.55672290e-04\n",
      " 9.99596298e-01 9.97740030e-01 9.99483466e-01 9.98655796e-01\n",
      " 9.98904705e-01 9.99581516e-01 9.44670498e-01 9.99871969e-01\n",
      " 9.98437583e-01 9.99939799e-01 9.51113328e-02 2.02696002e-03\n",
      " 9.06758785e-01 9.99654412e-01 9.79593039e-01 2.92742494e-02\n",
      " 9.99152541e-01 9.99358118e-01 9.99789655e-01 7.75768340e-01\n",
      " 9.99232411e-01 9.98502299e-02 9.99904752e-01 9.25678849e-01\n",
      " 9.16684985e-01 2.48408943e-01 1.73357234e-03 3.11871612e-04\n",
      " 9.98252332e-01 9.94973779e-01 9.99914289e-01 2.41463096e-03\n",
      " 5.78287640e-04 9.99993920e-01 5.92563162e-03 3.11566554e-02\n",
      " 1.94885783e-04 9.99203026e-01 2.36352062e-04 9.99957919e-01\n",
      " 9.99409437e-01 9.97292459e-01 2.38176523e-04 9.98337269e-01\n",
      " 9.92414773e-01 3.84040293e-04 9.97151852e-01 9.99998093e-01\n",
      " 9.93750989e-01 7.91316926e-01 9.99644995e-01 8.05539548e-01\n",
      " 9.91982758e-01 9.99576628e-01 2.41244066e-04 2.06740829e-03]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 29 TN= 55 FN= 27 FP= 5\n",
      "TP+FP 34\n",
      "precision 0.8529411764705882\n",
      "recall 0.5178571428571429\n",
      "F1 0.6444444444444445\n",
      "acc 0.7241379310344828\n",
      "AUCp 0.7172619047619047\n",
      "AUC 0.8321428571428571\n",
      "\n",
      " The epoch is 80, average recall: 0.5179, average precision: 0.8529,average F1: 0.6444, average accuracy: 0.7241, average AUC: 0.8321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 81 [0/106 (0%)]\tTrain Loss: 0.000155\n",
      "Train Epoch: 81 [10/106 (9%)]\tTrain Loss: 0.231693\n",
      "Train Epoch: 81 [20/106 (19%)]\tTrain Loss: 0.000376\n",
      "Train Epoch: 81 [30/106 (28%)]\tTrain Loss: 0.000621\n",
      "Train Epoch: 81 [40/106 (38%)]\tTrain Loss: 0.013020\n",
      "Train Epoch: 81 [50/106 (47%)]\tTrain Loss: 0.003687\n",
      "Train Epoch: 81 [60/106 (57%)]\tTrain Loss: 0.000357\n",
      "Train Epoch: 81 [70/106 (66%)]\tTrain Loss: 0.000115\n",
      "Train Epoch: 81 [80/106 (75%)]\tTrain Loss: 0.000137\n",
      "Train Epoch: 81 [90/106 (85%)]\tTrain Loss: 0.000301\n",
      "Train Epoch: 81 [100/106 (94%)]\tTrain Loss: 0.000843\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 409/424 (96%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [5.08723861e-06 9.99573290e-01 3.83859042e-05 6.30037266e-06\n",
      " 4.41650991e-06 3.93985147e-06 9.28200960e-01 7.29392377e-06\n",
      " 9.55248993e-07 3.68032238e-06 5.36909556e-06 1.31084914e-06\n",
      " 4.75233219e-06 2.91352426e-05 6.34796233e-05 3.17610102e-06\n",
      " 6.90311026e-06 2.96966106e-01 4.15350451e-06 9.99943137e-01\n",
      " 8.66148293e-01 3.19744518e-04 9.98979867e-01 9.99961853e-01\n",
      " 1.90296596e-05 9.96576846e-01 9.99992847e-01 3.51455628e-06\n",
      " 4.27328678e-06 9.24599590e-06 1.64792909e-05 5.03511583e-05\n",
      " 2.14376734e-04 3.66677818e-06 2.47110438e-06 3.18015327e-06\n",
      " 4.07194966e-06 6.70722395e-04 1.24372791e-05 4.31787339e-06\n",
      " 2.75341154e-05 3.54622102e-06 8.38362321e-05 8.56336374e-06\n",
      " 5.60205763e-06 6.02961200e-06 3.01005675e-05 5.79854495e-05\n",
      " 9.29794565e-04 4.07794960e-06 8.51557779e-05 1.86698287e-06\n",
      " 3.18076900e-06 3.32987111e-06 3.38047248e-05 4.23464917e-06\n",
      " 5.52763871e-04 2.41483804e-06 2.69846555e-06 4.85038163e-06\n",
      " 9.99994159e-01 9.99896646e-01 9.99989986e-01 9.99809086e-01\n",
      " 9.99983072e-01 6.05866371e-04 5.70657663e-04 9.99987245e-01\n",
      " 9.99313831e-01 9.08394575e-01 3.99941433e-04 1.00907026e-04\n",
      " 2.06211820e-01 9.79521334e-01 5.69946645e-03 9.98978615e-01\n",
      " 9.99440849e-01 9.98292983e-01 1.00000000e+00 9.60528970e-01\n",
      " 9.99878287e-01 9.99806345e-01 9.99999642e-01 1.59268051e-01\n",
      " 5.09382371e-05 3.55233759e-01 9.32376385e-01 7.43054698e-05\n",
      " 9.95055437e-01 4.25270118e-05 9.98805165e-01 4.50376319e-06\n",
      " 4.61147683e-05 9.99929905e-01 9.90116496e-06 3.75947934e-06\n",
      " 2.96944131e-06 9.99304891e-01 1.36231574e-05 9.98404920e-01\n",
      " 9.54596698e-01 9.97664452e-01 1.37622010e-05 2.50031844e-05\n",
      " 4.92460349e-05 1.15277817e-05 9.61472809e-01 2.54460727e-04\n",
      " 1.47265619e-05 1.21378362e-04 1.47137880e-05 4.21532022e-05\n",
      " 3.90024215e-05 4.09220811e-05 1.33166213e-05 2.59038698e-05]\n",
      "predict [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 82 [0/106 (0%)]\tTrain Loss: 0.000358\n",
      "Train Epoch: 82 [10/106 (9%)]\tTrain Loss: 0.072492\n",
      "Train Epoch: 82 [20/106 (19%)]\tTrain Loss: 0.000021\n",
      "Train Epoch: 82 [30/106 (28%)]\tTrain Loss: 0.000415\n",
      "Train Epoch: 82 [40/106 (38%)]\tTrain Loss: 0.000813\n",
      "Train Epoch: 82 [50/106 (47%)]\tTrain Loss: 0.015640\n",
      "Train Epoch: 82 [60/106 (57%)]\tTrain Loss: 0.000708\n",
      "Train Epoch: 82 [70/106 (66%)]\tTrain Loss: 0.000207\n",
      "Train Epoch: 82 [80/106 (75%)]\tTrain Loss: 0.000134\n",
      "Train Epoch: 82 [90/106 (85%)]\tTrain Loss: 0.000353\n",
      "Train Epoch: 82 [100/106 (94%)]\tTrain Loss: 0.001656\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 398/424 (94%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [8.34030070e-05 1.09037175e-03 9.15555283e-05 1.10237575e-04\n",
      " 4.24991558e-05 5.90936397e-05 1.90694758e-04 7.35629874e-05\n",
      " 1.41696992e-05 5.96415521e-05 2.92512937e-04 4.66115489e-05\n",
      " 1.20471668e-04 5.97535836e-05 1.55379355e-04 1.16630554e-05\n",
      " 1.72334112e-05 1.38463511e-03 1.81854375e-05 9.95032668e-01\n",
      " 4.00653156e-03 4.03524796e-03 3.65194003e-03 9.98168349e-01\n",
      " 5.53979844e-05 3.48428002e-04 9.97575223e-01 1.78351002e-05\n",
      " 2.23706356e-05 5.20232607e-05 5.48716380e-05 2.37155880e-04\n",
      " 9.76236362e-04 4.62443713e-05 2.37964032e-05 1.46281891e-05\n",
      " 1.70733310e-05 3.47686524e-04 8.07696779e-05 1.38117020e-05\n",
      " 3.84847554e-05 1.18807857e-05 3.14325240e-04 2.63030612e-04\n",
      " 3.71488459e-05 1.34544287e-04 5.63179376e-04 2.24766321e-03\n",
      " 7.06018209e-02 1.37974671e-03 2.41164211e-02 9.87014937e-06\n",
      " 4.86639728e-05 2.35918687e-05 1.34752772e-04 6.50791626e-05\n",
      " 7.34692658e-05 2.47258795e-05 4.91214560e-05 5.66510462e-05\n",
      " 9.96793449e-01 9.82536197e-01 9.98508871e-01 9.88055825e-01\n",
      " 3.33602238e-03 4.17444680e-04 1.62060460e-04 9.87367809e-01\n",
      " 9.70884144e-01 2.71495692e-02 1.86559546e-03 9.69712739e-04\n",
      " 2.64935638e-03 9.64907229e-01 1.32776308e-03 2.71111610e-03\n",
      " 9.63835239e-01 7.18300700e-01 3.69885797e-03 1.02181127e-03\n",
      " 7.64754508e-03 1.59373958e-04 9.99275863e-01 9.95086014e-01\n",
      " 8.69613898e-04 9.85295791e-03 2.90691387e-04 1.32204514e-04\n",
      " 5.70938981e-04 2.80210486e-04 2.17534066e-03 5.42946982e-05\n",
      " 3.53407057e-04 9.92119670e-01 6.54929376e-04 3.42655549e-05\n",
      " 4.09170862e-05 4.71214764e-04 1.69270315e-05 9.97821569e-01\n",
      " 2.52187967e-01 9.77060437e-01 2.23425712e-04 1.32148023e-04\n",
      " 3.55542032e-03 6.33519012e-05 1.23535521e-01 9.61228013e-01\n",
      " 8.58954882e-05 3.38682695e-03 3.71045491e-04 1.25159917e-04\n",
      " 2.57027248e-04 3.08537134e-03 4.97882575e-05 4.43560020e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 83 [0/106 (0%)]\tTrain Loss: 0.130741\n",
      "Train Epoch: 83 [10/106 (9%)]\tTrain Loss: 0.000304\n",
      "Train Epoch: 83 [20/106 (19%)]\tTrain Loss: 0.000127\n",
      "Train Epoch: 83 [30/106 (28%)]\tTrain Loss: 0.000988\n",
      "Train Epoch: 83 [40/106 (38%)]\tTrain Loss: 0.000438\n",
      "Train Epoch: 83 [50/106 (47%)]\tTrain Loss: 0.010873\n",
      "Train Epoch: 83 [60/106 (57%)]\tTrain Loss: 0.000114\n",
      "Train Epoch: 83 [70/106 (66%)]\tTrain Loss: 0.000080\n",
      "Train Epoch: 83 [80/106 (75%)]\tTrain Loss: 0.260676\n",
      "Train Epoch: 83 [90/106 (85%)]\tTrain Loss: 0.043229\n",
      "Train Epoch: 83 [100/106 (94%)]\tTrain Loss: 0.000258\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 410/424 (97%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.81539916e-04 8.36405624e-03 8.40838766e-05 1.25190127e-04\n",
      " 9.61772166e-05 2.08242156e-04 3.05569672e-04 4.13007656e-05\n",
      " 1.81521373e-05 1.96898018e-05 4.41389857e-04 4.10646171e-05\n",
      " 5.57230727e-04 1.27273973e-03 1.66054768e-03 1.38687747e-05\n",
      " 2.62087341e-02 2.51818728e-02 5.39904831e-05 9.99847770e-01\n",
      " 9.99105752e-01 9.97060955e-01 9.99327064e-01 9.99376595e-01\n",
      " 7.11117864e-01 9.99527335e-01 9.99318957e-01 2.73456099e-04\n",
      " 3.24791799e-05 1.02901548e-04 1.82358758e-03 1.43359750e-04\n",
      " 9.37318691e-05 1.85263911e-04 3.45229600e-05 2.01958374e-05\n",
      " 2.24662908e-05 1.43317666e-04 5.61961642e-05 2.74840058e-05\n",
      " 1.19919248e-03 2.07267385e-05 9.78576063e-05 1.18144184e-04\n",
      " 6.41085280e-05 1.97200992e-04 1.54940702e-04 3.19250452e-04\n",
      " 5.24265543e-02 1.19277299e-03 9.95357893e-03 1.59524043e-05\n",
      " 6.70709996e-05 2.70300134e-05 7.39872514e-04 8.35251849e-05\n",
      " 1.95239531e-03 3.28867245e-05 3.59561891e-05 4.61783166e-05\n",
      " 9.99603689e-01 9.99279559e-01 9.99759734e-01 9.98989880e-01\n",
      " 9.99372184e-01 2.91256729e-04 4.48165229e-05 9.99734819e-01\n",
      " 9.96291876e-01 4.41271305e-01 3.38457932e-04 2.22641480e-04\n",
      " 3.03118210e-03 9.99191940e-01 9.99604046e-01 9.98164117e-01\n",
      " 9.99075055e-01 9.98465180e-01 9.99820888e-01 2.23839073e-03\n",
      " 9.99663591e-01 2.00660434e-04 9.99818623e-01 9.99589503e-01\n",
      " 8.86176527e-01 5.92005290e-02 1.14867894e-03 3.74473486e-04\n",
      " 9.96889889e-01 9.99601901e-01 9.99302983e-01 2.85343976e-05\n",
      " 4.97556102e-05 9.99646902e-01 4.31604385e-02 3.47940513e-05\n",
      " 4.10379907e-05 9.97457922e-01 9.31729301e-05 9.99822795e-01\n",
      " 9.98759389e-01 9.97909129e-01 2.64098821e-03 1.12671718e-04\n",
      " 4.75123012e-03 5.02622097e-05 6.87767208e-01 9.99587953e-01\n",
      " 9.92230237e-01 9.25587714e-01 9.97524559e-01 6.13487326e-03\n",
      " 9.31912363e-01 9.98805285e-01 7.48340899e-05 1.67693908e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 84 [0/106 (0%)]\tTrain Loss: 0.000616\n",
      "Train Epoch: 84 [10/106 (9%)]\tTrain Loss: 0.009737\n",
      "Train Epoch: 84 [20/106 (19%)]\tTrain Loss: 0.000192\n",
      "Train Epoch: 84 [30/106 (28%)]\tTrain Loss: 0.000018\n",
      "Train Epoch: 84 [40/106 (38%)]\tTrain Loss: 0.001106\n",
      "Train Epoch: 84 [50/106 (47%)]\tTrain Loss: 0.000331\n",
      "Train Epoch: 84 [60/106 (57%)]\tTrain Loss: 0.000939\n",
      "Train Epoch: 84 [70/106 (66%)]\tTrain Loss: 0.005074\n",
      "Train Epoch: 84 [80/106 (75%)]\tTrain Loss: 0.000974\n",
      "Train Epoch: 84 [90/106 (85%)]\tTrain Loss: 0.000236\n",
      "Train Epoch: 84 [100/106 (94%)]\tTrain Loss: 0.001055\n",
      "\n",
      "Train set: Average loss: 0.0046, Accuracy: 402/424 (95%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [7.46207443e-05 3.57403071e-03 6.39635546e-05 7.86891615e-05\n",
      " 4.27732411e-05 9.96693634e-05 1.29770589e-04 4.74433400e-05\n",
      " 1.14854693e-05 1.85505432e-05 8.26212345e-05 2.53957805e-05\n",
      " 3.99313358e-05 6.19931234e-05 3.04101413e-04 7.71300802e-06\n",
      " 6.71777088e-05 2.59273394e-04 6.83179678e-05 9.99862432e-01\n",
      " 9.98400748e-01 4.10698622e-01 9.98494864e-01 9.99575675e-01\n",
      " 1.18600539e-04 9.97767329e-01 9.99856472e-01 5.88301045e-05\n",
      " 1.31895495e-05 1.84319542e-05 1.51888409e-04 7.72319327e-05\n",
      " 1.23130361e-04 5.16138025e-05 1.82324620e-05 9.37586719e-06\n",
      " 1.80303323e-05 8.62291927e-05 2.95541049e-05 1.25422621e-05\n",
      " 4.18301606e-05 1.17581940e-05 1.62958459e-04 4.09160748e-05\n",
      " 4.09343374e-05 8.37838888e-05 2.24421921e-04 5.70978445e-04\n",
      " 1.86241549e-02 5.08808647e-04 2.10421183e-03 9.65144500e-06\n",
      " 2.85526075e-05 1.52808079e-05 5.24706091e-04 4.82789292e-05\n",
      " 1.35670387e-04 1.34549855e-05 3.15997604e-05 3.46912748e-05\n",
      " 9.07443643e-01 8.03916872e-01 9.98327911e-01 9.14346576e-01\n",
      " 9.96901035e-01 7.25492864e-05 4.91297506e-05 9.99570310e-01\n",
      " 9.94225204e-01 4.94670063e-01 1.10419387e-04 1.17265692e-04\n",
      " 4.40007914e-03 9.97915447e-01 9.96923506e-01 9.74624395e-01\n",
      " 9.98694599e-01 9.98308539e-01 7.35942960e-01 6.84058876e-04\n",
      " 9.94584620e-01 4.32753121e-04 9.99874353e-01 9.96269584e-01\n",
      " 8.40807054e-03 6.76333206e-03 6.73286151e-04 2.28698045e-04\n",
      " 9.82555747e-01 9.90848660e-01 9.99175131e-01 2.54813676e-05\n",
      " 4.87407888e-05 9.99070108e-01 9.45120060e-04 3.75195486e-05\n",
      " 2.42831193e-05 6.81195874e-03 7.20822864e-05 9.99626517e-01\n",
      " 7.88317978e-01 3.06284547e-01 1.21202538e-04 7.51360567e-05\n",
      " 4.19526303e-04 6.01953361e-05 4.91407424e-01 9.93055761e-01\n",
      " 1.58785726e-04 4.15027002e-03 1.37358205e-02 1.76133079e-04\n",
      " 6.76116705e-01 9.07877386e-01 3.94061171e-05 1.26929212e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      "Train Epoch: 85 [0/106 (0%)]\tTrain Loss: 0.000086\n",
      "Train Epoch: 85 [10/106 (9%)]\tTrain Loss: 0.004341\n",
      "Train Epoch: 85 [20/106 (19%)]\tTrain Loss: 0.000606\n",
      "Train Epoch: 85 [30/106 (28%)]\tTrain Loss: 0.009723\n",
      "Train Epoch: 85 [40/106 (38%)]\tTrain Loss: 0.000044\n",
      "Train Epoch: 85 [50/106 (47%)]\tTrain Loss: 0.000111\n",
      "Train Epoch: 85 [60/106 (57%)]\tTrain Loss: 0.000097\n",
      "Train Epoch: 85 [70/106 (66%)]\tTrain Loss: 0.000272\n",
      "Train Epoch: 85 [80/106 (75%)]\tTrain Loss: 0.000263\n",
      "Train Epoch: 85 [90/106 (85%)]\tTrain Loss: 0.001553\n",
      "Train Epoch: 85 [100/106 (94%)]\tTrain Loss: 0.012888\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 393/424 (93%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.34936665e-04 1.52996127e-02 2.12639439e-04 1.61743999e-04\n",
      " 1.03322178e-04 1.91267638e-04 9.04934655e-04 1.02940219e-04\n",
      " 1.98907765e-05 3.88442168e-05 1.34238260e-04 3.76309144e-05\n",
      " 1.11219109e-04 9.99379481e-05 5.86488924e-04 1.47333885e-05\n",
      " 5.02253206e-05 3.09970113e-03 6.32855954e-05 9.99794185e-01\n",
      " 9.88574445e-01 5.04591456e-03 9.88589823e-01 9.99623179e-01\n",
      " 7.27045626e-05 9.98169065e-01 9.99811471e-01 1.20649180e-04\n",
      " 2.64466726e-05 3.24673492e-05 1.73772307e-04 1.42212273e-04\n",
      " 2.48428260e-04 5.75920858e-05 2.96958151e-05 2.02488900e-05\n",
      " 3.88139269e-05 2.30549602e-04 5.79478583e-05 2.77245617e-05\n",
      " 1.24679878e-04 1.96763976e-05 6.25974906e-04 6.74134208e-05\n",
      " 4.44956422e-05 1.24969840e-04 4.98951820e-04 1.20049075e-03\n",
      " 3.58108580e-02 6.37893099e-04 4.57661366e-03 2.25063468e-05\n",
      " 2.85256665e-05 2.67792875e-05 5.50589059e-04 6.01701940e-05\n",
      " 1.51277156e-04 1.82621498e-05 4.27246887e-05 4.86179561e-05\n",
      " 9.48365211e-01 5.41389465e-01 9.96223927e-01 2.85676539e-01\n",
      " 9.81299698e-01 7.49593673e-05 1.12781563e-04 9.99397159e-01\n",
      " 9.96367455e-01 2.70452816e-03 4.13393922e-04 4.27009159e-04\n",
      " 4.58702631e-03 9.98647749e-01 9.95560288e-01 6.11068169e-03\n",
      " 9.98941481e-01 9.98977661e-01 4.07131715e-03 1.96427736e-03\n",
      " 9.64758158e-01 4.90947044e-04 9.99828696e-01 9.94617522e-01\n",
      " 4.27123485e-03 6.70818007e-03 1.00748602e-03 4.87107638e-04\n",
      " 9.32874203e-01 2.39678815e-01 9.97085631e-01 5.76128514e-05\n",
      " 1.07490705e-04 9.96555448e-01 2.31950311e-04 6.50455331e-05\n",
      " 3.47155656e-05 3.18017701e-04 6.04344932e-05 9.99606073e-01\n",
      " 9.75303650e-01 7.72202164e-02 1.67721329e-04 7.60474868e-05\n",
      " 4.51332977e-04 7.64552678e-05 1.62841901e-02 9.96350408e-01\n",
      " 7.19581221e-05 1.03608530e-03 4.66653742e-02 2.60351429e-04\n",
      " 8.96771788e-04 3.47301201e-03 9.66431471e-05 2.68519972e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 86 [0/106 (0%)]\tTrain Loss: 0.009468\n",
      "Train Epoch: 86 [10/106 (9%)]\tTrain Loss: 0.000507\n",
      "Train Epoch: 86 [20/106 (19%)]\tTrain Loss: 0.000132\n",
      "Train Epoch: 86 [30/106 (28%)]\tTrain Loss: 0.000028\n",
      "Train Epoch: 86 [40/106 (38%)]\tTrain Loss: 0.001041\n",
      "Train Epoch: 86 [50/106 (47%)]\tTrain Loss: 0.000265\n",
      "Train Epoch: 86 [60/106 (57%)]\tTrain Loss: 0.001745\n",
      "Train Epoch: 86 [70/106 (66%)]\tTrain Loss: 0.000039\n",
      "Train Epoch: 86 [80/106 (75%)]\tTrain Loss: 0.000134\n",
      "Train Epoch: 86 [90/106 (85%)]\tTrain Loss: 0.000126\n",
      "Train Epoch: 86 [100/106 (94%)]\tTrain Loss: 0.000321\n",
      "\n",
      "Train set: Average loss: 0.0046, Accuracy: 406/424 (96%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.86648846e-04 4.37759131e-01 4.48928593e-04 1.33372931e-04\n",
      " 7.43347991e-05 2.13540916e-04 2.52730455e-02 9.91871566e-05\n",
      " 1.25428842e-05 2.84526341e-05 2.14447282e-04 2.93239500e-05\n",
      " 1.60767479e-04 2.81573040e-04 2.36983201e-03 1.17758800e-05\n",
      " 9.50726026e-05 2.88512744e-02 1.12643349e-04 9.99889731e-01\n",
      " 9.97876406e-01 9.08303916e-01 9.99248087e-01 9.99785483e-01\n",
      " 1.83536598e-04 9.99645710e-01 9.99914646e-01 5.92836237e-04\n",
      " 5.48557837e-05 5.09283309e-05 1.63250905e-03 6.70628448e-04\n",
      " 6.18691265e-04 5.26299336e-05 2.22247891e-05 1.48005620e-05\n",
      " 2.75261846e-05 4.89116646e-04 8.24518575e-05 4.38706265e-05\n",
      " 7.87938421e-04 2.15246091e-05 1.10845501e-03 1.72313725e-04\n",
      " 7.10112727e-05 1.36037896e-04 6.60307356e-04 2.98945722e-03\n",
      " 3.89289916e-01 2.13205675e-03 3.98804583e-02 1.42724848e-05\n",
      " 2.89859745e-05 2.01968596e-05 1.59090618e-03 5.50598052e-05\n",
      " 4.10622219e-04 1.36423996e-05 3.43438041e-05 5.30767975e-05\n",
      " 9.95591581e-01 9.82269824e-01 9.99266088e-01 9.83090937e-01\n",
      " 9.99559462e-01 1.26128085e-04 1.82727235e-04 9.99939680e-01\n",
      " 9.99273360e-01 6.55021667e-01 1.49157259e-03 8.77563551e-04\n",
      " 2.33235642e-01 9.99567211e-01 9.99366343e-01 9.96073723e-01\n",
      " 9.99636054e-01 9.99458611e-01 9.99311566e-01 1.59009583e-02\n",
      " 9.99088049e-01 7.77513394e-03 9.99947429e-01 9.99340117e-01\n",
      " 1.36778161e-01 1.70223713e-01 2.35055201e-02 4.94102994e-03\n",
      " 9.98019814e-01 9.98991191e-01 9.99475300e-01 8.18603658e-05\n",
      " 2.73344049e-04 9.99385595e-01 5.67760086e-04 9.54173665e-05\n",
      " 3.37374840e-05 2.39952296e-01 8.16986285e-05 9.99878168e-01\n",
      " 9.96843576e-01 9.76891696e-01 2.31660786e-04 1.11787092e-04\n",
      " 6.73077255e-03 1.28074476e-04 8.95916224e-01 9.98894393e-01\n",
      " 1.93423286e-04 2.33738124e-02 3.89614135e-01 5.64457907e-04\n",
      " 5.21805603e-03 9.79259968e-01 1.41966768e-04 1.73587049e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 87 [0/106 (0%)]\tTrain Loss: 0.000593\n",
      "Train Epoch: 87 [10/106 (9%)]\tTrain Loss: 0.000288\n",
      "Train Epoch: 87 [20/106 (19%)]\tTrain Loss: 0.000141\n",
      "Train Epoch: 87 [30/106 (28%)]\tTrain Loss: 0.000236\n",
      "Train Epoch: 87 [40/106 (38%)]\tTrain Loss: 0.000089\n",
      "Train Epoch: 87 [50/106 (47%)]\tTrain Loss: 0.000024\n",
      "Train Epoch: 87 [60/106 (57%)]\tTrain Loss: 0.000291\n",
      "Train Epoch: 87 [70/106 (66%)]\tTrain Loss: 0.000864\n",
      "Train Epoch: 87 [80/106 (75%)]\tTrain Loss: 0.000018\n",
      "Train Epoch: 87 [90/106 (85%)]\tTrain Loss: 0.000434\n",
      "Train Epoch: 87 [100/106 (94%)]\tTrain Loss: 0.000326\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 410/424 (97%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [4.71421226e-05 9.67210829e-02 1.11737660e-04 5.25973228e-05\n",
      " 2.99228941e-05 6.42755185e-05 1.55699940e-03 3.97324438e-05\n",
      " 6.43935664e-06 1.68858860e-05 7.21671677e-05 1.56843580e-05\n",
      " 7.41070471e-05 9.23872940e-05 5.41653950e-04 6.28617227e-06\n",
      " 3.46832348e-05 2.09271885e-03 3.61281127e-05 9.99968648e-01\n",
      " 9.98255193e-01 8.75851959e-02 9.99149561e-01 9.99853373e-01\n",
      " 4.78288348e-05 9.99817431e-01 9.99958158e-01 8.70505901e-05\n",
      " 2.30652531e-05 2.65067138e-05 3.71983333e-04 1.75310241e-04\n",
      " 1.28554486e-04 2.28455938e-05 1.02475506e-05 7.03801243e-06\n",
      " 1.36129893e-05 1.42979945e-04 3.41060622e-05 2.62746235e-05\n",
      " 4.20820812e-04 1.16372021e-05 2.14907399e-04 5.08067642e-05\n",
      " 3.12324592e-05 6.11048017e-05 2.30265970e-04 9.33173753e-04\n",
      " 6.84403926e-02 5.09842008e-04 4.98124398e-03 8.38475535e-06\n",
      " 1.45555778e-05 1.10463679e-05 2.74526101e-04 2.34097533e-05\n",
      " 9.64510109e-05 7.37092614e-06 1.86491325e-05 2.64652917e-05\n",
      " 9.95050132e-01 9.67056513e-01 9.99150276e-01 8.45634520e-01\n",
      " 9.99630332e-01 4.54293986e-05 5.94153862e-05 9.99947786e-01\n",
      " 9.98613238e-01 6.99823275e-02 3.92971968e-04 2.44444556e-04\n",
      " 4.64670062e-02 9.99634504e-01 9.99480188e-01 9.62838233e-01\n",
      " 9.99783933e-01 9.99468386e-01 9.94014323e-01 1.98209519e-03\n",
      " 9.99349296e-01 6.48442132e-04 9.99969840e-01 9.99142766e-01\n",
      " 1.01425350e-02 3.71564850e-02 1.73275243e-03 4.70015890e-04\n",
      " 9.95040596e-01 9.92337883e-01 9.99749839e-01 3.32802083e-05\n",
      " 1.01254613e-04 9.99447525e-01 2.65975279e-04 3.69452828e-05\n",
      " 1.73880471e-05 1.19904261e-02 2.68685671e-05 9.99943733e-01\n",
      " 9.92416382e-01 7.04075098e-01 5.01801296e-05 4.53679168e-05\n",
      " 4.78444947e-03 5.45474359e-05 1.18882991e-01 9.98040736e-01\n",
      " 5.60931439e-05 4.59609693e-03 1.36962175e-01 1.12346417e-04\n",
      " 1.74907735e-03 5.62973917e-01 4.45976330e-05 2.73044512e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 88 [0/106 (0%)]\tTrain Loss: 0.000222\n",
      "Train Epoch: 88 [10/106 (9%)]\tTrain Loss: 0.000018\n",
      "Train Epoch: 88 [20/106 (19%)]\tTrain Loss: 0.000127\n",
      "Train Epoch: 88 [30/106 (28%)]\tTrain Loss: 0.000499\n",
      "Train Epoch: 88 [40/106 (38%)]\tTrain Loss: 0.000070\n",
      "Train Epoch: 88 [50/106 (47%)]\tTrain Loss: 0.000135\n",
      "Train Epoch: 88 [60/106 (57%)]\tTrain Loss: 0.000073\n",
      "Train Epoch: 88 [70/106 (66%)]\tTrain Loss: 0.015711\n",
      "Train Epoch: 88 [80/106 (75%)]\tTrain Loss: 0.000025\n",
      "Train Epoch: 88 [90/106 (85%)]\tTrain Loss: 0.001033\n",
      "Train Epoch: 88 [100/106 (94%)]\tTrain Loss: 0.000036\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 416/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.85862550e-05 2.33815219e-02 7.34776695e-05 4.05352330e-05\n",
      " 2.05727811e-05 5.10322207e-05 1.74326589e-03 2.73330352e-05\n",
      " 3.42264821e-06 1.00543421e-05 5.25963733e-05 9.44511430e-06\n",
      " 3.46432716e-05 5.66473718e-05 3.50728718e-04 3.40241195e-06\n",
      " 1.53051933e-05 3.81142925e-03 1.81137129e-05 9.99865890e-01\n",
      " 9.95979190e-01 2.38862596e-02 9.99449790e-01 9.99839902e-01\n",
      " 3.08027338e-05 9.99757349e-01 9.99913216e-01 5.54501530e-05\n",
      " 1.17577792e-05 1.36193257e-05 1.50173961e-04 1.48094405e-04\n",
      " 1.91564963e-04 1.32542600e-05 6.21214303e-06 4.14009037e-06\n",
      " 7.24566326e-06 1.25619408e-04 2.13482290e-05 7.01714180e-06\n",
      " 5.61173329e-05 5.04451691e-06 1.78725895e-04 3.46140805e-05\n",
      " 1.95640223e-05 4.53456450e-05 1.84580815e-04 9.34959098e-04\n",
      " 4.09572162e-02 3.58870864e-04 4.10953723e-03 4.43304316e-06\n",
      " 8.04490173e-06 6.75199090e-06 1.83137832e-04 1.62290835e-05\n",
      " 6.61523081e-05 4.53986604e-06 1.16907395e-05 1.44939368e-05\n",
      " 9.89722073e-01 8.58538330e-01 9.98662949e-01 2.73434669e-01\n",
      " 9.99310493e-01 3.02998087e-05 4.64008881e-05 9.99914050e-01\n",
      " 9.98979270e-01 6.75986782e-02 5.44671144e-04 2.20935341e-04\n",
      " 7.49966968e-03 9.99556601e-01 9.99164581e-01 9.67607260e-01\n",
      " 9.99613583e-01 9.98680770e-01 9.93366420e-01 2.48420401e-03\n",
      " 9.99432623e-01 4.95518907e-04 9.99940634e-01 9.99367058e-01\n",
      " 8.96454602e-03 2.64907200e-02 1.82549807e-03 4.21720993e-04\n",
      " 9.91903603e-01 9.87116992e-01 9.99632239e-01 2.58047039e-05\n",
      " 1.07964581e-04 9.99285519e-01 1.85033146e-04 2.20894271e-05\n",
      " 9.66100379e-06 1.22043835e-02 1.57028408e-05 9.99851227e-01\n",
      " 9.95608628e-01 9.18833375e-01 4.30428954e-05 2.51131823e-05\n",
      " 3.01329140e-03 3.79290177e-05 5.67104757e-01 9.92587626e-01\n",
      " 4.27791128e-05 1.42319209e-03 1.98335573e-03 6.18812628e-05\n",
      " 4.02341364e-04 6.98771536e-01 2.52748814e-05 2.04346477e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 89 [0/106 (0%)]\tTrain Loss: 0.000421\n",
      "Train Epoch: 89 [10/106 (9%)]\tTrain Loss: 0.000043\n",
      "Train Epoch: 89 [20/106 (19%)]\tTrain Loss: 0.193111\n",
      "Train Epoch: 89 [30/106 (28%)]\tTrain Loss: 0.010342\n",
      "Train Epoch: 89 [40/106 (38%)]\tTrain Loss: 0.000069\n",
      "Train Epoch: 89 [50/106 (47%)]\tTrain Loss: 0.000247\n",
      "Train Epoch: 89 [60/106 (57%)]\tTrain Loss: 0.195662\n",
      "Train Epoch: 89 [70/106 (66%)]\tTrain Loss: 0.000021\n",
      "Train Epoch: 89 [80/106 (75%)]\tTrain Loss: 0.000310\n",
      "Train Epoch: 89 [90/106 (85%)]\tTrain Loss: 0.000024\n",
      "Train Epoch: 89 [100/106 (94%)]\tTrain Loss: 0.254694\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 416/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [5.75802260e-05 1.82462752e-01 9.71027985e-05 7.18617011e-05\n",
      " 3.34889301e-05 7.26893195e-05 2.29800702e-03 4.45640471e-05\n",
      " 5.06744345e-06 1.48677873e-05 7.60213079e-05 1.45754284e-05\n",
      " 6.98525764e-05 5.64557595e-05 5.45064104e-04 4.83019085e-06\n",
      " 2.88215833e-05 5.28972270e-03 3.16942205e-05 9.99930739e-01\n",
      " 9.98264730e-01 4.20133859e-01 9.98778760e-01 9.99795020e-01\n",
      " 6.20229330e-05 9.99591053e-01 9.99902248e-01 1.14057977e-04\n",
      " 1.63838922e-05 1.89171187e-05 4.10663750e-04 1.40479577e-04\n",
      " 1.63592078e-04 2.11862589e-05 9.22114759e-06 6.00508656e-06\n",
      " 1.14804552e-05 1.85130048e-04 3.00610391e-05 1.60083946e-05\n",
      " 4.58724622e-04 7.12692508e-06 2.88224517e-04 4.92772233e-05\n",
      " 2.69873108e-05 6.14353994e-05 2.54946615e-04 1.18242670e-03\n",
      " 2.42577747e-01 6.68914232e-04 9.75201931e-03 6.37124140e-06\n",
      " 1.08193644e-05 8.75514161e-06 3.54125252e-04 2.40870304e-05\n",
      " 1.22549958e-04 5.91040271e-06 1.78896971e-05 1.93746364e-05\n",
      " 9.94706690e-01 9.75018203e-01 9.98867631e-01 9.25771713e-01\n",
      " 9.99313593e-01 4.38658608e-05 6.50583097e-05 9.99901414e-01\n",
      " 9.98581171e-01 5.77270567e-01 6.43940875e-04 3.47139721e-04\n",
      " 1.55240208e-01 9.99468148e-01 9.99201834e-01 9.79065418e-01\n",
      " 9.99566495e-01 9.99334991e-01 9.83470261e-01 3.14453780e-03\n",
      " 9.99106586e-01 1.35916192e-03 9.99927044e-01 9.98971581e-01\n",
      " 1.73284318e-02 5.08794859e-02 2.95792520e-03 4.89023456e-04\n",
      " 9.95838284e-01 9.98018503e-01 9.99542356e-01 3.51292583e-05\n",
      " 1.14118258e-04 9.99407530e-01 4.59837989e-04 3.62117244e-05\n",
      " 1.37320076e-05 3.86025608e-02 3.07262999e-05 9.99894500e-01\n",
      " 9.95861471e-01 9.28745151e-01 8.30697973e-05 4.45358783e-05\n",
      " 1.49065871e-02 5.18013294e-05 6.15394831e-01 9.98386979e-01\n",
      " 4.90122038e-05 5.24427462e-03 4.20530021e-01 1.54665991e-04\n",
      " 2.70820362e-03 8.63952935e-01 4.65995436e-05 3.26616951e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 90 [0/106 (0%)]\tTrain Loss: 0.010826\n",
      "Train Epoch: 90 [10/106 (9%)]\tTrain Loss: 0.001505\n",
      "Train Epoch: 90 [20/106 (19%)]\tTrain Loss: 0.000499\n",
      "Train Epoch: 90 [30/106 (28%)]\tTrain Loss: 0.000073\n",
      "Train Epoch: 90 [40/106 (38%)]\tTrain Loss: 0.001460\n",
      "Train Epoch: 90 [50/106 (47%)]\tTrain Loss: 0.000042\n",
      "Train Epoch: 90 [60/106 (57%)]\tTrain Loss: 0.000232\n",
      "Train Epoch: 90 [70/106 (66%)]\tTrain Loss: 0.000266\n",
      "Train Epoch: 90 [80/106 (75%)]\tTrain Loss: 0.000298\n",
      "Train Epoch: 90 [90/106 (85%)]\tTrain Loss: 0.000030\n",
      "Train Epoch: 90 [100/106 (94%)]\tTrain Loss: 0.000102\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 411/424 (97%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.01387375e-05 1.34074658e-01 3.80734709e-05 3.34145370e-05\n",
      " 1.39130598e-05 3.56261698e-05 5.15335181e-04 2.09328100e-05\n",
      " 1.96844667e-06 6.66162850e-06 2.82230103e-05 5.97164308e-06\n",
      " 2.81846860e-05 3.07834671e-05 4.29927313e-04 2.06784284e-06\n",
      " 1.11835825e-05 1.26566831e-03 1.16519823e-05 9.99804914e-01\n",
      " 9.98354077e-01 4.84667011e-02 9.98910904e-01 9.99719679e-01\n",
      " 2.04245644e-05 9.99587119e-01 9.99831915e-01 1.62771175e-05\n",
      " 4.61548507e-06 7.08133666e-06 1.02413469e-04 3.41638115e-05\n",
      " 4.14535425e-05 1.01705118e-05 3.71278679e-06 2.63882339e-06\n",
      " 5.06607194e-06 6.65111511e-05 1.27494086e-05 8.61678473e-06\n",
      " 2.49956443e-04 3.43965712e-06 8.54262908e-05 1.70881885e-05\n",
      " 1.01978021e-05 2.99556123e-05 1.00347323e-04 6.43642677e-04\n",
      " 6.62144482e-01 2.66601885e-04 7.73664704e-03 2.47646449e-06\n",
      " 4.90562343e-06 3.59644673e-06 1.63497578e-04 8.94708137e-06\n",
      " 4.07231491e-05 2.32853654e-06 7.72991552e-06 8.46297735e-06\n",
      " 9.94225562e-01 9.72305655e-01 9.99293447e-01 9.82862294e-01\n",
      " 9.98553336e-01 2.83173413e-05 3.23905260e-05 9.99738514e-01\n",
      " 9.98813987e-01 7.06940815e-02 2.90332420e-04 1.74886009e-04\n",
      " 1.32141858e-01 9.99406219e-01 9.98921275e-01 3.71439278e-01\n",
      " 9.99433935e-01 9.99488711e-01 7.31092365e-03 1.71020790e-03\n",
      " 9.99308944e-01 6.14304445e-04 9.99839902e-01 9.99074101e-01\n",
      " 6.91125728e-03 4.62882072e-02 5.38182794e-04 1.28227839e-04\n",
      " 9.96798754e-01 9.97822523e-01 9.99458373e-01 1.66689606e-05\n",
      " 3.75066702e-05 9.99392629e-01 2.21444046e-04 1.40017091e-05\n",
      " 5.59295040e-06 6.00154418e-03 1.35220434e-05 9.99711096e-01\n",
      " 9.97194886e-01 7.45446265e-01 2.55724372e-05 2.27537930e-05\n",
      " 2.68905307e-03 1.87004953e-05 2.81824917e-02 9.98318672e-01\n",
      " 2.56328676e-05 5.32104122e-03 4.16765809e-01 8.73201934e-05\n",
      " 5.03803976e-03 6.58011138e-01 2.16966528e-05 1.52746929e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 25 TN= 54 FN= 31 FP= 6\n",
      "TP+FP 31\n",
      "precision 0.8064516129032258\n",
      "recall 0.44642857142857145\n",
      "F1 0.5747126436781609\n",
      "acc 0.6810344827586207\n",
      "AUCp 0.6732142857142858\n",
      "AUC 0.7901785714285714\n",
      "\n",
      " The epoch is 90, average recall: 0.4464, average precision: 0.8065,average F1: 0.5747, average accuracy: 0.6810, average AUC: 0.7902\n",
      "Train Epoch: 91 [0/106 (0%)]\tTrain Loss: 0.000322\n",
      "Train Epoch: 91 [10/106 (9%)]\tTrain Loss: 0.010731\n",
      "Train Epoch: 91 [20/106 (19%)]\tTrain Loss: 0.009521\n",
      "Train Epoch: 91 [30/106 (28%)]\tTrain Loss: 0.000024\n",
      "Train Epoch: 91 [40/106 (38%)]\tTrain Loss: 0.000093\n",
      "Train Epoch: 91 [50/106 (47%)]\tTrain Loss: 0.000064\n",
      "Train Epoch: 91 [60/106 (57%)]\tTrain Loss: 0.000016\n",
      "Train Epoch: 91 [70/106 (66%)]\tTrain Loss: 0.000072\n",
      "Train Epoch: 91 [80/106 (75%)]\tTrain Loss: 0.000015\n",
      "Train Epoch: 91 [90/106 (85%)]\tTrain Loss: 0.000030\n",
      "Train Epoch: 91 [100/106 (94%)]\tTrain Loss: 0.000023\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 414/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.94828315e-05 8.61182809e-02 3.46288070e-05 3.25654910e-05\n",
      " 1.59961273e-05 3.55867123e-05 7.59582967e-04 1.91504187e-05\n",
      " 2.08747792e-06 6.02389946e-06 3.00119973e-05 6.55319900e-06\n",
      " 2.75744278e-05 2.16848821e-05 2.10825950e-04 2.35744983e-06\n",
      " 1.44424166e-05 1.09974272e-03 1.08055474e-05 9.99968886e-01\n",
      " 9.99221325e-01 3.34774368e-02 9.99515772e-01 9.99904275e-01\n",
      " 1.60144409e-05 9.99813735e-01 9.99956131e-01 2.75943130e-05\n",
      " 7.07266463e-06 1.07610613e-05 5.45819858e-05 3.59376136e-05\n",
      " 4.10311441e-05 1.16301007e-05 4.21101822e-06 2.99935414e-06\n",
      " 5.46381807e-06 6.31464063e-05 1.23651835e-05 7.14609041e-06\n",
      " 9.98424875e-05 3.61480193e-06 7.65706282e-05 1.48701984e-05\n",
      " 1.03248594e-05 3.16677542e-05 9.17630168e-05 3.48145288e-04\n",
      " 2.92773154e-02 3.19030485e-04 1.88920577e-03 2.95561131e-06\n",
      " 5.04162608e-06 4.14974738e-06 1.39709329e-04 1.08855820e-05\n",
      " 2.93871908e-05 2.78119433e-06 7.89797195e-06 8.15314797e-06\n",
      " 9.82810020e-01 8.27113152e-01 9.98920679e-01 5.23627818e-01\n",
      " 9.99337971e-01 1.70755302e-05 2.45830670e-05 9.99914408e-01\n",
      " 9.99189317e-01 8.79664198e-02 2.73252605e-04 2.21987953e-04\n",
      " 1.49365719e-02 9.99697685e-01 9.99383807e-01 7.93480992e-01\n",
      " 9.99695778e-01 9.99584734e-01 3.59572377e-03 6.79339864e-04\n",
      " 9.99464810e-01 3.64848704e-04 9.99956846e-01 9.99288678e-01\n",
      " 8.25465471e-03 1.53158875e-02 5.92824072e-04 1.35520284e-04\n",
      " 9.97481644e-01 9.95606601e-01 9.99801219e-01 1.74368524e-05\n",
      " 4.03498052e-05 9.99739826e-01 2.00843060e-04 1.84640248e-05\n",
      " 6.63069841e-06 2.88207326e-02 1.38157984e-05 9.99930739e-01\n",
      " 9.98075724e-01 1.50097489e-01 3.38715290e-05 1.81486048e-05\n",
      " 4.31201747e-03 1.75365421e-05 1.98388323e-01 9.98566687e-01\n",
      " 2.44281728e-05 5.28529799e-03 3.32325876e-01 6.51342925e-05\n",
      " 1.20661280e-03 4.08566594e-01 1.81641899e-05 1.15731134e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 92 [0/106 (0%)]\tTrain Loss: 0.000078\n",
      "Train Epoch: 92 [10/106 (9%)]\tTrain Loss: 0.000169\n",
      "Train Epoch: 92 [20/106 (19%)]\tTrain Loss: 0.203549\n",
      "Train Epoch: 92 [30/106 (28%)]\tTrain Loss: 0.000019\n",
      "Train Epoch: 92 [40/106 (38%)]\tTrain Loss: 0.000027\n",
      "Train Epoch: 92 [50/106 (47%)]\tTrain Loss: 0.000034\n",
      "Train Epoch: 92 [60/106 (57%)]\tTrain Loss: 0.000020\n",
      "Train Epoch: 92 [70/106 (66%)]\tTrain Loss: 0.000196\n",
      "Train Epoch: 92 [80/106 (75%)]\tTrain Loss: 0.000051\n",
      "Train Epoch: 92 [90/106 (85%)]\tTrain Loss: 0.000041\n",
      "Train Epoch: 92 [100/106 (94%)]\tTrain Loss: 0.000078\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 409/424 (96%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [6.54328687e-05 2.11936031e-02 6.15772078e-05 3.53522373e-05\n",
      " 2.20690454e-05 5.89080264e-05 2.08474672e-03 2.44526436e-05\n",
      " 3.04255946e-06 9.90894932e-06 4.17333831e-05 9.02372267e-06\n",
      " 2.04352418e-05 3.28358292e-05 1.59680771e-04 3.79951689e-06\n",
      " 1.41181736e-05 4.50237887e-03 1.96494893e-05 9.99976993e-01\n",
      " 9.99538660e-01 3.11930813e-02 9.99877095e-01 9.99945641e-01\n",
      " 3.06669062e-05 9.99944925e-01 9.99973774e-01 9.37682416e-05\n",
      " 8.90939555e-06 1.32296100e-05 1.74843997e-04 8.59194988e-05\n",
      " 1.48583786e-04 1.29687933e-05 6.26362407e-06 3.39936310e-06\n",
      " 5.99726854e-06 9.57517041e-05 1.56952665e-05 4.02024398e-06\n",
      " 1.52149923e-05 3.42452199e-06 1.35427021e-04 2.73442474e-05\n",
      " 1.95966604e-05 6.08007540e-05 2.32759150e-04 7.50290346e-04\n",
      " 2.05828294e-01 3.65267601e-03 1.39939906e-02 4.77009053e-06\n",
      " 8.14582745e-06 6.38694928e-06 2.22011455e-04 1.77645870e-05\n",
      " 7.27085135e-05 4.79553319e-06 1.15122593e-05 1.69251252e-05\n",
      " 9.93180156e-01 8.55724812e-01 9.99158263e-01 1.42042220e-01\n",
      " 9.99820650e-01 2.26394241e-05 4.13015914e-05 9.99970198e-01\n",
      " 9.99487638e-01 9.72786129e-01 1.52145210e-03 5.37705433e-04\n",
      " 1.11609669e-02 9.99900937e-01 9.99841928e-01 9.92711425e-01\n",
      " 9.99824107e-01 9.99344409e-01 9.99380112e-01 1.24760787e-03\n",
      " 9.99808490e-01 3.28595517e-03 9.99982715e-01 9.99749482e-01\n",
      " 6.44749729e-03 1.09886946e-02 3.81020922e-03 6.15985948e-04\n",
      " 9.98246670e-01 1.96252212e-01 9.99916553e-01 2.71615390e-05\n",
      " 1.20575343e-04 9.99901295e-01 2.89526390e-04 2.48094439e-05\n",
      " 9.82205438e-06 1.21897105e-02 2.30273836e-05 9.99917507e-01\n",
      " 9.95777488e-01 9.48117197e-01 5.11326180e-05 2.70230030e-05\n",
      " 1.84555382e-01 7.77970345e-05 9.99273241e-01 5.50605357e-03\n",
      " 1.79537019e-05 1.24872301e-03 2.52580037e-03 4.44792640e-05\n",
      " 2.88454024e-03 9.81052041e-01 2.57301708e-05 1.51109343e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 93 [0/106 (0%)]\tTrain Loss: 0.000167\n",
      "Train Epoch: 93 [10/106 (9%)]\tTrain Loss: 0.012294\n",
      "Train Epoch: 93 [20/106 (19%)]\tTrain Loss: 0.000052\n",
      "Train Epoch: 93 [30/106 (28%)]\tTrain Loss: 0.000876\n",
      "Train Epoch: 93 [40/106 (38%)]\tTrain Loss: 0.018527\n",
      "Train Epoch: 93 [50/106 (47%)]\tTrain Loss: 0.001323\n",
      "Train Epoch: 93 [60/106 (57%)]\tTrain Loss: 0.000270\n",
      "Train Epoch: 93 [70/106 (66%)]\tTrain Loss: 0.002138\n",
      "Train Epoch: 93 [80/106 (75%)]\tTrain Loss: 0.000107\n",
      "Train Epoch: 93 [90/106 (85%)]\tTrain Loss: 0.000445\n",
      "Train Epoch: 93 [100/106 (94%)]\tTrain Loss: 0.000026\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 417/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.31153899e-05 2.28751032e-03 1.13938077e-05 1.73815988e-05\n",
      " 9.76007323e-06 3.12554002e-05 3.20365434e-05 1.28126321e-05\n",
      " 1.84597650e-06 8.49346816e-06 1.88266458e-05 3.47157720e-06\n",
      " 1.08689947e-05 6.69612564e-06 1.69135419e-05 2.06134132e-06\n",
      " 4.78335596e-06 4.26493389e-05 1.05051095e-05 9.99956369e-01\n",
      " 9.98827994e-01 3.58373327e-05 9.99627471e-01 9.99906182e-01\n",
      " 1.06106072e-05 9.99841571e-01 9.99969721e-01 4.79950040e-06\n",
      " 4.91053379e-06 5.84072586e-06 1.18208200e-05 1.48903500e-05\n",
      " 4.31351000e-05 5.45306284e-06 2.68087456e-06 2.43100340e-06\n",
      " 2.99485237e-06 2.37368877e-05 5.02163130e-06 1.83377313e-06\n",
      " 3.79738708e-06 2.03318018e-06 4.07661246e-05 5.27233806e-06\n",
      " 9.80083314e-06 2.34518266e-05 7.32421977e-05 2.20742062e-04\n",
      " 2.45654598e-01 1.91127445e-04 2.03865534e-03 2.26279144e-06\n",
      " 3.24624261e-06 2.95769246e-06 3.41957566e-05 7.68351128e-06\n",
      " 1.57332415e-05 1.92742687e-06 4.84974316e-06 7.54017537e-06\n",
      " 9.35696423e-01 2.34148093e-02 9.64737952e-01 1.08861772e-03\n",
      " 9.99227047e-01 1.61178014e-05 2.03043110e-05 9.99954581e-01\n",
      " 9.99231696e-01 9.92484856e-03 2.06684213e-04 7.83148716e-05\n",
      " 1.87428179e-03 9.99873757e-01 9.98771012e-01 1.60794929e-02\n",
      " 9.99568522e-01 9.99294519e-01 2.42388298e-04 1.34958798e-04\n",
      " 9.99301076e-01 6.22341468e-05 9.99964356e-01 9.98376846e-01\n",
      " 5.25364478e-04 4.83506033e-03 5.37983724e-05 2.23844872e-05\n",
      " 9.88931000e-01 4.91408573e-05 9.99877691e-01 8.37924017e-06\n",
      " 1.77710936e-05 9.99864101e-01 3.28249043e-05 1.52531729e-05\n",
      " 5.41346662e-06 2.34761501e-05 1.05698055e-05 9.98235464e-01\n",
      " 4.10681009e-01 3.93654744e-04 1.02289323e-05 1.51902595e-05\n",
      " 1.46224920e-04 1.86589514e-05 2.18344368e-02 1.14553655e-02\n",
      " 5.68988571e-06 7.75177759e-05 4.81601979e-04 1.15277162e-05\n",
      " 1.38381496e-04 3.09411269e-02 1.00384859e-05 1.90943974e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 94 [0/106 (0%)]\tTrain Loss: 0.000022\n",
      "Train Epoch: 94 [10/106 (9%)]\tTrain Loss: 0.000020\n",
      "Train Epoch: 94 [20/106 (19%)]\tTrain Loss: 0.000023\n",
      "Train Epoch: 94 [30/106 (28%)]\tTrain Loss: 0.000031\n",
      "Train Epoch: 94 [40/106 (38%)]\tTrain Loss: 0.000023\n",
      "Train Epoch: 94 [50/106 (47%)]\tTrain Loss: 0.000067\n",
      "Train Epoch: 94 [60/106 (57%)]\tTrain Loss: 0.000300\n",
      "Train Epoch: 94 [70/106 (66%)]\tTrain Loss: 0.243955\n",
      "Train Epoch: 94 [80/106 (75%)]\tTrain Loss: 0.000040\n",
      "Train Epoch: 94 [90/106 (85%)]\tTrain Loss: 0.001237\n",
      "Train Epoch: 94 [100/106 (94%)]\tTrain Loss: 0.000039\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 414/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.09953211e-05 1.43714145e-01 3.74425945e-05 3.03538327e-05\n",
      " 1.90071914e-05 2.14244646e-05 2.03956806e-04 2.39878427e-05\n",
      " 4.06197387e-06 1.97403369e-05 1.05230298e-04 1.05509316e-05\n",
      " 2.79821223e-04 1.16857118e-05 2.66346033e-05 5.61311708e-06\n",
      " 1.00395278e-04 8.85123911e-04 1.34311513e-05 9.99796093e-01\n",
      " 9.99365032e-01 7.74940789e-01 9.97507572e-01 9.99678731e-01\n",
      " 6.30992727e-05 9.99626040e-01 9.99842286e-01 6.37421317e-06\n",
      " 5.82400571e-06 1.29256896e-05 1.71455813e-05 3.77516517e-05\n",
      " 1.00258847e-04 1.83969205e-05 7.26471808e-06 5.36926973e-06\n",
      " 7.96479890e-06 2.66116840e-04 1.41468126e-05 4.75333809e-06\n",
      " 1.68782863e-05 3.77444576e-06 1.04175568e-04 1.41811088e-05\n",
      " 1.25537745e-05 4.33114401e-05 3.83173407e-04 6.70645759e-03\n",
      " 9.94477987e-01 6.03139214e-03 7.82638788e-01 5.75592958e-06\n",
      " 1.25741190e-05 6.70098098e-06 6.31060757e-05 1.54911104e-05\n",
      " 2.30782316e-05 5.29646240e-06 1.07880851e-05 2.99011554e-05\n",
      " 9.90809798e-01 9.66790259e-01 9.98097837e-01 8.44884336e-01\n",
      " 9.99765337e-01 3.79797566e-05 5.10090140e-05 9.99959350e-01\n",
      " 9.99788940e-01 9.96343911e-01 1.03630200e-02 5.61207009e-04\n",
      " 1.90667585e-02 9.99801219e-01 9.98772562e-01 1.55474083e-03\n",
      " 9.99809086e-01 9.99866247e-01 1.38386968e-03 6.88553555e-04\n",
      " 9.99543309e-01 5.52693928e-05 9.99924660e-01 9.99777138e-01\n",
      " 7.22147990e-03 1.21448986e-01 1.63857316e-04 4.99763992e-05\n",
      " 9.36903834e-01 2.57347524e-01 9.99282539e-01 3.37107231e-05\n",
      " 4.69015504e-05 9.99878764e-01 1.83977420e-04 4.72125284e-05\n",
      " 9.13056101e-06 1.18608907e-04 2.11566621e-05 9.99735177e-01\n",
      " 9.87203777e-01 8.58132243e-01 2.46250638e-05 8.30477002e-05\n",
      " 6.88679159e-01 5.64861330e-05 5.10223210e-01 9.94976163e-01\n",
      " 4.33985915e-05 1.72378495e-02 6.93375111e-01 8.84121109e-05\n",
      " 8.37864412e-04 9.96361077e-01 1.93052547e-05 4.21649820e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 95 [0/106 (0%)]\tTrain Loss: 0.008312\n",
      "Train Epoch: 95 [10/106 (9%)]\tTrain Loss: 0.000031\n",
      "Train Epoch: 95 [20/106 (19%)]\tTrain Loss: 0.000108\n",
      "Train Epoch: 95 [30/106 (28%)]\tTrain Loss: 0.223221\n",
      "Train Epoch: 95 [40/106 (38%)]\tTrain Loss: 0.000057\n",
      "Train Epoch: 95 [50/106 (47%)]\tTrain Loss: 0.000767\n",
      "Train Epoch: 95 [60/106 (57%)]\tTrain Loss: 0.000021\n",
      "Train Epoch: 95 [70/106 (66%)]\tTrain Loss: 0.000029\n",
      "Train Epoch: 95 [80/106 (75%)]\tTrain Loss: 0.000167\n",
      "Train Epoch: 95 [90/106 (85%)]\tTrain Loss: 0.000031\n",
      "Train Epoch: 95 [100/106 (94%)]\tTrain Loss: 0.000348\n",
      "\n",
      "Train set: Average loss: 0.0003, Accuracy: 410/424 (97%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.04170067e-06 6.54894952e-03 8.42070733e-07 1.80339862e-06\n",
      " 1.34295033e-06 5.16552291e-06 7.49888204e-05 1.28025863e-06\n",
      " 3.07596594e-07 5.43911540e-07 3.97592066e-06 6.47609625e-07\n",
      " 2.08153017e-03 1.93931214e-06 4.44254874e-06 5.03279807e-07\n",
      " 2.02033371e-02 2.08549257e-02 9.38087851e-07 9.99424934e-01\n",
      " 9.98624682e-01 3.99540067e-02 9.94732022e-01 9.99312878e-01\n",
      " 1.34105096e-06 9.99556124e-01 9.99324560e-01 1.03248715e-06\n",
      " 5.06213723e-07 1.21187725e-06 2.67919077e-05 2.11331349e-06\n",
      " 3.16117894e-06 1.45399463e-06 5.98607528e-07 4.27275921e-07\n",
      " 4.85247313e-07 1.07918295e-05 8.90026740e-07 1.08183303e-06\n",
      " 4.10923763e-04 3.61731537e-07 6.75034244e-06 2.54676547e-06\n",
      " 1.01370256e-06 1.62313086e-06 8.78375977e-06 6.47265042e-05\n",
      " 9.66922224e-01 2.52017853e-05 5.18412255e-02 6.55608460e-07\n",
      " 1.27057899e-06 4.29656922e-07 5.65231448e-06 7.35864546e-07\n",
      " 2.55376017e-06 3.13897402e-07 5.78738593e-07 1.62730487e-06\n",
      " 9.75522101e-01 7.87573397e-01 9.98074889e-01 9.27920043e-01\n",
      " 9.99312639e-01 1.24353210e-05 3.07813639e-06 9.99332964e-01\n",
      " 9.98927176e-01 9.61961389e-01 2.50243902e-01 1.33570451e-02\n",
      " 9.89830792e-01 9.99653339e-01 9.99470174e-01 7.84645751e-02\n",
      " 9.98934686e-01 9.98799205e-01 7.16038840e-03 6.67351705e-05\n",
      " 9.99239087e-01 1.10929450e-05 9.99578059e-01 9.99495983e-01\n",
      " 5.76974300e-04 1.94863342e-02 3.61729326e-05 9.77295440e-06\n",
      " 9.94226396e-01 9.99273241e-01 9.99179423e-01 3.02681588e-06\n",
      " 2.84579073e-06 9.99584019e-01 4.63981210e-04 3.28701822e-06\n",
      " 7.07876495e-07 6.37554303e-02 8.67836206e-07 9.99404073e-01\n",
      " 9.98586535e-01 9.96161222e-01 1.81879113e-06 2.06873496e-03\n",
      " 9.98570800e-01 5.51181529e-06 9.95979786e-01 9.70010400e-01\n",
      " 1.16563269e-05 2.14546606e-01 4.43728529e-02 5.08197718e-06\n",
      " 2.79432703e-02 9.98977542e-01 2.25225745e-06 1.76794438e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 96 [0/106 (0%)]\tTrain Loss: 0.001387\n",
      "Train Epoch: 96 [10/106 (9%)]\tTrain Loss: 0.000051\n",
      "Train Epoch: 96 [20/106 (19%)]\tTrain Loss: 0.000534\n",
      "Train Epoch: 96 [30/106 (28%)]\tTrain Loss: 0.210481\n",
      "Train Epoch: 96 [40/106 (38%)]\tTrain Loss: 0.000165\n",
      "Train Epoch: 96 [50/106 (47%)]\tTrain Loss: 0.000100\n",
      "Train Epoch: 96 [60/106 (57%)]\tTrain Loss: 0.000275\n",
      "Train Epoch: 96 [70/106 (66%)]\tTrain Loss: 0.217715\n",
      "Train Epoch: 96 [80/106 (75%)]\tTrain Loss: 0.385077\n",
      "Train Epoch: 96 [90/106 (85%)]\tTrain Loss: 0.000065\n",
      "Train Epoch: 96 [100/106 (94%)]\tTrain Loss: 0.002048\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 405/424 (96%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.09118555e-05 9.00356463e-06 5.77350988e-07 1.03444727e-05\n",
      " 6.50018637e-06 1.13359538e-05 1.40560076e-06 5.86461465e-06\n",
      " 6.66981464e-07 1.78521225e-06 9.83555219e-06 2.52343057e-06\n",
      " 6.15297322e-05 7.95846609e-06 1.12300322e-05 3.89174920e-06\n",
      " 2.14329666e-05 6.92937510e-06 2.55932173e-06 9.99795616e-01\n",
      " 9.88134623e-01 6.17281912e-05 9.91726398e-01 9.99728024e-01\n",
      " 7.95974120e-06 9.56827283e-01 9.99663472e-01 2.19222375e-06\n",
      " 1.37746270e-06 1.66668997e-06 3.26349527e-06 6.13844713e-06\n",
      " 7.15645683e-06 5.92151082e-06 3.08731478e-06 1.11794554e-06\n",
      " 1.87661158e-06 7.78503181e-06 2.41122962e-06 4.13095222e-06\n",
      " 3.90467794e-05 1.91315348e-06 3.85270914e-06 4.32512707e-06\n",
      " 4.98867621e-06 6.05029118e-06 2.12451760e-05 1.50366803e-04\n",
      " 5.73594356e-03 1.07095029e-05 7.01421523e-05 2.40752092e-06\n",
      " 3.19441824e-06 2.74131071e-06 6.41981524e-06 1.22948677e-05\n",
      " 3.79312337e-06 1.39778342e-06 2.14284637e-06 2.30141700e-06\n",
      " 1.51800935e-03 3.31670999e-05 1.31694600e-01 2.99617299e-03\n",
      " 9.98902917e-01 1.75834630e-05 5.07813047e-06 9.99657750e-01\n",
      " 9.96596754e-01 9.98683035e-01 1.15267793e-03 4.75304405e-04\n",
      " 9.68956590e-01 9.99820292e-01 9.99595225e-01 3.35208839e-04\n",
      " 9.99629855e-01 9.99388933e-01 2.08976653e-05 2.79928645e-05\n",
      " 9.99622345e-01 1.13573142e-05 9.99845147e-01 9.94311213e-01\n",
      " 9.45770880e-05 8.88647410e-05 1.66534111e-04 1.62129800e-05\n",
      " 8.55230552e-04 1.81096184e-05 9.99498367e-01 6.31466446e-06\n",
      " 5.57067779e-06 9.99777853e-01 1.04109931e-03 4.65585617e-06\n",
      " 2.00358750e-06 2.33347339e-04 1.92791267e-05 9.99868035e-01\n",
      " 9.83399034e-01 5.27784787e-03 1.35028031e-05 6.37555349e-05\n",
      " 1.55764888e-03 6.31049897e-06 9.77473378e-01 9.89790380e-01\n",
      " 2.55239302e-05 8.82715103e-05 1.36709930e-02 1.66022201e-05\n",
      " 3.42573911e-01 9.96082366e-01 3.67763482e-06 1.68944698e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 97 [0/106 (0%)]\tTrain Loss: 0.000401\n",
      "Train Epoch: 97 [10/106 (9%)]\tTrain Loss: 0.000039\n",
      "Train Epoch: 97 [20/106 (19%)]\tTrain Loss: 0.000018\n",
      "Train Epoch: 97 [30/106 (28%)]\tTrain Loss: 0.008181\n",
      "Train Epoch: 97 [40/106 (38%)]\tTrain Loss: 0.000055\n",
      "Train Epoch: 97 [50/106 (47%)]\tTrain Loss: 0.000236\n",
      "Train Epoch: 97 [60/106 (57%)]\tTrain Loss: 0.000184\n",
      "Train Epoch: 97 [70/106 (66%)]\tTrain Loss: 0.000021\n",
      "Train Epoch: 97 [80/106 (75%)]\tTrain Loss: 0.000019\n",
      "Train Epoch: 97 [90/106 (85%)]\tTrain Loss: 0.008894\n",
      "Train Epoch: 97 [100/106 (94%)]\tTrain Loss: 0.000241\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 416/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.48672113e-05 1.23793856e-04 9.75341027e-06 1.82862022e-05\n",
      " 1.99120914e-05 1.05053705e-05 3.18208768e-05 1.54744848e-05\n",
      " 4.05037690e-06 3.40696570e-05 7.69431135e-05 2.10497565e-05\n",
      " 4.51326283e-04 9.49519163e-04 6.49008295e-03 3.67423490e-04\n",
      " 3.22115988e-01 1.78107526e-03 2.39785550e-05 9.99881387e-01\n",
      " 9.96840358e-01 1.28539935e-01 9.99640822e-01 9.98854876e-01\n",
      " 7.57472080e-05 9.99879599e-01 9.99807775e-01 4.82002215e-05\n",
      " 3.43781503e-05 5.30278267e-05 9.77252364e-01 1.74975023e-03\n",
      " 4.36309783e-05 1.17821648e-04 2.35965708e-05 1.56216483e-05\n",
      " 2.54188726e-05 2.39395871e-04 2.21786959e-05 4.08042797e-05\n",
      " 1.80585193e-04 1.88607901e-05 7.12982946e-05 2.62302347e-04\n",
      " 4.84418233e-05 2.43235881e-05 1.64281446e-04 3.34062008e-03\n",
      " 9.92944658e-01 1.38638119e-04 9.89482820e-01 1.66827340e-05\n",
      " 1.01318743e-04 2.18345194e-05 7.90530920e-01 7.02375983e-05\n",
      " 3.53496790e-01 1.77478905e-05 1.02124586e-05 6.03261287e-05\n",
      " 9.99832869e-01 1.54691145e-01 9.99852180e-01 9.99533415e-01\n",
      " 9.99826968e-01 1.72305796e-02 1.82954405e-04 9.99865651e-01\n",
      " 9.97760177e-01 9.99911904e-01 7.03219235e-01 5.58081903e-02\n",
      " 9.99870062e-01 9.99915481e-01 9.99904037e-01 9.99870896e-01\n",
      " 9.99938369e-01 9.99924779e-01 9.99947667e-01 9.89115894e-01\n",
      " 9.99919057e-01 9.99704421e-01 9.99943733e-01 9.99837518e-01\n",
      " 9.23011065e-01 8.80205989e-01 2.06248555e-02 6.32886833e-04\n",
      " 9.66999769e-01 3.68463434e-02 9.99898195e-01 4.00095851e-05\n",
      " 4.64469667e-05 9.99712288e-01 4.33078676e-01 2.90297525e-04\n",
      " 4.89568447e-05 6.77471459e-01 9.89843742e-04 9.99931335e-01\n",
      " 9.99542117e-01 9.99605715e-01 2.44071671e-05 9.98943150e-01\n",
      " 9.99843717e-01 2.20793597e-02 9.99899268e-01 9.99546945e-01\n",
      " 1.07170932e-03 2.58981134e-03 8.08646083e-01 4.07572195e-04\n",
      " 9.98845935e-01 9.94580686e-01 8.49479518e-04 6.71699271e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0.]\n",
      "Train Epoch: 98 [0/106 (0%)]\tTrain Loss: 0.203760\n",
      "Train Epoch: 98 [10/106 (9%)]\tTrain Loss: 0.012950\n",
      "Train Epoch: 98 [20/106 (19%)]\tTrain Loss: 0.000308\n",
      "Train Epoch: 98 [30/106 (28%)]\tTrain Loss: 0.000043\n",
      "Train Epoch: 98 [40/106 (38%)]\tTrain Loss: 0.000232\n",
      "Train Epoch: 98 [50/106 (47%)]\tTrain Loss: 0.001198\n",
      "Train Epoch: 98 [60/106 (57%)]\tTrain Loss: 0.024258\n",
      "Train Epoch: 98 [70/106 (66%)]\tTrain Loss: 0.000109\n",
      "Train Epoch: 98 [80/106 (75%)]\tTrain Loss: 0.001137\n",
      "Train Epoch: 98 [90/106 (85%)]\tTrain Loss: 0.000341\n",
      "Train Epoch: 98 [100/106 (94%)]\tTrain Loss: 0.000078\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 405/424 (96%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [5.28793817e-06 3.73842595e-05 2.43725708e-06 2.39979590e-06\n",
      " 2.14039960e-06 4.25715461e-06 5.47302807e-06 2.52429936e-06\n",
      " 1.38854170e-06 3.94683866e-06 4.39208088e-06 3.02977037e-06\n",
      " 4.20698780e-06 7.25703194e-06 6.82902755e-05 2.46279956e-06\n",
      " 1.17781719e-05 5.01360570e-04 3.91343247e-06 9.73716199e-01\n",
      " 2.81291286e-04 1.33368361e-03 9.97148454e-01 9.97344553e-01\n",
      " 5.59504178e-06 9.99771178e-01 9.99532104e-01 2.02838100e-06\n",
      " 3.53242558e-06 2.93970606e-06 2.80520053e-05 1.73490880e-05\n",
      " 5.43612578e-06 4.14106171e-06 2.09055383e-06 1.66941959e-06\n",
      " 2.57715442e-06 5.38090626e-06 1.63565699e-06 1.79197048e-06\n",
      " 1.02914964e-06 1.09090035e-06 5.99069472e-06 3.43956845e-06\n",
      " 2.13879594e-06 2.78892867e-06 1.52456678e-05 1.71483553e-04\n",
      " 7.42954799e-05 1.99784736e-05 2.72599409e-05 1.42681029e-06\n",
      " 8.79324762e-06 1.66198208e-06 8.73364210e-01 3.76788580e-06\n",
      " 4.77086614e-05 1.36687891e-06 4.77505228e-06 7.74943146e-06\n",
      " 8.97566795e-01 2.23303476e-04 5.64668775e-01 9.66420397e-02\n",
      " 9.92856145e-01 4.38012694e-06 8.75864953e-06 9.98503804e-01\n",
      " 8.52372289e-01 9.98047113e-01 1.47527062e-05 9.61566093e-06\n",
      " 2.65244220e-04 9.98009503e-01 9.71657395e-01 9.68354583e-01\n",
      " 9.95833099e-01 9.41259980e-01 5.31682432e-01 7.86726523e-05\n",
      " 2.22745705e-02 4.82467860e-02 9.99960423e-01 9.96287107e-01\n",
      " 5.20918147e-05 8.14296654e-05 2.15811282e-03 5.77305400e-05\n",
      " 1.47756306e-04 7.39262177e-05 9.99954939e-01 3.44603973e-06\n",
      " 3.60347781e-06 9.94169354e-01 8.17722412e-06 3.96452742e-06\n",
      " 1.84708779e-06 1.65281296e-02 2.35242351e-06 9.99422908e-01\n",
      " 7.30684638e-01 5.91478834e-04 2.49685900e-06 9.71978807e-06\n",
      " 8.69980431e-05 5.41043664e-06 1.06413162e-03 5.62167773e-03\n",
      " 1.51322793e-05 4.13984089e-04 1.88179562e-04 1.05459440e-05\n",
      " 3.99468176e-04 2.35322332e-05 2.00378163e-05 1.15420786e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 99 [0/106 (0%)]\tTrain Loss: 0.342511\n",
      "Train Epoch: 99 [10/106 (9%)]\tTrain Loss: 0.000095\n",
      "Train Epoch: 99 [20/106 (19%)]\tTrain Loss: 0.005160\n",
      "Train Epoch: 99 [30/106 (28%)]\tTrain Loss: 0.000038\n",
      "Train Epoch: 99 [40/106 (38%)]\tTrain Loss: 0.016532\n",
      "Train Epoch: 99 [50/106 (47%)]\tTrain Loss: 0.006549\n",
      "Train Epoch: 99 [60/106 (57%)]\tTrain Loss: 0.000067\n",
      "Train Epoch: 99 [70/106 (66%)]\tTrain Loss: 0.000564\n",
      "Train Epoch: 99 [80/106 (75%)]\tTrain Loss: 0.000120\n",
      "Train Epoch: 99 [90/106 (85%)]\tTrain Loss: 0.000560\n",
      "Train Epoch: 99 [100/106 (94%)]\tTrain Loss: 0.000519\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 401/424 (95%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [8.82355471e-06 1.87125159e-04 1.16033580e-05 6.03067065e-06\n",
      " 4.30941509e-06 1.95188459e-05 8.87186106e-05 1.25411616e-05\n",
      " 4.15392469e-06 4.61538821e-06 6.69418478e-06 4.03664126e-06\n",
      " 7.38599192e-06 3.01984419e-05 4.53750225e-04 6.01552983e-06\n",
      " 7.93824089e-04 9.85129118e-01 1.17337506e-04 9.99105394e-01\n",
      " 9.27686214e-01 9.89510953e-01 9.92778122e-01 9.86687839e-01\n",
      " 3.19976628e-01 9.99598801e-01 9.99382973e-01 7.31103355e-04\n",
      " 2.74232425e-05 9.04514218e-06 9.96957660e-01 1.47104729e-04\n",
      " 1.16619158e-04 1.06610796e-05 2.14759280e-06 1.61663479e-06\n",
      " 3.21347557e-06 1.59507897e-02 1.20006633e-04 1.01263041e-03\n",
      " 1.19720353e-02 1.86082725e-05 7.15788337e-05 1.02577105e-05\n",
      " 1.00303614e-05 7.54795246e-06 1.13444694e-04 1.45888038e-03\n",
      " 2.32799724e-03 7.77925743e-05 7.39563431e-04 1.38498410e-06\n",
      " 1.22561876e-04 2.14823194e-06 9.93844032e-01 2.99762382e-06\n",
      " 8.99050355e-01 1.26614225e-06 9.19283957e-06 1.56277732e-04\n",
      " 4.01186138e-01 1.54621771e-03 9.59271669e-01 9.50156629e-01\n",
      " 9.99168396e-01 1.36428120e-04 9.08696165e-05 9.98355210e-01\n",
      " 8.98061931e-01 9.80970144e-01 6.21668150e-05 2.69310058e-05\n",
      " 9.78851676e-01 9.95945156e-01 9.97947633e-01 9.99413729e-01\n",
      " 9.98315096e-01 9.93355870e-01 9.99852300e-01 4.41465247e-03\n",
      " 9.96995687e-01 9.27651703e-01 9.99681234e-01 9.99048173e-01\n",
      " 6.27548695e-01 1.25627790e-04 7.65241217e-04 5.87130271e-05\n",
      " 8.12759995e-01 7.58269876e-02 9.99194801e-01 1.00502584e-05\n",
      " 7.99543068e-06 2.29588941e-01 9.00681625e-05 2.22208237e-05\n",
      " 3.56757437e-06 9.37703967e-01 2.84861380e-05 9.98824418e-01\n",
      " 3.28367442e-01 9.60367918e-01 9.06897276e-06 1.52480730e-03\n",
      " 9.51817989e-01 6.14993478e-05 2.07667753e-01 1.20223791e-04\n",
      " 3.47697292e-03 1.97409841e-04 8.66900664e-05 3.19862629e-05\n",
      " 1.42074778e-05 1.18960270e-04 1.54905149e-03 9.95555103e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "Train Epoch: 100 [0/106 (0%)]\tTrain Loss: 0.218274\n",
      "Train Epoch: 100 [10/106 (9%)]\tTrain Loss: 0.002061\n",
      "Train Epoch: 100 [20/106 (19%)]\tTrain Loss: 0.067657\n",
      "Train Epoch: 100 [30/106 (28%)]\tTrain Loss: 0.000952\n",
      "Train Epoch: 100 [40/106 (38%)]\tTrain Loss: 0.001182\n",
      "Train Epoch: 100 [50/106 (47%)]\tTrain Loss: 0.012177\n",
      "Train Epoch: 100 [60/106 (57%)]\tTrain Loss: 0.004973\n",
      "Train Epoch: 100 [70/106 (66%)]\tTrain Loss: 0.206385\n",
      "Train Epoch: 100 [80/106 (75%)]\tTrain Loss: 0.000184\n",
      "Train Epoch: 100 [90/106 (85%)]\tTrain Loss: 0.000353\n",
      "Train Epoch: 100 [100/106 (94%)]\tTrain Loss: 0.000070\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 408/424 (96%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [9.47945082e-05 4.97805595e-04 6.87926695e-06 7.71097893e-06\n",
      " 1.78407527e-06 3.21439002e-04 1.03057282e-04 1.45894319e-05\n",
      " 4.20710508e-04 2.79313157e-04 4.75348361e-06 4.91539598e-04\n",
      " 1.13006890e-05 1.17206514e-06 3.20492836e-05 1.46793286e-06\n",
      " 9.09912575e-04 4.86265817e-05 2.80966953e-04 9.99623179e-01\n",
      " 9.98425841e-01 9.70118046e-01 9.98645842e-01 9.98235464e-01\n",
      " 8.37103871e-04 9.98609662e-01 9.99095917e-01 8.20392324e-06\n",
      " 1.00016196e-05 1.05002209e-05 8.33226964e-02 1.34104950e-04\n",
      " 2.06061872e-03 1.04235187e-05 1.26643931e-06 3.80569304e-07\n",
      " 1.02173203e-06 2.94424972e-04 4.51724009e-06 4.30783757e-06\n",
      " 4.40194017e-05 1.36196759e-05 5.30655161e-05 7.04187960e-06\n",
      " 1.96040255e-05 1.27496151e-05 1.87207785e-04 9.95571725e-03\n",
      " 9.82681453e-01 9.96246636e-01 9.93371010e-01 3.24758503e-07\n",
      " 3.93454850e-01 7.26915630e-07 9.99199092e-01 1.54376776e-05\n",
      " 9.92493451e-01 6.76008199e-07 4.10749926e-05 9.81516600e-01\n",
      " 9.63764369e-01 5.78361571e-01 9.98148799e-01 9.88854945e-01\n",
      " 9.99110043e-01 1.19082967e-03 1.18722254e-02 9.99123633e-01\n",
      " 7.89362013e-01 9.96779382e-01 6.25651423e-03 1.42524089e-03\n",
      " 9.98659372e-01 9.98452425e-01 9.96316254e-01 9.96888340e-01\n",
      " 9.99414325e-01 9.98770177e-01 9.99551356e-01 1.82186277e-03\n",
      " 9.98698473e-01 9.98371303e-01 9.99597013e-01 9.99229670e-01\n",
      " 9.83370781e-01 1.12875300e-02 9.94891763e-01 4.20103985e-04\n",
      " 9.71655250e-01 1.19188255e-04 9.99057233e-01 3.15527432e-04\n",
      " 5.62906716e-05 9.97981906e-01 6.29543734e-04 6.99585711e-04\n",
      " 6.00929161e-06 9.94462669e-01 2.90701864e-05 9.98684227e-01\n",
      " 9.84566391e-01 9.92257595e-01 5.46971478e-06 9.66652110e-02\n",
      " 9.87979770e-01 6.24816000e-01 9.99499917e-01 1.66269613e-03\n",
      " 2.62635494e-05 7.13049024e-02 2.10143371e-05 6.35572418e-04\n",
      " 3.77705961e-04 2.12117005e-03 1.10972296e-05 1.35632232e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 22 TN= 54 FN= 34 FP= 6\n",
      "TP+FP 28\n",
      "precision 0.7857142857142857\n",
      "recall 0.39285714285714285\n",
      "F1 0.5238095238095237\n",
      "acc 0.6551724137931034\n",
      "AUCp 0.6464285714285715\n",
      "AUC 0.7907738095238096\n",
      "\n",
      " The epoch is 100, average recall: 0.3929, average precision: 0.7857,average F1: 0.5238, average accuracy: 0.6552, average AUC: 0.7908\n",
      "Train Epoch: 101 [0/106 (0%)]\tTrain Loss: 0.000124\n",
      "Train Epoch: 101 [10/106 (9%)]\tTrain Loss: 0.004939\n",
      "Train Epoch: 101 [20/106 (19%)]\tTrain Loss: 0.028658\n",
      "Train Epoch: 101 [30/106 (28%)]\tTrain Loss: 0.000249\n",
      "Train Epoch: 101 [40/106 (38%)]\tTrain Loss: 0.001881\n",
      "Train Epoch: 101 [50/106 (47%)]\tTrain Loss: 0.003625\n",
      "Train Epoch: 101 [60/106 (57%)]\tTrain Loss: 0.241307\n",
      "Train Epoch: 101 [70/106 (66%)]\tTrain Loss: 0.000312\n",
      "Train Epoch: 101 [80/106 (75%)]\tTrain Loss: 0.010714\n",
      "Train Epoch: 101 [90/106 (85%)]\tTrain Loss: 0.000193\n",
      "Train Epoch: 101 [100/106 (94%)]\tTrain Loss: 0.000065\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 412/424 (97%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.13587221e-05 8.06227035e-05 2.53795451e-06 1.79881931e-06\n",
      " 2.96356529e-06 3.38062523e-06 2.70283635e-05 3.61921707e-06\n",
      " 1.23461564e-06 2.92904724e-06 1.31942534e-06 1.55760404e-06\n",
      " 1.03807145e-06 1.01005992e-06 9.32826060e-06 8.25436416e-07\n",
      " 7.64856486e-06 2.63447204e-04 5.13436044e-06 9.99994397e-01\n",
      " 7.62690615e-05 2.15611994e-01 9.98403728e-01 9.93490875e-01\n",
      " 8.79995059e-06 9.99967813e-01 9.99953270e-01 1.55070916e-06\n",
      " 1.76021547e-06 3.59862202e-06 5.14486919e-06 7.83508222e-06\n",
      " 1.81861142e-05 3.52224674e-06 6.79173979e-07 6.79810341e-07\n",
      " 8.62497870e-07 1.47093551e-05 3.61745401e-06 2.35542711e-06\n",
      " 2.59304102e-06 1.15027910e-06 1.71717948e-05 3.28587726e-06\n",
      " 2.96475832e-06 3.02552871e-06 1.44275773e-05 1.87962767e-04\n",
      " 8.05330274e-05 1.02482159e-06 3.05601716e-05 5.46359161e-07\n",
      " 1.15547971e-06 5.17859064e-07 2.49507593e-05 5.27562463e-07\n",
      " 2.49041659e-05 3.18467983e-07 1.42464421e-06 2.47585967e-06\n",
      " 1.39406446e-04 1.47502768e-04 9.98933733e-01 1.41909495e-01\n",
      " 9.99917388e-01 3.48340909e-05 1.30526587e-05 9.99999642e-01\n",
      " 1.85306894e-03 9.60599720e-01 6.04189609e-06 2.22195322e-05\n",
      " 1.64277442e-02 9.99897361e-01 1.00673050e-01 2.01128572e-02\n",
      " 9.99990225e-01 9.99985814e-01 4.38184448e-04 8.20146932e-04\n",
      " 8.27744126e-01 1.44689275e-05 9.99993563e-01 9.99986768e-01\n",
      " 9.99723732e-01 4.45864534e-05 9.99982238e-01 4.66432131e-04\n",
      " 6.33031595e-04 1.43046927e-04 9.98558819e-01 2.44882676e-06\n",
      " 3.36139442e-06 9.99858022e-01 4.17861065e-06 1.59753699e-06\n",
      " 5.54175585e-07 1.26785529e-03 2.51865549e-06 9.96645629e-01\n",
      " 9.99259293e-01 9.99928474e-01 1.24674341e-06 3.55652037e-06\n",
      " 2.90480966e-05 3.69959957e-06 9.46070161e-03 3.16337854e-01\n",
      " 9.41739290e-06 2.32465263e-05 5.52246911e-06 6.92811955e-06\n",
      " 5.02433832e-05 1.48160636e-01 4.49159006e-06 7.42720295e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 102 [0/106 (0%)]\tTrain Loss: 0.000021\n",
      "Train Epoch: 102 [10/106 (9%)]\tTrain Loss: 0.000144\n",
      "Train Epoch: 102 [20/106 (19%)]\tTrain Loss: 0.000028\n",
      "Train Epoch: 102 [30/106 (28%)]\tTrain Loss: 0.000906\n",
      "Train Epoch: 102 [40/106 (38%)]\tTrain Loss: 0.230551\n",
      "Train Epoch: 102 [50/106 (47%)]\tTrain Loss: 0.000063\n",
      "Train Epoch: 102 [60/106 (57%)]\tTrain Loss: 0.186077\n",
      "Train Epoch: 102 [70/106 (66%)]\tTrain Loss: 0.000603\n",
      "Train Epoch: 102 [80/106 (75%)]\tTrain Loss: 0.000962\n",
      "Train Epoch: 102 [90/106 (85%)]\tTrain Loss: 0.000577\n",
      "Train Epoch: 102 [100/106 (94%)]\tTrain Loss: 0.000031\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 411/424 (97%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [7.13246845e-05 5.99486008e-03 8.87951101e-05 3.17365048e-05\n",
      " 3.40844417e-05 3.96354917e-05 1.15288573e-03 5.13571722e-05\n",
      " 2.39210422e-05 1.83629367e-04 1.82345484e-05 4.32348461e-05\n",
      " 2.78883344e-05 3.77726501e-05 2.39564848e-04 2.30823262e-05\n",
      " 2.67118885e-04 2.32997860e-04 1.42140692e-04 9.99852419e-01\n",
      " 1.27814259e-04 3.80757041e-02 9.99744117e-01 9.97185409e-01\n",
      " 1.87262427e-04 9.99642611e-01 9.99826968e-01 2.77665022e-05\n",
      " 5.26715048e-05 7.82761854e-05 1.89228787e-03 4.70271101e-04\n",
      " 5.35250991e-04 6.82782338e-05 1.79778435e-05 1.16153151e-05\n",
      " 1.87974019e-05 1.26788771e-04 3.68202054e-05 2.87789062e-05\n",
      " 3.24273096e-05 1.89716247e-05 1.42933888e-04 4.45825426e-05\n",
      " 2.87039948e-05 4.68057042e-05 3.87303473e-04 4.71496256e-03\n",
      " 1.83695951e-03 3.12566517e-05 9.75263072e-04 1.26963860e-05\n",
      " 3.70191665e-05 1.30575627e-05 1.66682177e-04 1.69845662e-05\n",
      " 4.77731373e-04 8.96623260e-06 3.64598236e-05 9.93437206e-05\n",
      " 9.65367675e-01 3.44017148e-02 9.99067605e-01 9.90306497e-01\n",
      " 9.98888433e-01 1.47587183e-04 4.16205497e-04 9.99994636e-01\n",
      " 4.51245934e-01 9.88088310e-01 1.33534952e-04 1.83748460e-04\n",
      " 9.96582806e-01 9.99780715e-01 9.88564551e-01 9.99697089e-01\n",
      " 9.99977708e-01 9.99933958e-01 9.99062717e-01 5.20958304e-01\n",
      " 9.98753428e-01 6.33530598e-03 9.99982476e-01 9.99947309e-01\n",
      " 9.98361290e-01 3.58286723e-02 9.99945402e-01 2.75944937e-02\n",
      " 9.33670044e-01 9.99324322e-01 9.99887586e-01 4.07595944e-05\n",
      " 7.26685248e-05 9.99558747e-01 8.32817313e-05 1.67134320e-04\n",
      " 2.34334275e-05 7.32904486e-03 5.07505938e-05 8.07799280e-01\n",
      " 8.61460626e-01 9.99011278e-01 2.27264700e-05 1.51279906e-04\n",
      " 3.36173479e-03 1.04360297e-04 5.14644384e-02 6.03533874e-04\n",
      " 2.29165162e-04 1.54659050e-04 5.24175593e-05 1.10772045e-04\n",
      " 7.46109895e-03 9.24874902e-01 2.42706301e-04 2.23824292e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 103 [0/106 (0%)]\tTrain Loss: 0.007501\n",
      "Train Epoch: 103 [10/106 (9%)]\tTrain Loss: 0.001191\n",
      "Train Epoch: 103 [20/106 (19%)]\tTrain Loss: 0.000180\n",
      "Train Epoch: 103 [30/106 (28%)]\tTrain Loss: 0.000927\n",
      "Train Epoch: 103 [40/106 (38%)]\tTrain Loss: 0.001310\n",
      "Train Epoch: 103 [50/106 (47%)]\tTrain Loss: 0.000051\n",
      "Train Epoch: 103 [60/106 (57%)]\tTrain Loss: 0.000216\n",
      "Train Epoch: 103 [70/106 (66%)]\tTrain Loss: 0.000212\n",
      "Train Epoch: 103 [80/106 (75%)]\tTrain Loss: 0.000998\n",
      "Train Epoch: 103 [90/106 (85%)]\tTrain Loss: 0.007499\n",
      "Train Epoch: 103 [100/106 (94%)]\tTrain Loss: 0.000032\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 416/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.94001036e-05 5.00970520e-04 7.46307660e-06 2.72036650e-05\n",
      " 1.68781735e-05 1.45572712e-05 4.22091143e-05 4.50248372e-05\n",
      " 1.08096901e-05 1.02775346e-04 1.06335010e-05 3.02020690e-05\n",
      " 1.91267827e-05 2.05932556e-05 1.21804907e-04 5.33710681e-06\n",
      " 5.15725733e-05 1.31076536e-04 3.99437922e-05 9.97966170e-01\n",
      " 5.63184789e-04 2.74819672e-01 9.93342459e-01 9.89050746e-01\n",
      " 9.91937777e-05 9.94650662e-01 9.99353945e-01 7.05529646e-06\n",
      " 1.53664423e-05 1.90941610e-05 1.25770195e-04 1.78910734e-04\n",
      " 1.33030509e-04 3.72115428e-05 7.59031718e-06 4.42039072e-06\n",
      " 6.94267010e-06 1.87880694e-04 1.10199271e-05 6.88908221e-06\n",
      " 9.98361793e-06 4.69153156e-06 5.91719690e-05 2.48131819e-05\n",
      " 1.22319234e-05 1.26016093e-05 1.34272545e-04 2.18469650e-03\n",
      " 1.00718386e-01 4.39355244e-05 3.99524495e-02 3.43109946e-06\n",
      " 3.84176565e-05 4.23734355e-06 7.61079937e-05 1.50480073e-05\n",
      " 6.58027595e-04 5.03838692e-06 2.23002626e-05 1.23970109e-04\n",
      " 9.78567123e-01 4.75918502e-02 9.98019934e-01 9.92605090e-01\n",
      " 8.35208520e-02 1.85780940e-04 4.34940506e-04 9.99362409e-01\n",
      " 9.87596512e-01 9.12948668e-01 1.06400039e-04 1.34906557e-04\n",
      " 9.85393524e-01 9.98809814e-01 2.85107512e-02 2.14872546e-02\n",
      " 9.99803603e-01 9.99579728e-01 9.30975773e-04 6.57551661e-02\n",
      " 9.94592845e-01 3.66268703e-03 9.99744236e-01 9.99212861e-01\n",
      " 9.09744129e-02 7.94289261e-03 1.65862784e-01 1.32226525e-03\n",
      " 9.46662486e-01 1.27471343e-01 9.98887122e-01 2.18465811e-05\n",
      " 3.51682756e-05 9.98283148e-01 4.42942001e-05 1.15001785e-04\n",
      " 1.62427568e-05 5.96044119e-03 1.43870448e-05 5.24598546e-03\n",
      " 7.39662200e-02 9.96108592e-01 6.90629076e-06 6.00531384e-05\n",
      " 4.92734388e-02 9.60144680e-05 7.08861053e-01 2.87924602e-04\n",
      " 2.43301300e-04 1.65557067e-04 3.87915097e-05 9.00145096e-05\n",
      " 8.66059959e-03 3.15197860e-03 3.57914978e-05 1.72126543e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 104 [0/106 (0%)]\tTrain Loss: 0.000360\n",
      "Train Epoch: 104 [10/106 (9%)]\tTrain Loss: 0.000115\n",
      "Train Epoch: 104 [20/106 (19%)]\tTrain Loss: 0.002095\n",
      "Train Epoch: 104 [30/106 (28%)]\tTrain Loss: 0.000068\n",
      "Train Epoch: 104 [40/106 (38%)]\tTrain Loss: 0.000200\n",
      "Train Epoch: 104 [50/106 (47%)]\tTrain Loss: 0.000109\n",
      "Train Epoch: 104 [60/106 (57%)]\tTrain Loss: 0.000025\n",
      "Train Epoch: 104 [70/106 (66%)]\tTrain Loss: 0.000046\n",
      "Train Epoch: 104 [80/106 (75%)]\tTrain Loss: 0.000371\n",
      "Train Epoch: 104 [90/106 (85%)]\tTrain Loss: 0.000169\n",
      "Train Epoch: 104 [100/106 (94%)]\tTrain Loss: 0.143573\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 415/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.37564246e-04 4.59147675e-04 5.92347624e-06 3.05338399e-05\n",
      " 6.41004517e-05 3.46279812e-05 4.95207787e-05 4.70487030e-05\n",
      " 1.01151136e-05 2.05344157e-04 2.43776722e-05 5.03332012e-05\n",
      " 2.25790518e-05 2.40943846e-05 1.00518082e-04 1.12732941e-05\n",
      " 2.45789415e-05 4.49897867e-04 1.55552407e-05 9.66984749e-01\n",
      " 1.41409066e-04 6.79318374e-03 9.99851823e-01 9.95626569e-01\n",
      " 5.74335827e-05 9.99766767e-01 9.99883771e-01 1.53185938e-05\n",
      " 1.27726898e-05 2.35076950e-05 7.98037945e-05 1.09485176e-04\n",
      " 4.32772096e-04 1.94747081e-05 1.06571661e-05 1.34063403e-05\n",
      " 1.32316791e-05 3.26658686e-04 1.05593681e-05 8.20458808e-06\n",
      " 6.48168452e-06 7.09326514e-06 1.16410381e-04 4.33053247e-05\n",
      " 4.48260107e-05 2.88359915e-05 5.78502368e-04 8.87527480e-04\n",
      " 1.94349084e-02 9.27823748e-06 3.22212861e-03 9.02367174e-06\n",
      " 2.85584356e-05 1.01165324e-05 2.23352090e-05 2.37317945e-05\n",
      " 3.27493733e-04 8.03392959e-06 1.59626907e-05 1.01014572e-04\n",
      " 5.08921802e-01 2.06678873e-03 1.26842007e-01 1.36754373e-02\n",
      " 8.00826073e-01 7.79692709e-05 2.48120050e-04 9.99998331e-01\n",
      " 9.99672532e-01 8.80880952e-01 1.11681511e-04 1.08020722e-04\n",
      " 8.30132782e-01 9.99765694e-01 8.57757866e-01 3.95181090e-01\n",
      " 9.99992132e-01 9.99964237e-01 3.34337098e-03 1.20957553e-01\n",
      " 8.08360696e-01 3.55266180e-04 9.99988556e-01 9.99880791e-01\n",
      " 3.33258044e-03 1.38994423e-03 1.64098840e-03 2.51951191e-04\n",
      " 5.34131750e-02 2.54873448e-04 9.99874830e-01 1.97565732e-05\n",
      " 4.51258529e-05 9.99233246e-01 2.08159545e-05 1.40878532e-04\n",
      " 1.41238297e-05 1.89645260e-04 1.84810087e-05 3.39180493e-04\n",
      " 1.51323471e-02 9.98209000e-01 2.13868243e-05 3.60126760e-05\n",
      " 3.39886150e-03 6.05009154e-05 4.61710915e-02 6.00966814e-05\n",
      " 7.96512031e-05 1.28378204e-04 1.83596603e-05 2.05798642e-05\n",
      " 2.36037464e-04 1.84562145e-04 1.96201108e-05 6.98467193e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 105 [0/106 (0%)]\tTrain Loss: 0.038378\n",
      "Train Epoch: 105 [10/106 (9%)]\tTrain Loss: 0.000077\n",
      "Train Epoch: 105 [20/106 (19%)]\tTrain Loss: 0.025371\n",
      "Train Epoch: 105 [30/106 (28%)]\tTrain Loss: 0.000285\n",
      "Train Epoch: 105 [40/106 (38%)]\tTrain Loss: 0.000321\n",
      "Train Epoch: 105 [50/106 (47%)]\tTrain Loss: 0.000457\n",
      "Train Epoch: 105 [60/106 (57%)]\tTrain Loss: 0.001817\n",
      "Train Epoch: 105 [70/106 (66%)]\tTrain Loss: 0.129645\n",
      "Train Epoch: 105 [80/106 (75%)]\tTrain Loss: 0.000034\n",
      "Train Epoch: 105 [90/106 (85%)]\tTrain Loss: 0.000065\n",
      "Train Epoch: 105 [100/106 (94%)]\tTrain Loss: 0.015362\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 414/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.12827410e-06 6.08335075e-04 7.69651137e-07 3.47142486e-06\n",
      " 1.86328839e-06 1.06279799e-06 4.17820365e-06 3.23900667e-06\n",
      " 4.20489954e-07 9.77405398e-06 9.87981934e-07 1.89304058e-06\n",
      " 1.51618599e-06 2.39576889e-06 5.08844187e-06 9.17717898e-07\n",
      " 4.38724601e-06 2.92886816e-05 7.76119691e-07 9.94601250e-01\n",
      " 1.40651164e-05 1.47714317e-01 9.98573542e-01 9.87624168e-01\n",
      " 1.82074054e-05 9.98069823e-01 9.98997748e-01 8.54852715e-07\n",
      " 1.10830365e-06 2.67395512e-06 4.13119851e-05 2.15278942e-05\n",
      " 1.59387782e-05 1.61178093e-06 5.54314568e-07 5.20776155e-07\n",
      " 4.91682272e-07 7.04295453e-05 1.06566620e-06 8.46741386e-07\n",
      " 1.55817156e-06 5.48961737e-07 4.48811761e-06 2.99482372e-06\n",
      " 2.23407596e-06 1.81396501e-06 1.79056897e-05 4.14239330e-04\n",
      " 2.61081327e-02 3.10580117e-06 1.00306026e-03 3.69596364e-07\n",
      " 2.38050734e-06 5.40673000e-07 3.83525230e-06 9.63396019e-07\n",
      " 1.58255323e-04 3.23792392e-07 7.94774849e-07 1.52649318e-05\n",
      " 9.80180562e-01 1.91012863e-02 9.76187289e-01 5.61047196e-01\n",
      " 9.72271502e-01 1.09189978e-05 6.68781286e-05 9.99207556e-01\n",
      " 9.85654891e-01 9.52002048e-01 4.09585846e-05 5.33723505e-05\n",
      " 9.85016823e-01 9.98016000e-01 1.50628403e-01 6.67779922e-01\n",
      " 9.99684095e-01 9.99205410e-01 4.23933798e-03 1.18718613e-02\n",
      " 9.96364415e-01 1.68765342e-04 9.99189436e-01 9.98823345e-01\n",
      " 3.13995928e-02 1.87375734e-03 6.99610217e-03 2.51243455e-05\n",
      " 9.73885536e-01 3.48766078e-03 9.98698831e-01 2.01896478e-06\n",
      " 2.37761219e-06 9.97093201e-01 1.28927932e-05 1.86802099e-05\n",
      " 9.57968496e-07 1.09344139e-03 6.05793548e-06 2.55316263e-03\n",
      " 2.34205779e-02 9.94145870e-01 1.54468114e-06 1.36989684e-05\n",
      " 7.71714985e-01 2.21258560e-05 8.60683799e-01 3.86380680e-06\n",
      " 1.53778328e-05 9.59382378e-05 2.90307048e-06 8.52267749e-06\n",
      " 3.72913107e-03 4.82905161e-04 2.14393390e-06 1.62891065e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 106 [0/106 (0%)]\tTrain Loss: 0.000102\n",
      "Train Epoch: 106 [10/106 (9%)]\tTrain Loss: 0.000491\n",
      "Train Epoch: 106 [20/106 (19%)]\tTrain Loss: 0.000149\n",
      "Train Epoch: 106 [30/106 (28%)]\tTrain Loss: 0.000390\n",
      "Train Epoch: 106 [40/106 (38%)]\tTrain Loss: 0.000011\n",
      "Train Epoch: 106 [50/106 (47%)]\tTrain Loss: 0.000103\n",
      "Train Epoch: 106 [60/106 (57%)]\tTrain Loss: 0.000262\n",
      "Train Epoch: 106 [70/106 (66%)]\tTrain Loss: 0.000074\n",
      "Train Epoch: 106 [80/106 (75%)]\tTrain Loss: 0.000173\n",
      "Train Epoch: 106 [90/106 (85%)]\tTrain Loss: 0.000010\n",
      "Train Epoch: 106 [100/106 (94%)]\tTrain Loss: 0.000011\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 421/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.70049918e-06 4.14915470e-04 7.93210461e-07 3.36215749e-06\n",
      " 2.12715236e-06 1.34742868e-06 3.04324726e-06 3.65786195e-06\n",
      " 4.78687468e-07 7.09249525e-06 1.05689924e-06 1.98032785e-06\n",
      " 1.84551175e-06 4.60459069e-06 1.17080899e-05 6.44347040e-07\n",
      " 4.56079579e-06 6.67317945e-05 1.15661737e-06 9.97424960e-01\n",
      " 4.23357778e-05 8.78822625e-01 9.99792874e-01 9.96283591e-01\n",
      " 1.36374965e-05 9.99783456e-01 9.99901652e-01 1.15684804e-06\n",
      " 1.37340294e-06 2.34257755e-06 3.59917321e-05 2.13606108e-05\n",
      " 9.73360875e-06 2.24172072e-06 6.20063247e-07 5.66184042e-07\n",
      " 7.16394879e-07 1.88056147e-04 1.28548402e-06 1.40682118e-06\n",
      " 3.03969432e-06 8.81727942e-07 1.09057819e-05 6.65861808e-06\n",
      " 2.11046381e-06 1.57042700e-06 1.94506538e-05 1.95848857e-04\n",
      " 2.71480288e-02 3.00754050e-06 1.04377919e-03 4.39616116e-07\n",
      " 3.02791886e-06 5.79226139e-07 3.56131613e-06 1.05604090e-06\n",
      " 7.99539266e-05 3.89966715e-07 1.21654739e-06 1.22203573e-05\n",
      " 9.97410476e-01 7.09709385e-03 9.97751296e-01 9.13267612e-01\n",
      " 9.87878740e-01 1.16521705e-05 5.60250992e-05 9.99956727e-01\n",
      " 9.98691857e-01 9.90545392e-01 1.68346578e-05 2.57431293e-05\n",
      " 9.96666491e-01 9.99652505e-01 6.51561260e-01 7.69275725e-01\n",
      " 9.99986410e-01 9.99967217e-01 4.82673233e-04 7.02514127e-03\n",
      " 9.93696034e-01 5.06759752e-05 9.99966264e-01 9.99717772e-01\n",
      " 2.16144845e-02 8.54729500e-04 2.10673851e-03 3.66841523e-05\n",
      " 9.95568216e-01 4.89287137e-04 9.99840260e-01 2.33614810e-06\n",
      " 2.83822351e-06 9.99362767e-01 8.29427336e-06 1.78796163e-05\n",
      " 1.29073544e-06 4.89709608e-04 4.61568698e-06 7.18411570e-03\n",
      " 2.81661586e-03 9.98416066e-01 1.01077683e-06 6.62048114e-06\n",
      " 6.62895799e-01 1.15700095e-05 5.54425493e-02 1.43231682e-05\n",
      " 2.99836775e-05 3.97444237e-05 4.11321889e-06 6.02151567e-06\n",
      " 8.77965335e-03 2.31354861e-04 2.10741268e-06 1.13184751e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 107 [0/106 (0%)]\tTrain Loss: 0.000234\n",
      "Train Epoch: 107 [10/106 (9%)]\tTrain Loss: 0.135100\n",
      "Train Epoch: 107 [20/106 (19%)]\tTrain Loss: 0.000091\n",
      "Train Epoch: 107 [30/106 (28%)]\tTrain Loss: 0.000087\n",
      "Train Epoch: 107 [40/106 (38%)]\tTrain Loss: 0.000016\n",
      "Train Epoch: 107 [50/106 (47%)]\tTrain Loss: 0.000076\n",
      "Train Epoch: 107 [60/106 (57%)]\tTrain Loss: 0.000016\n",
      "Train Epoch: 107 [70/106 (66%)]\tTrain Loss: 0.004125\n",
      "Train Epoch: 107 [80/106 (75%)]\tTrain Loss: 0.000010\n",
      "Train Epoch: 107 [90/106 (85%)]\tTrain Loss: 0.000056\n",
      "Train Epoch: 107 [100/106 (94%)]\tTrain Loss: 0.000143\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.04118344e-05 3.73830204e-04 2.61827290e-06 7.92287392e-06\n",
      " 6.35151900e-06 5.62563082e-06 1.49322141e-05 1.04458877e-05\n",
      " 1.67381143e-06 1.70942676e-05 3.08875201e-06 4.88158503e-06\n",
      " 6.03679291e-06 1.31032684e-05 5.27530392e-05 2.08395750e-06\n",
      " 1.32391006e-05 8.62101209e-04 4.20280958e-06 9.99085069e-01\n",
      " 8.18244662e-05 9.70615327e-01 9.99924421e-01 9.98440325e-01\n",
      " 4.84758275e-05 9.99951720e-01 9.99971628e-01 4.49131585e-06\n",
      " 4.78000902e-06 7.10794620e-06 1.36115224e-04 6.73442919e-05\n",
      " 3.83618972e-05 5.86575607e-06 1.85110468e-06 1.67373480e-06\n",
      " 2.14072406e-06 5.35250467e-04 3.98309749e-06 4.64772893e-06\n",
      " 1.00646266e-05 2.37808854e-06 2.68745134e-05 1.52895536e-05\n",
      " 5.44864452e-06 4.20723700e-06 5.72372446e-05 3.21435014e-04\n",
      " 1.12881698e-02 6.17970909e-06 1.40204758e-03 1.14877389e-06\n",
      " 6.51129858e-06 1.41981434e-06 1.40832635e-05 2.68495546e-06\n",
      " 3.21783911e-04 1.09771315e-06 3.41264195e-06 2.57760221e-05\n",
      " 9.98773754e-01 5.26714176e-02 9.99132931e-01 9.46727633e-01\n",
      " 9.97412503e-01 2.53144881e-05 1.60634314e-04 9.99994755e-01\n",
      " 9.99498129e-01 9.97825384e-01 4.77254107e-05 7.48185339e-05\n",
      " 9.96837854e-01 9.99878049e-01 9.89406407e-01 8.69300485e-01\n",
      " 9.99995589e-01 9.99984145e-01 3.33429431e-03 4.59392183e-02\n",
      " 9.91755724e-01 7.70713959e-05 9.99993920e-01 9.99913692e-01\n",
      " 4.10505474e-01 2.35355063e-03 1.45831527e-02 1.43396537e-04\n",
      " 9.97728765e-01 4.40260535e-03 9.99952197e-01 7.40403175e-06\n",
      " 8.40411303e-06 9.99549091e-01 2.12533632e-05 3.84690175e-05\n",
      " 3.01937166e-06 5.61113283e-03 1.72515320e-05 2.44392425e-01\n",
      " 3.20831180e-01 9.99707162e-01 4.00803219e-06 1.27950252e-05\n",
      " 8.59829962e-01 2.21115806e-05 3.20989549e-01 3.86706561e-05\n",
      " 1.06282510e-04 1.04873470e-04 1.08426366e-05 1.94325567e-05\n",
      " 3.58455419e-03 9.15260112e-04 8.44696297e-06 5.73856851e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 108 [0/106 (0%)]\tTrain Loss: 0.000224\n",
      "Train Epoch: 108 [10/106 (9%)]\tTrain Loss: 0.000129\n",
      "Train Epoch: 108 [20/106 (19%)]\tTrain Loss: 0.000013\n",
      "Train Epoch: 108 [30/106 (28%)]\tTrain Loss: 0.000033\n",
      "Train Epoch: 108 [40/106 (38%)]\tTrain Loss: 0.000020\n",
      "Train Epoch: 108 [50/106 (47%)]\tTrain Loss: 0.000011\n",
      "Train Epoch: 108 [60/106 (57%)]\tTrain Loss: 0.009107\n",
      "Train Epoch: 108 [70/106 (66%)]\tTrain Loss: 0.000008\n",
      "Train Epoch: 108 [80/106 (75%)]\tTrain Loss: 0.009438\n",
      "Train Epoch: 108 [90/106 (85%)]\tTrain Loss: 0.121732\n",
      "Train Epoch: 108 [100/106 (94%)]\tTrain Loss: 0.000285\n",
      "\n",
      "Train set: Average loss: 0.0002, Accuracy: 419/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.01446801e-06 1.14961877e-04 5.18611273e-07 1.74600791e-06\n",
      " 1.09524103e-06 8.71370730e-07 2.46545028e-06 2.33634410e-06\n",
      " 3.29855709e-07 3.25032556e-06 5.85568955e-07 9.50022354e-07\n",
      " 1.10013616e-06 1.87097180e-06 9.34353739e-06 4.00952871e-07\n",
      " 2.50478161e-06 2.07513542e-04 6.63478204e-07 9.90196943e-01\n",
      " 1.42063536e-05 8.65405917e-01 9.98484671e-01 9.85254228e-01\n",
      " 1.90137525e-05 9.99102712e-01 9.99570072e-01 6.94237826e-07\n",
      " 8.54877101e-07 1.25644976e-06 3.61183629e-05 1.47718811e-05\n",
      " 8.35312494e-06 1.36111373e-06 3.75497962e-07 3.15671599e-07\n",
      " 3.83606817e-07 1.65681675e-04 8.91269394e-07 1.09781570e-06\n",
      " 3.18338789e-06 5.93227924e-07 5.71716873e-06 3.25580754e-06\n",
      " 1.09400719e-06 1.10988594e-06 1.28493657e-05 1.78775328e-04\n",
      " 2.50526308e-03 1.95446069e-06 2.36536682e-04 2.27835116e-07\n",
      " 1.44207422e-06 3.07229556e-07 2.89661034e-06 5.38396705e-07\n",
      " 5.52779275e-05 2.07772544e-07 6.99327984e-07 5.83939482e-06\n",
      " 9.86899495e-01 5.15893288e-03 9.91242647e-01 6.79913998e-01\n",
      " 9.56440449e-01 9.52393202e-06 4.04893362e-05 9.99807298e-01\n",
      " 9.96097803e-01 9.86429334e-01 1.33826443e-05 1.95888133e-05\n",
      " 9.71264124e-01 9.98771369e-01 7.05790639e-01 2.28272498e-01\n",
      " 9.99881983e-01 9.99660611e-01 4.18090215e-03 4.52223420e-03\n",
      " 9.39066112e-01 2.46913332e-05 9.99848008e-01 9.99362290e-01\n",
      " 1.77047670e-01 6.47403474e-04 6.52350951e-04 1.92503958e-05\n",
      " 9.39065039e-01 1.68568396e-03 9.99033809e-01 1.78179516e-06\n",
      " 2.19371691e-06 9.96922314e-01 4.83427857e-06 6.05180321e-06\n",
      " 5.31130581e-07 2.38685752e-03 3.10408382e-06 1.56050138e-02\n",
      " 1.50791556e-02 9.98254597e-01 7.47826334e-07 4.06126901e-06\n",
      " 4.87307519e-01 5.69196391e-06 1.49369940e-01 1.13319975e-05\n",
      " 4.47978455e-05 2.95146710e-05 3.13657301e-06 4.81698862e-06\n",
      " 6.98904740e-04 2.32855935e-04 1.64952667e-06 2.10580492e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 109 [0/106 (0%)]\tTrain Loss: 0.000099\n",
      "Train Epoch: 109 [10/106 (9%)]\tTrain Loss: 0.000036\n",
      "Train Epoch: 109 [20/106 (19%)]\tTrain Loss: 0.000682\n",
      "Train Epoch: 109 [30/106 (28%)]\tTrain Loss: 0.000026\n",
      "Train Epoch: 109 [40/106 (38%)]\tTrain Loss: 0.000047\n",
      "Train Epoch: 109 [50/106 (47%)]\tTrain Loss: 0.000007\n",
      "Train Epoch: 109 [60/106 (57%)]\tTrain Loss: 0.000148\n",
      "Train Epoch: 109 [70/106 (66%)]\tTrain Loss: 0.148803\n",
      "Train Epoch: 109 [80/106 (75%)]\tTrain Loss: 0.000007\n",
      "Train Epoch: 109 [90/106 (85%)]\tTrain Loss: 0.000074\n",
      "Train Epoch: 109 [100/106 (94%)]\tTrain Loss: 0.000130\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 419/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.94750214e-06 1.53453933e-04 4.19000457e-07 1.61640810e-06\n",
      " 9.77140417e-07 8.53593292e-07 3.74298793e-06 2.34227809e-06\n",
      " 3.33716883e-07 5.49434981e-06 5.13287375e-07 1.19899960e-06\n",
      " 9.22774746e-07 3.53231485e-06 1.74992547e-05 3.61362595e-07\n",
      " 3.77977699e-06 3.43896827e-04 7.52859705e-07 9.96482849e-01\n",
      " 1.57873783e-05 9.50795710e-01 9.99692440e-01 9.94133174e-01\n",
      " 2.66741008e-05 9.99705970e-01 9.99788225e-01 4.63938107e-07\n",
      " 6.36111167e-07 1.18030061e-06 2.27748533e-05 1.59724677e-05\n",
      " 1.39421081e-05 1.19726053e-06 2.89777432e-07 2.59408466e-07\n",
      " 3.12410123e-07 4.33299312e-04 7.88974262e-07 1.06653044e-06\n",
      " 3.94211020e-06 5.95313509e-07 5.88314515e-06 3.57447402e-06\n",
      " 9.91432671e-07 8.99409940e-07 1.76491867e-05 3.80298705e-04\n",
      " 1.04595954e-02 1.89695675e-06 6.81331148e-04 1.61403307e-07\n",
      " 1.43503632e-06 2.49597036e-07 2.74818444e-06 4.56567733e-07\n",
      " 1.19571778e-04 1.62003190e-07 8.43724251e-07 1.09560688e-05\n",
      " 9.98989642e-01 4.60101068e-01 9.99300838e-01 9.92431760e-01\n",
      " 9.59776163e-01 2.36446631e-05 6.96234129e-05 9.99945402e-01\n",
      " 9.98950958e-01 9.87545550e-01 2.75115162e-05 5.14839267e-05\n",
      " 9.72237170e-01 9.99545515e-01 4.97219563e-01 9.44686651e-01\n",
      " 9.99954462e-01 9.99841332e-01 2.05237996e-02 2.57276222e-02\n",
      " 9.87897158e-01 2.77810959e-05 9.99952674e-01 9.99770820e-01\n",
      " 5.41067302e-01 2.44003488e-03 1.45891588e-03 2.55641480e-05\n",
      " 9.30444062e-01 2.36203941e-03 9.99777138e-01 1.92974994e-06\n",
      " 2.62960384e-06 9.99143839e-01 5.11595772e-06 9.05923116e-06\n",
      " 4.64656296e-07 3.42770363e-03 2.49810682e-06 4.45694253e-02\n",
      " 1.68752015e-01 9.99356329e-01 5.97297117e-07 5.33767661e-06\n",
      " 1.24493212e-01 7.01466706e-06 2.33879745e-01 5.42473936e-05\n",
      " 9.34029740e-05 5.24586176e-05 2.81003577e-06 5.91938260e-06\n",
      " 1.92639907e-03 3.27602087e-04 2.12959412e-06 5.35559484e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 110 [0/106 (0%)]\tTrain Loss: 0.000016\n",
      "Train Epoch: 110 [10/106 (9%)]\tTrain Loss: 0.155315\n",
      "Train Epoch: 110 [20/106 (19%)]\tTrain Loss: 0.138999\n",
      "Train Epoch: 110 [30/106 (28%)]\tTrain Loss: 0.000009\n",
      "Train Epoch: 110 [40/106 (38%)]\tTrain Loss: 0.002049\n",
      "Train Epoch: 110 [50/106 (47%)]\tTrain Loss: 0.000012\n",
      "Train Epoch: 110 [60/106 (57%)]\tTrain Loss: 0.000107\n",
      "Train Epoch: 110 [70/106 (66%)]\tTrain Loss: 0.000126\n",
      "Train Epoch: 110 [80/106 (75%)]\tTrain Loss: 0.009202\n",
      "Train Epoch: 110 [90/106 (85%)]\tTrain Loss: 0.000151\n",
      "Train Epoch: 110 [100/106 (94%)]\tTrain Loss: 0.000057\n",
      "\n",
      "Train set: Average loss: 0.0002, Accuracy: 419/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.84693090e-05 1.60820445e-03 3.85045041e-06 1.75441364e-05\n",
      " 1.33142594e-05 9.65759500e-06 1.65957445e-05 2.71377721e-05\n",
      " 2.74675131e-06 4.14517635e-05 5.77916308e-06 9.75373587e-06\n",
      " 1.11698109e-05 3.21250227e-05 1.16763127e-04 3.86036299e-06\n",
      " 2.74433078e-05 5.29138139e-04 8.46902458e-06 9.99485016e-01\n",
      " 5.16670349e-04 9.82173026e-01 9.99852419e-01 9.98757720e-01\n",
      " 1.04688828e-04 9.99920011e-01 9.99964118e-01 7.31371983e-06\n",
      " 8.39294808e-06 1.39428521e-05 5.81852859e-04 1.89957616e-04\n",
      " 6.63514438e-05 9.01380645e-06 3.21165885e-06 2.93461426e-06\n",
      " 3.83646329e-06 1.59802334e-03 9.52643950e-06 9.82370329e-06\n",
      " 2.57331885e-05 5.04673517e-06 6.17525147e-05 3.57366589e-05\n",
      " 9.71125519e-06 7.65009645e-06 1.29464446e-04 8.68005736e-04\n",
      " 1.13932781e-01 1.46712891e-05 8.76570866e-03 2.16749368e-06\n",
      " 1.42486988e-05 2.76016181e-06 2.68133026e-05 4.44816988e-06\n",
      " 1.40740408e-03 2.10441226e-06 7.47613376e-06 6.05455898e-05\n",
      " 9.99229670e-01 4.09657061e-01 9.99407291e-01 9.94620204e-01\n",
      " 9.97336090e-01 7.92852588e-05 4.65341931e-04 9.99988437e-01\n",
      " 9.99680281e-01 9.96933818e-01 1.33725567e-04 2.60976492e-04\n",
      " 9.97505724e-01 9.99800265e-01 9.93760884e-01 9.91130352e-01\n",
      " 9.99996662e-01 9.99996662e-01 2.89954757e-03 1.38860047e-01\n",
      " 9.97842669e-01 2.72795471e-04 9.99993205e-01 9.99854684e-01\n",
      " 2.48121291e-01 2.08338331e-02 1.55358193e-02 2.79423431e-04\n",
      " 9.97994184e-01 5.21635590e-03 9.99967813e-01 1.27619023e-05\n",
      " 1.67805138e-05 9.99602735e-01 5.47577329e-05 1.02442580e-04\n",
      " 6.64779373e-06 2.21593361e-02 4.91988067e-05 5.16883910e-01\n",
      " 8.58674347e-01 9.99502778e-01 7.44951694e-06 3.33903299e-05\n",
      " 8.52152526e-01 6.15597892e-05 4.12256539e-01 6.96685747e-05\n",
      " 2.15029751e-04 2.72599718e-04 2.54978950e-05 4.79166702e-05\n",
      " 7.65910512e-03 1.43305224e-03 2.15858454e-05 9.09072260e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 20 TN= 55 FN= 36 FP= 5\n",
      "TP+FP 25\n",
      "precision 0.8\n",
      "recall 0.35714285714285715\n",
      "F1 0.4938271604938272\n",
      "acc 0.646551724137931\n",
      "AUCp 0.6369047619047619\n",
      "AUC 0.8023809523809524\n",
      "\n",
      " The epoch is 110, average recall: 0.3571, average precision: 0.8000,average F1: 0.4938, average accuracy: 0.6466, average AUC: 0.8024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 111 [0/106 (0%)]\tTrain Loss: 0.000010\n",
      "Train Epoch: 111 [10/106 (9%)]\tTrain Loss: 0.000021\n",
      "Train Epoch: 111 [20/106 (19%)]\tTrain Loss: 0.416993\n",
      "Train Epoch: 111 [30/106 (28%)]\tTrain Loss: 0.002041\n",
      "Train Epoch: 111 [40/106 (38%)]\tTrain Loss: 0.000023\n",
      "Train Epoch: 111 [50/106 (47%)]\tTrain Loss: 0.000018\n",
      "Train Epoch: 111 [60/106 (57%)]\tTrain Loss: 0.000126\n",
      "Train Epoch: 111 [70/106 (66%)]\tTrain Loss: 0.000014\n",
      "Train Epoch: 111 [80/106 (75%)]\tTrain Loss: 0.000489\n",
      "Train Epoch: 111 [90/106 (85%)]\tTrain Loss: 0.013493\n",
      "Train Epoch: 111 [100/106 (94%)]\tTrain Loss: 0.000171\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 417/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [7.43023484e-05 1.99480751e-03 1.87112491e-05 4.57994029e-05\n",
      " 4.01986363e-05 4.22931626e-05 1.19951270e-04 6.82942555e-05\n",
      " 1.09795619e-05 1.08327687e-04 1.82178956e-05 2.96949638e-05\n",
      " 3.84880259e-05 5.68798059e-05 1.92488500e-04 1.19792212e-05\n",
      " 8.46434632e-05 5.27168298e-03 2.78283478e-05 9.96428430e-01\n",
      " 1.23954553e-03 9.84216571e-01 9.99690890e-01 9.96793211e-01\n",
      " 6.01335894e-04 9.99780357e-01 9.99873877e-01 2.69784050e-05\n",
      " 2.50649009e-05 4.93334592e-05 5.61198744e-04 2.93105317e-04\n",
      " 4.50986990e-04 3.20089639e-05 1.04541987e-05 9.53857580e-06\n",
      " 1.14297645e-05 4.27613501e-03 2.50710218e-05 2.82857127e-05\n",
      " 8.25801908e-05 1.68913939e-05 1.82879769e-04 1.06737745e-04\n",
      " 3.27817033e-05 2.62457488e-05 5.54502651e-04 2.53634597e-03\n",
      " 4.49816063e-02 4.74240296e-05 6.81139436e-03 5.74013666e-06\n",
      " 3.66291679e-05 7.44614317e-06 1.56360591e-04 1.47570690e-05\n",
      " 1.37502968e-03 6.22259040e-06 2.17357119e-05 1.73596272e-04\n",
      " 9.91346061e-01 4.50408123e-02 9.92168486e-01 8.34296763e-01\n",
      " 9.41060960e-01 2.07996898e-04 8.87185044e-04 9.99991775e-01\n",
      " 9.99558985e-01 9.96198118e-01 3.12110147e-04 4.31691762e-04\n",
      " 9.68845606e-01 9.99681830e-01 9.39638734e-01 3.52269486e-02\n",
      " 9.99992967e-01 9.99982953e-01 7.45438877e-03 4.36296314e-02\n",
      " 8.71722400e-01 3.05775407e-04 9.99986887e-01 9.99826729e-01\n",
      " 8.82553399e-01 1.45206414e-02 4.19670250e-03 3.50437243e-04\n",
      " 9.87046123e-01 8.23950768e-03 9.99660730e-01 4.11480760e-05\n",
      " 5.42008565e-05 9.98723924e-01 1.58830240e-04 3.29457835e-04\n",
      " 1.65678812e-05 3.61941159e-02 1.01792677e-04 2.77586758e-01\n",
      " 8.51387203e-01 9.99499679e-01 2.69062039e-05 7.39239549e-05\n",
      " 2.83606678e-01 1.48640218e-04 3.69276553e-01 3.75792326e-04\n",
      " 9.36119584e-04 7.55212677e-04 7.30863394e-05 1.75210153e-04\n",
      " 7.22878892e-03 1.11372992e-02 3.84137747e-05 3.52822361e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 112 [0/106 (0%)]\tTrain Loss: 0.000009\n",
      "Train Epoch: 112 [10/106 (9%)]\tTrain Loss: 0.010878\n",
      "Train Epoch: 112 [20/106 (19%)]\tTrain Loss: 0.000092\n",
      "Train Epoch: 112 [30/106 (28%)]\tTrain Loss: 0.000351\n",
      "Train Epoch: 112 [40/106 (38%)]\tTrain Loss: 0.000236\n",
      "Train Epoch: 112 [50/106 (47%)]\tTrain Loss: 0.000013\n",
      "Train Epoch: 112 [60/106 (57%)]\tTrain Loss: 0.000183\n",
      "Train Epoch: 112 [70/106 (66%)]\tTrain Loss: 0.000037\n",
      "Train Epoch: 112 [80/106 (75%)]\tTrain Loss: 0.000066\n",
      "Train Epoch: 112 [90/106 (85%)]\tTrain Loss: 0.000014\n",
      "Train Epoch: 112 [100/106 (94%)]\tTrain Loss: 0.000199\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 424/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [9.26097255e-06 5.93785371e-04 2.46498007e-06 8.41679321e-06\n",
      " 5.68829046e-06 5.55292581e-06 1.35166019e-05 1.13751557e-05\n",
      " 1.52304358e-06 2.23365932e-05 3.36729795e-06 6.79223331e-06\n",
      " 7.31360069e-06 1.16163346e-05 2.91757515e-05 1.74914783e-06\n",
      " 1.40476104e-05 9.47602326e-04 3.89964043e-06 9.99543726e-01\n",
      " 1.22299558e-03 9.85617697e-01 9.99954462e-01 9.99348104e-01\n",
      " 6.37732883e-05 9.99983788e-01 9.99993563e-01 3.60554714e-06\n",
      " 4.60734918e-06 8.65215043e-06 4.05945088e-04 1.02092279e-04\n",
      " 6.34258904e-05 4.46713875e-06 1.41371265e-06 1.29602211e-06\n",
      " 1.74227864e-06 1.15847366e-03 3.43347915e-06 6.30305885e-06\n",
      " 5.12780352e-05 2.82973156e-06 2.49145705e-05 1.98996386e-05\n",
      " 6.11661517e-06 3.73621901e-06 6.59080033e-05 3.50817310e-04\n",
      " 3.33266966e-02 8.07995457e-06 4.25679749e-03 8.30169938e-07\n",
      " 8.83329631e-06 1.35643575e-06 2.60522847e-05 2.59042599e-06\n",
      " 5.45625982e-04 9.96048016e-07 3.80593610e-06 4.85730998e-05\n",
      " 9.99380469e-01 1.92750394e-02 9.99561012e-01 9.79235709e-01\n",
      " 9.99768078e-01 3.64062544e-05 1.39258991e-04 9.99999285e-01\n",
      " 9.99966383e-01 9.98715043e-01 5.84971349e-05 1.41985467e-04\n",
      " 9.99564111e-01 9.99978662e-01 9.99121606e-01 2.09590316e-01\n",
      " 9.99999642e-01 9.99999523e-01 6.56287419e-04 9.27301124e-02\n",
      " 9.99122202e-01 9.81536796e-05 9.99999404e-01 9.99978423e-01\n",
      " 7.10973084e-01 5.70012117e-03 8.78336548e-04 5.96549180e-05\n",
      " 9.97224927e-01 1.05032686e-03 9.99989867e-01 6.81466281e-06\n",
      " 9.57778138e-06 9.99881148e-01 3.51433300e-05 1.29668653e-04\n",
      " 3.78380901e-06 5.22724679e-03 2.72842844e-05 9.49250817e-01\n",
      " 7.95218289e-01 9.99910235e-01 2.66960296e-06 1.68434744e-05\n",
      " 9.44846511e-01 5.25947144e-05 8.83493483e-01 4.67695245e-05\n",
      " 1.42797391e-04 1.36973918e-04 9.05216712e-06 2.70314576e-05\n",
      " 2.76544690e-02 3.10559478e-03 6.78838660e-06 2.81011598e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 113 [0/106 (0%)]\tTrain Loss: 0.000149\n",
      "Train Epoch: 113 [10/106 (9%)]\tTrain Loss: 0.000687\n",
      "Train Epoch: 113 [20/106 (19%)]\tTrain Loss: 0.000564\n",
      "Train Epoch: 113 [30/106 (28%)]\tTrain Loss: 0.000044\n",
      "Train Epoch: 113 [40/106 (38%)]\tTrain Loss: 0.010552\n",
      "Train Epoch: 113 [50/106 (47%)]\tTrain Loss: 0.000025\n",
      "Train Epoch: 113 [60/106 (57%)]\tTrain Loss: 0.000008\n",
      "Train Epoch: 113 [70/106 (66%)]\tTrain Loss: 0.000024\n",
      "Train Epoch: 113 [80/106 (75%)]\tTrain Loss: 0.000132\n",
      "Train Epoch: 113 [90/106 (85%)]\tTrain Loss: 0.000049\n",
      "Train Epoch: 113 [100/106 (94%)]\tTrain Loss: 0.000008\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 423/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.02289266e-06 2.58271990e-04 4.79287678e-07 2.96984649e-06\n",
      " 1.72816988e-06 1.05220465e-06 2.75529874e-06 3.98505790e-06\n",
      " 2.84571826e-07 3.52173606e-06 6.47194099e-07 7.46748753e-07\n",
      " 3.17442027e-06 3.46696493e-06 1.11812897e-05 5.30357227e-07\n",
      " 6.12429631e-06 1.04164836e-04 1.02527531e-06 9.98730242e-01\n",
      " 8.77679122e-05 9.47480083e-01 9.99932289e-01 9.98659849e-01\n",
      " 2.29262168e-05 9.99967217e-01 9.99978662e-01 7.85340205e-07\n",
      " 1.14650516e-06 2.21363121e-06 3.91751928e-05 3.20841136e-05\n",
      " 1.39700851e-05 1.39948543e-06 3.33707021e-07 3.25176899e-07\n",
      " 3.91639389e-07 6.06464819e-05 8.22077936e-07 8.18072124e-07\n",
      " 3.91049980e-06 4.96260725e-07 4.64596997e-06 3.35120967e-06\n",
      " 1.39414237e-06 1.00966690e-06 1.86148027e-05 2.27277706e-04\n",
      " 7.99460243e-03 1.75973207e-06 1.88702857e-03 1.78148298e-07\n",
      " 1.15906880e-06 3.73868517e-07 6.45161390e-06 4.64477750e-07\n",
      " 7.94448060e-05 1.94575378e-07 6.61779609e-07 6.47080788e-06\n",
      " 9.99647021e-01 5.52680790e-01 9.99860287e-01 9.98487115e-01\n",
      " 9.98631299e-01 1.61075968e-05 5.57994535e-05 9.99995470e-01\n",
      " 9.99920726e-01 9.98797774e-01 4.63182550e-05 1.16004041e-04\n",
      " 9.99630570e-01 9.99959230e-01 9.81549799e-01 5.37986010e-02\n",
      " 9.99997735e-01 9.99995708e-01 1.23714199e-04 4.28416841e-02\n",
      " 9.99774516e-01 2.96390008e-05 9.99996662e-01 9.99961972e-01\n",
      " 6.89967930e-01 9.14344471e-03 4.57967049e-04 2.26777593e-05\n",
      " 9.92026031e-01 1.75963773e-03 9.99979973e-01 1.85670751e-06\n",
      " 1.73888247e-06 9.99857068e-01 2.25417461e-05 3.32904310e-05\n",
      " 8.31356772e-07 2.72737555e-02 1.37756460e-05 9.98869717e-01\n",
      " 8.37349117e-01 9.99856830e-01 9.09153130e-07 9.60501984e-06\n",
      " 9.94342566e-01 1.77774000e-05 4.51253593e-01 3.03311481e-05\n",
      " 1.40466873e-04 2.55714665e-04 2.81947928e-06 7.54373559e-06\n",
      " 2.36152927e-03 1.42059019e-02 2.39605924e-06 1.20510877e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 114 [0/106 (0%)]\tTrain Loss: 0.000013\n",
      "Train Epoch: 114 [10/106 (9%)]\tTrain Loss: 0.000233\n",
      "Train Epoch: 114 [20/106 (19%)]\tTrain Loss: 0.000114\n",
      "Train Epoch: 114 [30/106 (28%)]\tTrain Loss: 0.107399\n",
      "Train Epoch: 114 [40/106 (38%)]\tTrain Loss: 0.000166\n",
      "Train Epoch: 114 [50/106 (47%)]\tTrain Loss: 0.000011\n",
      "Train Epoch: 114 [60/106 (57%)]\tTrain Loss: 0.028292\n",
      "Train Epoch: 114 [70/106 (66%)]\tTrain Loss: 0.000009\n",
      "Train Epoch: 114 [80/106 (75%)]\tTrain Loss: 0.000139\n",
      "Train Epoch: 114 [90/106 (85%)]\tTrain Loss: 0.000358\n",
      "Train Epoch: 114 [100/106 (94%)]\tTrain Loss: 0.000229\n",
      "\n",
      "Train set: Average loss: 0.0003, Accuracy: 419/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.07385695e-07 9.00357372e-06 1.75553971e-08 3.48390444e-07\n",
      " 6.56346657e-08 8.28453821e-08 7.79659999e-08 4.75514298e-07\n",
      " 2.27689956e-08 6.66100448e-07 2.58261572e-08 1.57099308e-07\n",
      " 2.85073440e-08 6.57041781e-08 4.09686095e-07 9.66506342e-09\n",
      " 1.98971577e-08 1.20655875e-06 1.72338481e-08 9.99893546e-01\n",
      " 4.30103615e-02 1.57430855e-04 9.99077797e-01 9.99264181e-01\n",
      " 9.90108262e-08 9.99773920e-01 9.99904275e-01 1.21285495e-07\n",
      " 9.95949421e-08 1.63255592e-07 4.59165266e-03 4.03744571e-06\n",
      " 4.57508747e-07 2.03123225e-08 9.53857082e-09 8.67050076e-09\n",
      " 1.29488598e-08 7.22047162e-06 4.69082408e-08 2.76278413e-08\n",
      " 5.67081138e-08 1.61536438e-08 5.84735801e-07 1.83226987e-07\n",
      " 3.49954178e-07 1.36065779e-07 1.11582699e-06 2.16147891e-05\n",
      " 8.18287546e-04 8.05938441e-07 4.03296994e-03 8.27138269e-09\n",
      " 5.11017163e-07 2.31223947e-08 6.42127861e-06 4.40877308e-08\n",
      " 9.23973694e-03 1.01111599e-08 1.31002267e-07 3.21992120e-05\n",
      " 9.98674035e-01 1.08371314e-03 9.94186044e-01 4.28907573e-01\n",
      " 9.99870777e-01 2.32748789e-07 7.06255378e-06 9.99934196e-01\n",
      " 9.99802291e-01 9.99388337e-01 2.49591631e-06 1.05432482e-05\n",
      " 9.99819934e-01 9.99911189e-01 9.99519110e-01 9.99296784e-01\n",
      " 9.99943376e-01 9.99772727e-01 4.85745804e-05 4.45930724e-04\n",
      " 9.99215841e-01 1.31980296e-05 9.99985695e-01 9.98372495e-01\n",
      " 1.77376369e-05 2.33279949e-04 9.16397478e-03 1.03509465e-05\n",
      " 9.99365032e-01 4.98190275e-07 9.99902844e-01 1.16398546e-07\n",
      " 3.23044134e-07 9.99826849e-01 5.27830252e-07 4.79317998e-04\n",
      " 8.82062920e-08 3.94329369e-01 1.40260943e-06 5.26127260e-05\n",
      " 5.14995813e-01 9.99422789e-01 3.88091586e-08 2.36914858e-07\n",
      " 9.96067107e-01 7.99961072e-06 9.95846093e-01 4.73522199e-08\n",
      " 1.14350939e-07 6.14108694e-06 7.25819760e-08 1.84344870e-07\n",
      " 3.68823200e-01 7.84245969e-07 3.69463784e-08 3.35563044e-08]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 115 [0/106 (0%)]\tTrain Loss: 0.000031\n",
      "Train Epoch: 115 [10/106 (9%)]\tTrain Loss: 0.000016\n",
      "Train Epoch: 115 [20/106 (19%)]\tTrain Loss: 0.000140\n",
      "Train Epoch: 115 [30/106 (28%)]\tTrain Loss: 0.000085\n",
      "Train Epoch: 115 [40/106 (38%)]\tTrain Loss: 0.000149\n",
      "Train Epoch: 115 [50/106 (47%)]\tTrain Loss: 0.000008\n",
      "Train Epoch: 115 [60/106 (57%)]\tTrain Loss: 0.000185\n",
      "Train Epoch: 115 [70/106 (66%)]\tTrain Loss: 0.000307\n",
      "Train Epoch: 115 [80/106 (75%)]\tTrain Loss: 0.000245\n",
      "Train Epoch: 115 [90/106 (85%)]\tTrain Loss: 0.000659\n",
      "Train Epoch: 115 [100/106 (94%)]\tTrain Loss: 0.213042\n",
      "\n",
      "Train set: Average loss: 0.0002, Accuracy: 420/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [9.62123679e-08 3.06535924e-07 1.55570978e-09 1.00695353e-07\n",
      " 2.05909174e-08 6.69212810e-08 8.11915424e-09 1.55398382e-07\n",
      " 1.41387773e-08 2.52304517e-07 3.95789623e-09 5.38248948e-08\n",
      " 2.64705835e-09 7.41949835e-09 5.17341277e-07 2.32352515e-09\n",
      " 3.90459887e-09 4.03015719e-07 3.36895511e-09 9.93299723e-01\n",
      " 4.21137975e-06 5.02261400e-07 9.65297043e-01 9.99620080e-01\n",
      " 9.34655109e-09 9.99958277e-01 9.99983788e-01 1.19921310e-07\n",
      " 5.26117461e-08 4.03282101e-08 9.96354699e-01 2.10836333e-06\n",
      " 9.88558668e-07 3.02022296e-09 1.61526503e-09 1.84248228e-09\n",
      " 2.58198374e-09 6.22661105e-07 2.65350995e-08 8.96246988e-09\n",
      " 2.06966764e-08 4.43804726e-09 2.61883230e-07 3.13001145e-08\n",
      " 3.02897014e-08 3.18341620e-08 4.31262862e-07 2.09237278e-05\n",
      " 5.25584328e-04 1.45705229e-07 2.74933409e-04 1.45121815e-09\n",
      " 7.77469609e-08 4.38784875e-09 6.01559407e-07 6.13511553e-09\n",
      " 7.49269724e-02 1.59136648e-09 2.14391633e-07 5.24680502e-07\n",
      " 9.17966485e-01 5.92626020e-05 9.98440325e-01 9.89755094e-01\n",
      " 9.93330538e-01 9.18811054e-08 4.56235284e-06 9.99994636e-01\n",
      " 9.99964356e-01 6.10309303e-01 2.74610443e-07 1.23340465e-06\n",
      " 9.99225855e-01 9.99945402e-01 9.99456108e-01 9.99441564e-01\n",
      " 9.99993205e-01 9.99966264e-01 6.45729721e-01 3.59675440e-04\n",
      " 8.39847140e-03 3.50234586e-05 9.99998450e-01 9.99621511e-01\n",
      " 3.60892009e-05 3.57427460e-04 8.38490605e-01 8.79460276e-05\n",
      " 9.50202942e-01 6.41340563e-08 9.99978662e-01 2.22639631e-08\n",
      " 1.27963702e-07 9.88113821e-01 9.81249570e-08 1.44581281e-06\n",
      " 3.42340734e-08 3.42492074e-01 1.20417917e-06 3.40318024e-07\n",
      " 9.81291294e-01 9.99855399e-01 8.92909480e-09 2.53676298e-08\n",
      " 1.25381484e-05 3.15928787e-07 4.76629753e-03 5.26147304e-09\n",
      " 5.74025938e-09 1.77613586e-07 4.56267806e-08 3.50306060e-08\n",
      " 5.13206896e-06 1.91106189e-08 2.86956610e-08 3.26896910e-08]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 116 [0/106 (0%)]\tTrain Loss: 0.000023\n",
      "Train Epoch: 116 [10/106 (9%)]\tTrain Loss: 0.001477\n",
      "Train Epoch: 116 [20/106 (19%)]\tTrain Loss: 0.000177\n",
      "Train Epoch: 116 [30/106 (28%)]\tTrain Loss: 0.000348\n",
      "Train Epoch: 116 [40/106 (38%)]\tTrain Loss: 0.062475\n",
      "Train Epoch: 116 [50/106 (47%)]\tTrain Loss: 0.000011\n",
      "Train Epoch: 116 [60/106 (57%)]\tTrain Loss: 0.000020\n",
      "Train Epoch: 116 [70/106 (66%)]\tTrain Loss: 0.000542\n",
      "Train Epoch: 116 [80/106 (75%)]\tTrain Loss: 0.097336\n",
      "Train Epoch: 116 [90/106 (85%)]\tTrain Loss: 0.000055\n",
      "Train Epoch: 116 [100/106 (94%)]\tTrain Loss: 0.000764\n",
      "\n",
      "Train set: Average loss: 0.0011, Accuracy: 410/424 (97%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [9.48060770e-05 2.93425721e-04 9.69041139e-06 1.85989738e-05\n",
      " 3.76212993e-05 1.53871079e-04 3.92506226e-05 5.55338702e-05\n",
      " 7.69176495e-06 1.78399659e-05 9.94973379e-06 5.46220826e-06\n",
      " 2.24882806e-05 2.42428378e-05 2.04441807e-04 1.06105872e-05\n",
      " 1.03528211e-04 7.53811473e-05 2.97290186e-04 9.99969482e-01\n",
      " 9.84864175e-01 8.32983851e-01 9.97625411e-01 9.97091174e-01\n",
      " 1.49132265e-03 9.98373628e-01 9.99780834e-01 1.55518755e-05\n",
      " 9.30388633e-05 7.28979721e-05 7.96517343e-05 8.30953941e-04\n",
      " 5.99468331e-05 2.36867982e-05 1.00373472e-05 9.42268434e-06\n",
      " 1.14909044e-05 1.04824678e-04 5.45712683e-05 5.71523742e-05\n",
      " 1.37861382e-04 2.35856587e-05 5.90912168e-05 2.39614110e-05\n",
      " 1.04670671e-05 1.65085967e-05 1.07267704e-04 5.31079480e-04\n",
      " 1.11590698e-03 8.10596521e-06 1.33070862e-04 6.49802314e-06\n",
      " 2.11155439e-05 5.53101836e-06 4.77345311e-04 6.44925149e-06\n",
      " 4.73480731e-01 5.72094314e-06 1.46958937e-05 7.61097981e-05\n",
      " 7.47609794e-01 6.08227961e-02 8.45808446e-01 6.01407290e-01\n",
      " 9.91782427e-01 1.41446828e-04 1.50639491e-04 9.99681830e-01\n",
      " 9.94459033e-01 9.94823098e-01 1.48240055e-04 9.61564656e-04\n",
      " 5.39527601e-03 9.88900781e-01 1.00662687e-03 4.50691761e-04\n",
      " 9.99330640e-01 9.98307586e-01 6.99789554e-04 2.95151118e-03\n",
      " 5.32102525e-01 2.34690102e-04 9.98850584e-01 9.96644855e-01\n",
      " 9.70718801e-01 1.11379521e-02 5.08978143e-02 1.80885894e-03\n",
      " 9.82930601e-01 5.74409962e-03 9.98373151e-01 1.47421306e-05\n",
      " 2.86104478e-05 9.95712399e-01 1.07025953e-04 1.98896552e-04\n",
      " 1.70584226e-05 9.95094895e-01 1.04205264e-02 2.72590101e-01\n",
      " 9.96608496e-01 9.96696949e-01 1.19134679e-03 4.29304928e-05\n",
      " 3.15108290e-03 1.32417219e-04 1.24111155e-03 1.98415061e-03\n",
      " 8.31368088e-04 1.61234720e-03 8.06228563e-05 5.26800468e-05\n",
      " 5.86266746e-04 1.97974592e-03 1.34546470e-04 3.77043674e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 117 [0/106 (0%)]\tTrain Loss: 0.000140\n",
      "Train Epoch: 117 [10/106 (9%)]\tTrain Loss: 0.000128\n",
      "Train Epoch: 117 [20/106 (19%)]\tTrain Loss: 0.000031\n",
      "Train Epoch: 117 [30/106 (28%)]\tTrain Loss: 0.000079\n",
      "Train Epoch: 117 [40/106 (38%)]\tTrain Loss: 0.000006\n",
      "Train Epoch: 117 [50/106 (47%)]\tTrain Loss: 0.000095\n",
      "Train Epoch: 117 [60/106 (57%)]\tTrain Loss: 0.000656\n",
      "Train Epoch: 117 [70/106 (66%)]\tTrain Loss: 0.009036\n",
      "Train Epoch: 117 [80/106 (75%)]\tTrain Loss: 0.000126\n",
      "Train Epoch: 117 [90/106 (85%)]\tTrain Loss: 0.000517\n",
      "Train Epoch: 117 [100/106 (94%)]\tTrain Loss: 0.000138\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 414/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.31319743e-04 1.43852003e-03 4.81305392e-07 1.67727849e-05\n",
      " 4.60139017e-06 2.98867308e-05 2.02183255e-05 1.29079754e-02\n",
      " 2.08266897e-07 1.50829474e-05 4.16792818e-06 8.34591503e-07\n",
      " 3.15192810e-05 5.70788598e-06 4.61244286e-04 1.56676344e-06\n",
      " 2.95809034e-04 3.35346581e-03 1.65598758e-04 9.99911547e-01\n",
      " 1.54452488e-01 9.96768355e-01 9.86836314e-01 9.99337256e-01\n",
      " 4.50927269e-04 9.99939561e-01 9.99988914e-01 4.45596561e-06\n",
      " 2.33960491e-05 1.02242113e-04 9.96062100e-01 1.63121924e-01\n",
      " 6.02287582e-05 2.48552897e-06 3.52973473e-07 5.00357658e-07\n",
      " 6.40478106e-07 7.58871669e-03 1.34892243e-05 1.03346002e-03\n",
      " 9.92989063e-01 5.25925097e-05 1.67648669e-03 5.48623348e-06\n",
      " 1.03662899e-06 2.72108264e-06 2.09747654e-04 1.02292463e-01\n",
      " 8.54488015e-01 2.05348124e-06 8.81912589e-01 2.41939205e-07\n",
      " 1.63422089e-06 4.28374676e-07 1.39344245e-01 5.57400313e-07\n",
      " 9.99439776e-01 1.40545342e-06 9.43683062e-06 2.34120744e-05\n",
      " 9.99657989e-01 9.71485257e-01 9.99923110e-01 9.99917984e-01\n",
      " 9.99791682e-01 9.98344541e-01 5.65022826e-01 9.99998093e-01\n",
      " 9.98872578e-01 9.99992609e-01 3.36244293e-02 2.39674537e-03\n",
      " 9.99428809e-01 9.99862552e-01 5.27041614e-01 3.53592336e-01\n",
      " 9.99934793e-01 9.99935031e-01 9.99516010e-01 9.99926209e-01\n",
      " 9.99908090e-01 9.08041775e-01 9.99994040e-01 9.99952555e-01\n",
      " 9.99552429e-01 9.99221087e-01 8.31674695e-01 4.69641201e-02\n",
      " 9.93221164e-01 5.80474809e-02 9.99216080e-01 4.48337145e-04\n",
      " 3.50293012e-05 9.91172731e-01 5.19924106e-05 3.30689436e-05\n",
      " 8.72818703e-07 9.98594344e-01 6.74629337e-05 9.90429640e-01\n",
      " 9.99217153e-01 9.99998331e-01 2.61004807e-05 3.31282383e-04\n",
      " 8.73416662e-01 2.25582538e-04 5.74217319e-01 9.97518659e-01\n",
      " 8.24734016e-05 2.72010621e-02 1.96741343e-01 6.85336854e-05\n",
      " 5.26101561e-04 7.44435412e-04 6.77486751e-05 9.99357164e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "Train Epoch: 118 [0/106 (0%)]\tTrain Loss: 0.000777\n",
      "Train Epoch: 118 [10/106 (9%)]\tTrain Loss: 0.000011\n",
      "Train Epoch: 118 [20/106 (19%)]\tTrain Loss: 0.118848\n",
      "Train Epoch: 118 [30/106 (28%)]\tTrain Loss: 0.000135\n",
      "Train Epoch: 118 [40/106 (38%)]\tTrain Loss: 0.000739\n",
      "Train Epoch: 118 [50/106 (47%)]\tTrain Loss: 0.000092\n",
      "Train Epoch: 118 [60/106 (57%)]\tTrain Loss: 0.000107\n",
      "Train Epoch: 118 [70/106 (66%)]\tTrain Loss: 0.000230\n",
      "Train Epoch: 118 [80/106 (75%)]\tTrain Loss: 0.000052\n",
      "Train Epoch: 118 [90/106 (85%)]\tTrain Loss: 0.000073\n",
      "Train Epoch: 118 [100/106 (94%)]\tTrain Loss: 0.000315\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 417/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.89452685e-08 1.38348769e-02 2.30448158e-08 6.16265510e-08\n",
      " 8.15152301e-09 1.03385833e-09 1.62051292e-05 4.35871250e-09\n",
      " 1.72689724e-10 1.31483773e-07 3.05576755e-08 1.60969205e-09\n",
      " 1.38591167e-06 9.69177876e-08 1.40647239e-06 1.00649622e-08\n",
      " 4.96068643e-03 2.59502740e-05 1.93828562e-08 9.99981046e-01\n",
      " 9.98616457e-01 2.51019951e-02 9.95722294e-01 9.99520421e-01\n",
      " 1.09239792e-07 9.99946237e-01 9.99986768e-01 2.06138523e-07\n",
      " 6.72455644e-04 4.10573375e-06 9.64366436e-01 9.63174989e-06\n",
      " 3.48300091e-07 1.38446437e-08 8.20348900e-10 2.22627605e-09\n",
      " 2.77393442e-09 4.15642338e-04 1.79281483e-06 1.45613958e-04\n",
      " 3.08323264e-01 1.12701414e-07 5.27917337e-07 2.13634688e-09\n",
      " 3.55983287e-09 2.12612843e-08 4.12802422e-07 1.31213330e-02\n",
      " 1.35901831e-02 1.13183258e-08 3.76863126e-03 2.22065588e-10\n",
      " 4.60869704e-10 4.63353494e-10 4.32329535e-01 5.60363000e-09\n",
      " 1.10494155e-04 5.43829426e-09 1.50189261e-09 2.47744003e-09\n",
      " 9.79433954e-01 8.22715275e-03 9.99437392e-01 9.99670029e-01\n",
      " 7.80806765e-02 9.76374736e-07 9.14927625e-07 9.99905348e-01\n",
      " 9.97416615e-01 9.99891043e-01 2.73149414e-03 2.60346278e-05\n",
      " 1.11555129e-01 9.99592125e-01 5.57644351e-04 1.52155859e-04\n",
      " 9.99993801e-01 9.99979258e-01 9.31089889e-05 9.90235746e-01\n",
      " 9.99907136e-01 6.23430533e-04 9.99907613e-01 9.99944806e-01\n",
      " 9.99877334e-01 1.28898211e-02 6.31220173e-05 1.69595733e-05\n",
      " 9.99636054e-01 9.91299450e-01 9.99987841e-01 3.30308831e-06\n",
      " 4.03463680e-08 9.99543190e-01 1.72359738e-02 6.71149473e-06\n",
      " 1.33586708e-09 9.98868644e-01 8.86820138e-01 9.99914408e-01\n",
      " 9.99813616e-01 9.98806238e-01 2.31651565e-09 4.69028018e-05\n",
      " 9.99460399e-01 9.15595319e-06 9.93497252e-01 5.38388342e-02\n",
      " 4.98151337e-07 3.14790606e-01 4.41742624e-04 2.19565211e-03\n",
      " 1.50352344e-01 6.01382077e-01 1.39796748e-06 9.99845505e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "Train Epoch: 119 [0/106 (0%)]\tTrain Loss: 0.011735\n",
      "Train Epoch: 119 [10/106 (9%)]\tTrain Loss: 0.000079\n",
      "Train Epoch: 119 [20/106 (19%)]\tTrain Loss: 0.001728\n",
      "Train Epoch: 119 [30/106 (28%)]\tTrain Loss: 0.000006\n",
      "Train Epoch: 119 [40/106 (38%)]\tTrain Loss: 0.061742\n",
      "Train Epoch: 119 [50/106 (47%)]\tTrain Loss: 0.000014\n",
      "Train Epoch: 119 [60/106 (57%)]\tTrain Loss: 0.000140\n",
      "Train Epoch: 119 [70/106 (66%)]\tTrain Loss: 0.000502\n",
      "Train Epoch: 119 [80/106 (75%)]\tTrain Loss: 0.000056\n",
      "Train Epoch: 119 [90/106 (85%)]\tTrain Loss: 0.000098\n",
      "Train Epoch: 119 [100/106 (94%)]\tTrain Loss: 0.012193\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 414/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.30672856e-06 4.35942929e-05 4.03435280e-07 9.94265633e-07\n",
      " 3.77271846e-07 1.74594629e-06 2.52242467e-06 7.29115811e-07\n",
      " 9.34320425e-08 1.24893768e-06 4.81784355e-07 1.60031988e-07\n",
      " 4.66222191e-05 8.86589760e-07 1.45517777e-06 4.34525909e-08\n",
      " 1.76829496e-06 1.49042222e-07 1.44453247e-06 9.99970555e-01\n",
      " 9.99888301e-01 9.98664021e-01 4.62717609e-04 9.99761879e-01\n",
      " 5.94406592e-05 2.45758425e-02 9.99996066e-01 8.41009751e-07\n",
      " 1.15700254e-06 4.85902149e-07 9.10365547e-04 3.14350359e-07\n",
      " 7.83012274e-06 1.65888324e-07 3.92411721e-08 1.08489580e-07\n",
      " 2.23822497e-07 9.91413474e-01 1.78353730e-05 4.66423947e-03\n",
      " 9.98483121e-01 6.51634872e-01 2.07894584e-04 6.87807301e-07\n",
      " 5.53408711e-07 1.56627181e-07 1.73119645e-06 2.13724474e-04\n",
      " 3.48057151e-02 1.78646576e-06 2.15526496e-04 3.63380082e-08\n",
      " 8.21096364e-08 4.36952767e-08 9.94505823e-01 1.19818083e-07\n",
      " 8.66029970e-03 7.36420631e-08 5.14116266e-07 5.60697856e-07\n",
      " 9.82833505e-01 7.53747416e-04 9.99357998e-01 9.98535752e-01\n",
      " 9.22305048e-01 4.96533685e-05 2.52173952e-04 9.97191727e-01\n",
      " 9.99057472e-01 9.99788940e-01 2.53663020e-04 6.72290055e-03\n",
      " 8.23706269e-01 9.99890447e-01 4.15255345e-04 9.88827669e-04\n",
      " 9.99839187e-01 9.99649525e-01 8.15427193e-05 9.43224430e-01\n",
      " 8.72872353e-01 3.08137896e-06 9.99387860e-01 9.99821961e-01\n",
      " 9.99468029e-01 2.91162985e-03 2.63719312e-06 1.97057670e-06\n",
      " 9.81961787e-01 8.06946628e-06 9.99993801e-01 4.10026696e-05\n",
      " 1.37259713e-05 9.99437630e-01 1.93754840e-06 7.56267505e-03\n",
      " 4.32073008e-08 9.99037266e-01 2.55205613e-02 9.99927759e-01\n",
      " 9.99887347e-01 9.99792755e-01 7.49161131e-07 4.53181092e-05\n",
      " 9.39913630e-01 1.19074400e-06 4.49377626e-01 9.99020934e-01\n",
      " 1.00938587e-06 3.56815313e-03 4.55273122e-01 8.56541155e-04\n",
      " 1.02476124e-03 1.05139598e-05 4.58719933e-06 9.99851227e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 120 [0/106 (0%)]\tTrain Loss: 0.000975\n",
      "Train Epoch: 120 [10/106 (9%)]\tTrain Loss: 0.006783\n",
      "Train Epoch: 120 [20/106 (19%)]\tTrain Loss: 0.000490\n",
      "Train Epoch: 120 [30/106 (28%)]\tTrain Loss: 0.000199\n",
      "Train Epoch: 120 [40/106 (38%)]\tTrain Loss: 0.000076\n",
      "Train Epoch: 120 [50/106 (47%)]\tTrain Loss: 0.000011\n",
      "Train Epoch: 120 [60/106 (57%)]\tTrain Loss: 0.000391\n",
      "Train Epoch: 120 [70/106 (66%)]\tTrain Loss: 0.000045\n",
      "Train Epoch: 120 [80/106 (75%)]\tTrain Loss: 0.000110\n",
      "Train Epoch: 120 [90/106 (85%)]\tTrain Loss: 0.000052\n",
      "Train Epoch: 120 [100/106 (94%)]\tTrain Loss: 0.001839\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 420/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.47836113e-07 6.49766343e-06 7.91530042e-08 5.97644600e-08\n",
      " 4.06838332e-08 1.12327143e-07 2.01333296e-06 3.28500747e-08\n",
      " 3.08429016e-08 1.92393983e-07 2.39833486e-08 2.80000414e-08\n",
      " 3.99809572e-08 4.94653776e-08 1.90028885e-07 9.77824577e-09\n",
      " 6.36662776e-08 1.11436002e-07 9.94089305e-07 9.99924302e-01\n",
      " 9.99921322e-01 5.05816698e-01 9.97703016e-01 9.99966741e-01\n",
      " 2.43777777e-06 9.99755204e-01 9.99999523e-01 9.22382299e-07\n",
      " 1.48028832e-07 1.08537812e-07 6.80462290e-06 1.29321307e-07\n",
      " 1.73951804e-07 2.78893886e-08 1.01613109e-08 2.63199631e-08\n",
      " 5.24070352e-08 3.11008819e-07 2.55242810e-07 2.13222066e-07\n",
      " 1.26495687e-02 8.80805331e-08 3.81155161e-07 2.50874912e-08\n",
      " 2.64188440e-08 2.53569876e-08 1.57002560e-07 8.62354796e-07\n",
      " 5.94498415e-05 1.28165493e-06 9.25977361e-07 7.83475862e-09\n",
      " 2.03403143e-08 1.20887407e-08 8.31030831e-02 1.65678458e-08\n",
      " 9.98395383e-01 1.57277338e-08 1.40365430e-07 4.73546606e-08\n",
      " 2.30749743e-03 1.95628081e-06 9.99603212e-01 9.99254048e-01\n",
      " 9.40974295e-01 1.08180046e-07 7.03488581e-07 9.99613345e-01\n",
      " 9.83285129e-01 9.99818265e-01 1.63416658e-07 1.70467914e-07\n",
      " 1.68949864e-05 9.99827087e-01 9.50404108e-01 3.71489733e-07\n",
      " 9.99762475e-01 9.99781907e-01 7.03096839e-06 2.37889617e-05\n",
      " 6.25589601e-05 1.17379621e-07 9.99728501e-01 9.99719203e-01\n",
      " 9.97891843e-01 1.89106780e-04 4.78149334e-04 5.19995519e-05\n",
      " 5.91795519e-02 1.74877627e-07 9.99926925e-01 3.24371534e-08\n",
      " 7.08589809e-08 9.99796450e-01 2.21010751e-07 5.53170707e-07\n",
      " 1.54561910e-08 9.97708082e-01 5.91718072e-06 1.33812428e-04\n",
      " 9.99711931e-01 9.96007562e-01 6.68615669e-08 9.89311602e-08\n",
      " 4.35919901e-06 9.49616847e-08 6.44045353e-01 9.79000390e-01\n",
      " 1.76610286e-07 1.39979511e-06 4.07700099e-05 1.02776476e-05\n",
      " 8.41918165e-07 6.55243639e-06 4.22693660e-07 9.99881506e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 22 TN= 54 FN= 34 FP= 6\n",
      "TP+FP 28\n",
      "precision 0.7857142857142857\n",
      "recall 0.39285714285714285\n",
      "F1 0.5238095238095237\n",
      "acc 0.6551724137931034\n",
      "AUCp 0.6464285714285715\n",
      "AUC 0.8083333333333333\n",
      "\n",
      " The epoch is 120, average recall: 0.3929, average precision: 0.7857,average F1: 0.5238, average accuracy: 0.6552, average AUC: 0.8083\n",
      "Train Epoch: 121 [0/106 (0%)]\tTrain Loss: 0.000239\n",
      "Train Epoch: 121 [10/106 (9%)]\tTrain Loss: 0.008744\n",
      "Train Epoch: 121 [20/106 (19%)]\tTrain Loss: 0.000187\n",
      "Train Epoch: 121 [30/106 (28%)]\tTrain Loss: 0.008980\n",
      "Train Epoch: 121 [40/106 (38%)]\tTrain Loss: 0.000055\n",
      "Train Epoch: 121 [50/106 (47%)]\tTrain Loss: 0.000032\n",
      "Train Epoch: 121 [60/106 (57%)]\tTrain Loss: 0.002817\n",
      "Train Epoch: 121 [70/106 (66%)]\tTrain Loss: 0.000603\n",
      "Train Epoch: 121 [80/106 (75%)]\tTrain Loss: 0.001568\n",
      "Train Epoch: 121 [90/106 (85%)]\tTrain Loss: 0.002131\n",
      "Train Epoch: 121 [100/106 (94%)]\tTrain Loss: 0.023157\n",
      "\n",
      "Train set: Average loss: 0.0003, Accuracy: 414/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [5.79732470e-04 8.95596095e-05 1.15380578e-06 8.17689670e-06\n",
      " 2.40919053e-06 1.20608147e-05 5.24203606e-05 3.71826854e-06\n",
      " 5.74123590e-07 5.10235850e-06 2.51565671e-05 7.62847890e-07\n",
      " 2.01385701e-03 2.11027054e-06 1.08803924e-05 1.54583205e-07\n",
      " 3.10374744e-05 1.07599213e-03 1.76575377e-05 9.96229470e-01\n",
      " 9.96675491e-01 4.87145808e-05 8.21221948e-01 9.93438065e-01\n",
      " 8.15910698e-06 9.70579743e-01 9.98445094e-01 3.68085580e-06\n",
      " 4.72744796e-06 1.34842849e-05 1.06182844e-04 1.30341665e-04\n",
      " 1.04510179e-02 1.53128985e-06 4.26818559e-07 2.87280898e-07\n",
      " 6.28445434e-07 9.79184270e-01 2.84572374e-02 4.77625392e-02\n",
      " 9.93743002e-01 6.32478259e-05 9.58440651e-04 1.78516748e-05\n",
      " 3.56397948e-07 2.45169622e-06 1.44603346e-05 3.08464140e-01\n",
      " 5.18850565e-01 8.74748002e-05 2.20912118e-02 1.43488279e-07\n",
      " 7.11510950e-07 2.06601612e-07 8.29018772e-01 5.38893971e-07\n",
      " 9.55309033e-01 2.20473737e-07 7.76609249e-06 1.69269413e-06\n",
      " 9.58019137e-01 8.64030182e-01 9.96853530e-01 9.95902598e-01\n",
      " 4.03394580e-01 9.28468828e-04 3.31087820e-02 9.98417974e-01\n",
      " 9.93018687e-01 9.98138785e-01 4.25166130e-01 5.05384505e-01\n",
      " 9.74116802e-01 9.96801257e-01 2.79080775e-02 6.84636689e-05\n",
      " 9.96596992e-01 9.95292306e-01 5.94062498e-04 1.24186312e-03\n",
      " 8.23120177e-01 5.07761506e-05 9.96046484e-01 9.98016715e-01\n",
      " 9.91874039e-01 9.45810080e-01 1.55770991e-04 2.62643676e-04\n",
      " 1.04581367e-03 1.02619288e-06 9.84503269e-01 5.01971021e-02\n",
      " 1.01781048e-01 9.81664538e-01 9.58631572e-05 1.22254396e-05\n",
      " 2.67637859e-07 9.48519170e-01 7.77824521e-01 9.68103707e-01\n",
      " 9.97825801e-01 9.81335163e-01 6.69063229e-05 8.34577009e-02\n",
      " 9.51162219e-01 6.91606037e-05 9.94591415e-01 9.94119644e-01\n",
      " 1.51105469e-05 8.90404463e-01 9.76226747e-01 9.57526684e-01\n",
      " 2.14274079e-01 9.81475353e-01 6.75575720e-05 9.93992388e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1.\n",
      " 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1.]\n",
      "Train Epoch: 122 [0/106 (0%)]\tTrain Loss: 0.029158\n",
      "Train Epoch: 122 [10/106 (9%)]\tTrain Loss: 0.000101\n",
      "Train Epoch: 122 [20/106 (19%)]\tTrain Loss: 0.006967\n",
      "Train Epoch: 122 [30/106 (28%)]\tTrain Loss: 0.000312\n",
      "Train Epoch: 122 [40/106 (38%)]\tTrain Loss: 0.000056\n",
      "Train Epoch: 122 [50/106 (47%)]\tTrain Loss: 0.000010\n",
      "Train Epoch: 122 [60/106 (57%)]\tTrain Loss: 0.000010\n",
      "Train Epoch: 122 [70/106 (66%)]\tTrain Loss: 0.000289\n",
      "Train Epoch: 122 [80/106 (75%)]\tTrain Loss: 0.000007\n",
      "Train Epoch: 122 [90/106 (85%)]\tTrain Loss: 0.950146\n",
      "Train Epoch: 122 [100/106 (94%)]\tTrain Loss: 0.007923\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 414/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [6.85130374e-07 5.85984671e-05 8.47086255e-07 5.12080646e-07\n",
      " 3.82744417e-07 4.82837152e-07 6.78754150e-05 4.50636549e-07\n",
      " 2.40235977e-07 8.03945781e-08 1.85665826e-07 3.06564516e-08\n",
      " 1.60987611e-06 1.14784200e-06 3.63246068e-06 1.79510408e-07\n",
      " 1.16575902e-04 7.93718982e-07 9.74421368e-07 9.99262989e-01\n",
      " 9.97464538e-01 9.96206999e-01 9.96647179e-01 9.98176336e-01\n",
      " 1.06615081e-01 9.98760700e-01 9.99372542e-01 4.75916249e-06\n",
      " 3.62382053e-07 1.12080545e-06 4.06727740e-05 2.77529725e-05\n",
      " 2.98406985e-06 2.15834962e-06 2.11297404e-07 1.12689811e-07\n",
      " 2.05259099e-07 1.44474725e-05 1.26074417e-06 1.26940140e-05\n",
      " 5.17990232e-01 1.16392926e-06 2.38894950e-06 5.24053166e-06\n",
      " 6.37041808e-07 2.05239132e-07 4.49261006e-06 3.87190667e-05\n",
      " 4.88294572e-05 4.14752577e-07 1.37080233e-05 6.17178344e-08\n",
      " 5.46505078e-07 6.90029367e-08 7.04432478e-06 1.33097856e-07\n",
      " 9.96074557e-01 7.16438038e-08 3.22863031e-07 1.54662530e-06\n",
      " 7.95980523e-05 2.02680949e-05 9.82621551e-01 9.76302385e-01\n",
      " 9.77613032e-01 6.68465418e-06 1.50940277e-05 9.92998481e-01\n",
      " 9.46067989e-01 9.98089015e-01 3.24978942e-06 1.31235774e-05\n",
      " 4.84887557e-03 9.98745322e-01 9.95357454e-01 4.84517986e-05\n",
      " 9.99200046e-01 9.99288678e-01 9.87042069e-01 3.25633504e-04\n",
      " 5.88521600e-01 8.27278200e-05 9.96895671e-01 9.98635590e-01\n",
      " 3.98218960e-01 8.32038582e-04 4.24541475e-04 4.52119304e-04\n",
      " 9.97869611e-01 1.36318131e-05 9.97744441e-01 3.17477884e-07\n",
      " 8.27323902e-07 9.96055961e-01 7.50164509e-06 8.09439189e-06\n",
      " 1.02324591e-07 1.26088634e-02 1.77137600e-03 9.93264675e-01\n",
      " 9.94425654e-01 9.22919691e-01 4.87750867e-07 3.59278129e-05\n",
      " 1.00626703e-02 7.64290689e-06 7.68150270e-01 9.89495337e-01\n",
      " 7.16683985e-07 6.34412863e-04 2.22779978e-02 5.24345596e-05\n",
      " 1.00742531e-04 7.22300410e-01 1.48820300e-05 9.51811492e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 123 [0/106 (0%)]\tTrain Loss: 0.001347\n",
      "Train Epoch: 123 [10/106 (9%)]\tTrain Loss: 0.000572\n",
      "Train Epoch: 123 [20/106 (19%)]\tTrain Loss: 0.008368\n",
      "Train Epoch: 123 [30/106 (28%)]\tTrain Loss: 0.000014\n",
      "Train Epoch: 123 [40/106 (38%)]\tTrain Loss: 0.000134\n",
      "Train Epoch: 123 [50/106 (47%)]\tTrain Loss: 0.000015\n",
      "Train Epoch: 123 [60/106 (57%)]\tTrain Loss: 0.000181\n",
      "Train Epoch: 123 [70/106 (66%)]\tTrain Loss: 0.000086\n",
      "Train Epoch: 123 [80/106 (75%)]\tTrain Loss: 0.000217\n",
      "Train Epoch: 123 [90/106 (85%)]\tTrain Loss: 0.007826\n",
      "Train Epoch: 123 [100/106 (94%)]\tTrain Loss: 0.000026\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 418/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [5.05195521e-06 1.91288607e-04 5.88430112e-06 4.60383944e-06\n",
      " 6.79729374e-06 3.40036831e-06 8.13626757e-05 3.46261027e-06\n",
      " 1.29792329e-06 1.68151996e-06 2.40070472e-06 3.83062485e-07\n",
      " 3.30813673e-05 3.28054807e-06 2.04664830e-05 1.02064712e-06\n",
      " 7.23479679e-05 6.09773269e-06 6.69923020e-06 9.99852777e-01\n",
      " 9.99008894e-01 4.41189902e-03 9.99163866e-01 9.99943972e-01\n",
      " 3.18299208e-05 9.99876976e-01 9.99948263e-01 1.25519537e-05\n",
      " 2.81927510e-06 6.90745674e-06 1.98366961e-05 4.44892758e-05\n",
      " 1.52108723e-05 1.92619223e-06 3.64277753e-07 1.06408697e-06\n",
      " 4.98703503e-06 2.27713259e-04 3.58866282e-06 3.49527681e-06\n",
      " 2.14924023e-01 1.37263953e-06 2.51125348e-05 3.59774203e-05\n",
      " 2.10488342e-05 3.31521937e-06 3.18334387e-05 1.32891568e-04\n",
      " 9.84864164e-05 1.53666747e-06 3.31018127e-05 7.05555294e-07\n",
      " 3.46447609e-06 1.16899093e-06 2.52001755e-05 1.52903090e-06\n",
      " 9.96414006e-01 5.88940452e-07 4.50056905e-06 9.22720028e-06\n",
      " 1.10159817e-04 3.28372080e-05 7.40371943e-01 8.76397192e-02\n",
      " 5.78788579e-01 3.89776178e-05 1.06117463e-04 9.98955488e-01\n",
      " 9.95023847e-01 9.94853437e-01 1.04127485e-05 2.51081528e-05\n",
      " 2.48641329e-04 9.99039829e-01 9.97578681e-01 3.08379676e-05\n",
      " 9.98425722e-01 9.96211290e-01 4.70839615e-04 9.91472480e-05\n",
      " 3.55911791e-01 1.49955731e-05 9.98497128e-01 9.97764349e-01\n",
      " 3.71857643e-01 9.52126866e-05 9.62201520e-05 9.22683466e-05\n",
      " 9.96816814e-01 2.01487073e-05 9.99721110e-01 2.93939775e-06\n",
      " 1.96008859e-05 9.98998463e-01 4.49240943e-05 3.94863200e-05\n",
      " 7.63360276e-07 1.79819681e-03 2.54369644e-03 9.64375436e-01\n",
      " 9.73609984e-01 1.23851493e-01 8.77090088e-07 1.45689250e-04\n",
      " 6.04429506e-02 2.32768107e-05 9.55368817e-01 9.88729119e-01\n",
      " 1.47468177e-06 4.48176777e-03 6.06035022e-03 3.76286189e-05\n",
      " 1.88772072e-04 1.83463413e-02 1.54042536e-05 9.68220993e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 124 [0/106 (0%)]\tTrain Loss: 0.000007\n",
      "Train Epoch: 124 [10/106 (9%)]\tTrain Loss: 0.000234\n",
      "Train Epoch: 124 [20/106 (19%)]\tTrain Loss: 0.000255\n",
      "Train Epoch: 124 [30/106 (28%)]\tTrain Loss: 0.000515\n",
      "Train Epoch: 124 [40/106 (38%)]\tTrain Loss: 0.005324\n",
      "Train Epoch: 124 [50/106 (47%)]\tTrain Loss: 0.000120\n",
      "Train Epoch: 124 [60/106 (57%)]\tTrain Loss: 0.000127\n",
      "Train Epoch: 124 [70/106 (66%)]\tTrain Loss: 0.000147\n",
      "Train Epoch: 124 [80/106 (75%)]\tTrain Loss: 0.000025\n",
      "Train Epoch: 124 [90/106 (85%)]\tTrain Loss: 0.000144\n",
      "Train Epoch: 124 [100/106 (94%)]\tTrain Loss: 0.000267\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 424/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.91732170e-06 2.77663814e-04 2.60428988e-06 1.31567595e-06\n",
      " 1.91543018e-06 9.59447902e-07 4.46790691e-05 1.34933953e-06\n",
      " 8.47716592e-07 3.51791243e-07 8.57561986e-07 1.00233549e-07\n",
      " 6.06516915e-05 8.15648605e-07 1.47107576e-05 7.34522587e-07\n",
      " 1.63860887e-03 2.39112615e-06 9.13801705e-06 9.99982238e-01\n",
      " 9.99544322e-01 9.96380508e-01 9.99789655e-01 9.99968410e-01\n",
      " 2.59921163e-01 9.99954104e-01 9.99996066e-01 9.94027778e-06\n",
      " 2.22058543e-06 3.98584871e-06 1.11914142e-05 1.27771491e-05\n",
      " 5.13013129e-06 2.02019169e-06 1.85073603e-07 4.40808435e-07\n",
      " 2.03274772e-06 1.05219653e-04 2.02675460e-06 2.42288552e-05\n",
      " 9.94759381e-01 1.49456696e-06 1.57318609e-05 2.16254684e-05\n",
      " 5.99284340e-06 4.46766961e-07 1.01703563e-05 2.98865307e-05\n",
      " 9.83172285e-05 6.23018650e-07 1.78386890e-05 2.62692168e-07\n",
      " 7.91940579e-07 2.60904045e-07 4.68828584e-05 2.95086153e-07\n",
      " 9.99344885e-01 1.41928751e-07 4.04877568e-07 1.99073679e-06\n",
      " 6.24680426e-04 8.16006723e-05 9.98972416e-01 9.98030722e-01\n",
      " 8.79939437e-01 2.95684382e-04 8.76705453e-05 9.99723613e-01\n",
      " 9.99286711e-01 9.99790728e-01 2.20014426e-05 6.17109472e-05\n",
      " 1.22689739e-01 9.99792755e-01 9.90840137e-01 1.80675434e-05\n",
      " 9.99835253e-01 9.99660850e-01 1.77017020e-04 1.50766093e-04\n",
      " 1.18284940e-03 6.11228825e-06 9.99646902e-01 9.99739945e-01\n",
      " 9.77253258e-01 2.26872813e-04 1.35162772e-04 2.60695262e-04\n",
      " 9.99390602e-01 1.99200877e-05 9.99913454e-01 1.13804083e-06\n",
      " 8.38256437e-06 9.99811471e-01 3.11368458e-05 2.63037546e-05\n",
      " 1.84449505e-07 9.70906466e-02 4.64447252e-02 9.99652624e-01\n",
      " 9.98582125e-01 9.97157454e-01 9.91842171e-07 8.44217837e-03\n",
      " 9.96015608e-01 6.28185171e-06 9.84419167e-01 9.98949945e-01\n",
      " 8.17448017e-06 1.62052035e-01 9.57773030e-01 3.95282113e-05\n",
      " 1.02720410e-01 9.96168315e-01 8.91784566e-06 7.95372017e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 125 [0/106 (0%)]\tTrain Loss: 0.001288\n",
      "Train Epoch: 125 [10/106 (9%)]\tTrain Loss: 0.000741\n",
      "Train Epoch: 125 [20/106 (19%)]\tTrain Loss: 0.003046\n",
      "Train Epoch: 125 [30/106 (28%)]\tTrain Loss: 0.000007\n",
      "Train Epoch: 125 [40/106 (38%)]\tTrain Loss: 0.000023\n",
      "Train Epoch: 125 [50/106 (47%)]\tTrain Loss: 0.169734\n",
      "Train Epoch: 125 [60/106 (57%)]\tTrain Loss: 0.000029\n",
      "Train Epoch: 125 [70/106 (66%)]\tTrain Loss: 0.000067\n",
      "Train Epoch: 125 [80/106 (75%)]\tTrain Loss: 0.000406\n",
      "Train Epoch: 125 [90/106 (85%)]\tTrain Loss: 0.000288\n",
      "Train Epoch: 125 [100/106 (94%)]\tTrain Loss: 0.096915\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 421/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [5.91377921e-06 8.97555656e-05 2.70205010e-06 1.27194289e-06\n",
      " 3.43557872e-06 2.32722664e-06 1.83161301e-05 1.92304265e-06\n",
      " 8.85498082e-07 2.67512320e-07 3.45697060e-07 1.04265972e-07\n",
      " 7.36448965e-06 4.52408045e-07 5.67408506e-06 4.62866382e-07\n",
      " 9.42623956e-05 4.42611508e-06 7.48874982e-06 9.99999046e-01\n",
      " 9.99940515e-01 9.99316931e-01 9.99995232e-01 9.99997616e-01\n",
      " 8.98091555e-01 9.99999881e-01 9.99999762e-01 1.20746718e-05\n",
      " 1.69885732e-06 3.16789715e-06 7.71283012e-05 6.95457811e-06\n",
      " 7.16584009e-06 1.42085594e-06 1.51589816e-07 5.26056795e-07\n",
      " 2.51452275e-06 2.22293038e-05 2.26033922e-06 1.71604006e-05\n",
      " 9.98795509e-01 1.43901877e-06 1.28356114e-05 6.68114853e-06\n",
      " 3.99363989e-06 6.99820305e-07 1.19965061e-05 2.00192135e-05\n",
      " 8.99173101e-05 2.91147245e-07 1.69516024e-05 3.01677630e-07\n",
      " 6.21999959e-07 3.96107822e-07 1.38587930e-05 3.30169485e-07\n",
      " 9.99776542e-01 1.48237902e-07 4.18235146e-07 2.27911255e-06\n",
      " 2.75461443e-05 8.87223450e-06 9.98429596e-01 9.98810291e-01\n",
      " 9.99901056e-01 5.45039729e-05 2.19310168e-05 9.99982476e-01\n",
      " 9.99848962e-01 9.99975204e-01 1.03816828e-05 1.87946043e-05\n",
      " 1.38980395e-04 9.99966860e-01 9.99852061e-01 3.83825754e-05\n",
      " 9.99981046e-01 9.99963284e-01 8.60329270e-01 1.74144170e-05\n",
      " 4.16350958e-04 6.56698739e-06 9.99958754e-01 9.99984145e-01\n",
      " 9.99089122e-01 2.09109821e-05 2.66781444e-05 5.56232808e-05\n",
      " 9.99856472e-01 5.37019150e-06 9.99992728e-01 7.42987424e-07\n",
      " 2.66013353e-06 9.99965549e-01 1.03019738e-05 1.12488106e-05\n",
      " 1.89565327e-07 1.91476867e-02 9.44187559e-05 9.99716222e-01\n",
      " 9.99698877e-01 9.99675155e-01 1.17969751e-06 5.90421550e-05\n",
      " 1.48090243e-01 5.73199168e-06 9.27570283e-01 9.99720037e-01\n",
      " 1.16287474e-05 3.33448476e-03 8.20831478e-01 2.55173327e-05\n",
      " 3.84514824e-05 9.98529434e-01 6.02480713e-06 8.78520310e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 126 [0/106 (0%)]\tTrain Loss: 0.000416\n",
      "Train Epoch: 126 [10/106 (9%)]\tTrain Loss: 0.000025\n",
      "Train Epoch: 126 [20/106 (19%)]\tTrain Loss: 0.000015\n",
      "Train Epoch: 126 [30/106 (28%)]\tTrain Loss: 0.000031\n",
      "Train Epoch: 126 [40/106 (38%)]\tTrain Loss: 0.000010\n",
      "Train Epoch: 126 [50/106 (47%)]\tTrain Loss: 0.000039\n",
      "Train Epoch: 126 [60/106 (57%)]\tTrain Loss: 0.000081\n",
      "Train Epoch: 126 [70/106 (66%)]\tTrain Loss: 0.086728\n",
      "Train Epoch: 126 [80/106 (75%)]\tTrain Loss: 0.002267\n",
      "Train Epoch: 126 [90/106 (85%)]\tTrain Loss: 0.000616\n",
      "Train Epoch: 126 [100/106 (94%)]\tTrain Loss: 0.000174\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 419/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.63448351e-06 2.58631742e-04 2.56326598e-06 1.11426596e-06\n",
      " 1.61965022e-06 8.49184460e-07 4.51389315e-05 1.52778614e-06\n",
      " 8.73707336e-07 3.87055962e-07 6.22153038e-07 1.09470690e-07\n",
      " 5.85940506e-05 9.19420927e-07 9.39273650e-06 8.35292212e-07\n",
      " 3.81287886e-04 1.19118604e-05 4.92201143e-05 9.99994040e-01\n",
      " 9.99916196e-01 9.98999059e-01 9.99986410e-01 9.99984145e-01\n",
      " 9.88052130e-01 9.99999285e-01 9.99996781e-01 1.40506645e-05\n",
      " 5.55191400e-06 1.05431382e-05 1.16230908e-03 2.62204558e-05\n",
      " 1.29298569e-05 2.85765918e-06 2.83668726e-07 5.38784491e-07\n",
      " 2.12959617e-06 1.67984781e-05 3.34396873e-06 1.18914295e-05\n",
      " 9.96622562e-01 1.64553421e-06 1.30639764e-05 1.06714133e-05\n",
      " 6.63555602e-06 3.92759233e-07 7.74122509e-06 2.22890994e-05\n",
      " 9.53031486e-05 4.81667655e-07 2.11683455e-05 2.58048914e-07\n",
      " 8.45523743e-07 2.99065732e-07 3.32523347e-03 5.57231260e-07\n",
      " 9.99676347e-01 1.99192058e-07 4.14490813e-07 2.45413617e-06\n",
      " 9.58297096e-05 1.13039769e-05 9.97792482e-01 9.98993456e-01\n",
      " 9.99889374e-01 1.42553181e-05 1.87806672e-05 9.99921203e-01\n",
      " 9.98868883e-01 9.99948621e-01 1.32436297e-04 1.86692821e-04\n",
      " 7.35261679e-01 9.99906301e-01 9.99886274e-01 8.88110313e-04\n",
      " 9.99858260e-01 9.99607027e-01 9.99674916e-01 4.99527669e-05\n",
      " 9.75559294e-01 1.74839079e-05 9.99845147e-01 9.99951601e-01\n",
      " 9.99431551e-01 1.09407119e-04 1.72138694e-04 2.17002467e-04\n",
      " 9.99660969e-01 1.28872380e-05 9.99974132e-01 2.01600756e-06\n",
      " 3.89969227e-06 9.99852538e-01 1.22212896e-05 6.31923613e-05\n",
      " 1.78662120e-07 8.13274443e-01 9.06152725e-01 9.99653935e-01\n",
      " 9.99278009e-01 9.99734819e-01 1.41086537e-06 2.51470995e-03\n",
      " 9.99286711e-01 8.18513581e-06 9.98976946e-01 9.98386979e-01\n",
      " 2.48474680e-05 1.89652890e-01 5.56942582e-01 2.92022778e-05\n",
      " 1.23483199e-03 9.98631656e-01 1.01292753e-05 9.99903440e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
      "Train Epoch: 127 [0/106 (0%)]\tTrain Loss: 0.000160\n",
      "Train Epoch: 127 [10/106 (9%)]\tTrain Loss: 0.000112\n",
      "Train Epoch: 127 [20/106 (19%)]\tTrain Loss: 0.000056\n",
      "Train Epoch: 127 [30/106 (28%)]\tTrain Loss: 0.000020\n",
      "Train Epoch: 127 [40/106 (38%)]\tTrain Loss: 0.000031\n",
      "Train Epoch: 127 [50/106 (47%)]\tTrain Loss: 0.000159\n",
      "Train Epoch: 127 [60/106 (57%)]\tTrain Loss: 0.000042\n",
      "Train Epoch: 127 [70/106 (66%)]\tTrain Loss: 0.000046\n",
      "Train Epoch: 127 [80/106 (75%)]\tTrain Loss: 0.000242\n",
      "Train Epoch: 127 [90/106 (85%)]\tTrain Loss: 0.000129\n",
      "Train Epoch: 127 [100/106 (94%)]\tTrain Loss: 0.000133\n",
      "\n",
      "Train set: Average loss: 0.0002, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [9.28914744e-07 4.02118239e-05 6.05199546e-07 4.13867497e-07\n",
      " 5.26548206e-07 3.83972463e-07 6.93085531e-06 4.67805620e-07\n",
      " 2.27797315e-07 1.34179771e-07 1.50680336e-07 4.01743563e-08\n",
      " 4.51261121e-06 2.34712545e-07 1.81547773e-06 1.46410727e-07\n",
      " 1.66694863e-05 1.21559867e-06 2.00071554e-06 9.99981999e-01\n",
      " 9.99748170e-01 3.75471897e-02 9.99893069e-01 9.99938726e-01\n",
      " 6.24857566e-05 9.99994159e-01 9.99981880e-01 3.06245965e-06\n",
      " 7.13005988e-07 1.20721711e-06 7.40876785e-05 6.46997478e-06\n",
      " 3.03153320e-06 4.30747036e-07 9.11619082e-08 1.21842319e-07\n",
      " 3.78558155e-07 4.16808325e-06 5.18103604e-07 1.18660728e-06\n",
      " 4.35443550e-01 1.99852522e-07 3.63301524e-06 2.37812719e-06\n",
      " 1.24114160e-06 1.14787227e-07 2.83756572e-06 9.11321513e-06\n",
      " 3.44303771e-05 1.22721616e-07 6.40917733e-06 8.33606890e-08\n",
      " 1.90874559e-07 7.76792177e-08 8.21123376e-06 1.13356791e-07\n",
      " 9.99285519e-01 4.37773586e-08 1.93524983e-07 5.04460502e-07\n",
      " 1.95405828e-05 3.78448749e-06 9.84312236e-01 9.94628489e-01\n",
      " 9.99494433e-01 3.63073605e-06 5.44101658e-06 9.99789774e-01\n",
      " 9.94235218e-01 9.99823630e-01 2.83619374e-05 3.01814562e-05\n",
      " 1.69452932e-02 9.99782026e-01 9.99735773e-01 2.50257526e-05\n",
      " 9.99891162e-01 9.99767721e-01 4.53051813e-02 1.53922574e-05\n",
      " 1.14683798e-02 3.53641690e-06 9.99783337e-01 9.99896526e-01\n",
      " 9.84810531e-01 3.80207821e-05 1.50932792e-05 2.67701471e-05\n",
      " 9.99499679e-01 2.99982889e-06 9.99897480e-01 3.00466240e-07\n",
      " 9.61780302e-07 9.99602616e-01 3.50018036e-06 5.14389785e-06\n",
      " 5.10722842e-08 1.39080884e-03 8.60176151e-05 9.95717943e-01\n",
      " 9.98623490e-01 9.99383092e-01 2.09016633e-07 2.63122114e-04\n",
      " 9.87880528e-01 2.14815827e-06 9.63673472e-01 9.92542505e-01\n",
      " 1.65759741e-06 1.77590956e-03 4.26418288e-03 7.48111142e-06\n",
      " 2.31907052e-05 7.90484130e-01 2.41470912e-06 4.61280614e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 128 [0/106 (0%)]\tTrain Loss: 0.000074\n",
      "Train Epoch: 128 [10/106 (9%)]\tTrain Loss: 0.092246\n",
      "Train Epoch: 128 [20/106 (19%)]\tTrain Loss: 0.000143\n",
      "Train Epoch: 128 [30/106 (28%)]\tTrain Loss: 0.000057\n",
      "Train Epoch: 128 [40/106 (38%)]\tTrain Loss: 0.000031\n",
      "Train Epoch: 128 [50/106 (47%)]\tTrain Loss: 0.000006\n",
      "Train Epoch: 128 [60/106 (57%)]\tTrain Loss: 0.000111\n",
      "Train Epoch: 128 [70/106 (66%)]\tTrain Loss: 0.000032\n",
      "Train Epoch: 128 [80/106 (75%)]\tTrain Loss: 0.000174\n",
      "Train Epoch: 128 [90/106 (85%)]\tTrain Loss: 0.001283\n",
      "Train Epoch: 128 [100/106 (94%)]\tTrain Loss: 0.000017\n",
      "\n",
      "Train set: Average loss: 0.0020, Accuracy: 421/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [5.87846762e-06 1.60523006e-04 3.03288380e-06 1.37251652e-06\n",
      " 2.03534705e-06 1.95460052e-06 4.23897873e-05 1.81717519e-06\n",
      " 8.92904552e-07 4.01722559e-07 4.06659126e-07 1.32553296e-07\n",
      " 7.30922193e-06 9.61019282e-07 8.10777328e-06 5.11196561e-07\n",
      " 6.28008056e-05 8.04731735e-06 4.47218054e-06 9.99978781e-01\n",
      " 9.99783933e-01 8.73617589e-01 9.99899507e-01 9.99956250e-01\n",
      " 1.46455260e-03 9.99994159e-01 9.99989986e-01 8.58207022e-06\n",
      " 2.31780245e-06 4.27061423e-06 2.80112989e-04 1.49566422e-05\n",
      " 1.73508251e-05 1.44137573e-06 2.38647829e-07 3.66810383e-07\n",
      " 1.31177205e-06 1.50480073e-05 2.23182701e-06 3.21015204e-06\n",
      " 6.29076838e-01 8.70730219e-07 1.77014408e-05 1.02466520e-05\n",
      " 4.27225996e-06 5.37647622e-07 1.36818635e-05 3.95730931e-05\n",
      " 1.62677432e-04 4.75898986e-07 3.24808570e-05 2.33193148e-07\n",
      " 6.39592429e-07 2.21020031e-07 4.39878459e-05 3.93660343e-07\n",
      " 9.99498487e-01 1.59611616e-07 4.88851697e-07 1.90558956e-06\n",
      " 3.65570304e-05 8.54694827e-06 9.73798513e-01 9.91545081e-01\n",
      " 9.98994887e-01 1.20118930e-05 1.53766741e-05 9.99823987e-01\n",
      " 9.96392787e-01 9.99853969e-01 1.02935308e-04 8.75641854e-05\n",
      " 5.11131110e-03 9.99835968e-01 9.99733746e-01 1.34428090e-04\n",
      " 9.99874711e-01 9.99770105e-01 9.58232105e-01 3.25593755e-05\n",
      " 1.10255589e-03 9.83422069e-06 9.99786079e-01 9.99881148e-01\n",
      " 9.96351600e-01 7.80806513e-05 5.65958035e-05 1.21384604e-04\n",
      " 9.99567688e-01 8.41815017e-06 9.99892950e-01 1.12880571e-06\n",
      " 2.79621531e-06 9.99716580e-01 1.11475592e-05 2.05690139e-05\n",
      " 1.38911489e-07 6.87796762e-03 1.46934952e-04 9.95043278e-01\n",
      " 9.98596847e-01 9.99402642e-01 7.52500114e-07 1.09216693e-04\n",
      " 9.96032894e-01 8.40820030e-06 9.97887075e-01 9.94195402e-01\n",
      " 5.31895648e-06 1.27814729e-02 8.35573860e-03 2.50720241e-05\n",
      " 5.92667275e-05 9.32799995e-01 6.72276974e-06 9.97085989e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 129 [0/106 (0%)]\tTrain Loss: 0.000106\n",
      "Train Epoch: 129 [10/106 (9%)]\tTrain Loss: 0.019768\n",
      "Train Epoch: 129 [20/106 (19%)]\tTrain Loss: 0.000047\n",
      "Train Epoch: 129 [30/106 (28%)]\tTrain Loss: 0.000008\n",
      "Train Epoch: 129 [40/106 (38%)]\tTrain Loss: 0.000256\n",
      "Train Epoch: 129 [50/106 (47%)]\tTrain Loss: 0.000009\n",
      "Train Epoch: 129 [60/106 (57%)]\tTrain Loss: 0.000074\n",
      "Train Epoch: 129 [70/106 (66%)]\tTrain Loss: 0.000016\n",
      "Train Epoch: 129 [80/106 (75%)]\tTrain Loss: 0.091658\n",
      "Train Epoch: 129 [90/106 (85%)]\tTrain Loss: 0.000008\n",
      "Train Epoch: 129 [100/106 (94%)]\tTrain Loss: 0.007557\n",
      "\n",
      "Train set: Average loss: 0.0003, Accuracy: 424/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [7.51526898e-07 3.02869830e-05 2.36064380e-07 2.05952048e-07\n",
      " 1.85564758e-07 2.60053980e-07 6.44253896e-06 1.97453545e-07\n",
      " 7.00078004e-08 4.43378561e-08 8.84450060e-08 1.58968358e-08\n",
      " 1.72159650e-06 1.13422537e-07 9.49214552e-07 6.55132197e-08\n",
      " 7.90584727e-06 1.34657830e-06 4.83300198e-07 9.99756277e-01\n",
      " 9.99257505e-01 7.90597405e-03 9.99353349e-01 9.99555171e-01\n",
      " 6.71435773e-05 9.99819100e-01 9.99741614e-01 1.35917446e-06\n",
      " 3.32895837e-07 5.45545845e-07 3.08620911e-05 4.02780415e-06\n",
      " 2.07816220e-06 2.04276049e-07 4.55203839e-08 3.91057853e-08\n",
      " 1.00550459e-07 2.41953148e-06 2.51478866e-07 6.11008716e-07\n",
      " 5.72903641e-02 6.39066897e-08 1.83630880e-06 1.02072397e-06\n",
      " 4.19593846e-07 5.82754502e-08 1.05901006e-06 4.82257838e-06\n",
      " 1.74935794e-05 5.56033264e-08 2.42881356e-06 3.40486146e-08\n",
      " 7.41992636e-08 2.75452834e-08 4.05330593e-06 4.26400284e-08\n",
      " 9.97412741e-01 1.58666964e-08 6.12339193e-08 1.61173048e-07\n",
      " 4.59398007e-06 1.14315333e-06 3.43091846e-01 9.02325690e-01\n",
      " 9.98143435e-01 1.36518668e-06 2.10451253e-06 9.99150276e-01\n",
      " 9.73730326e-01 9.99318480e-01 2.26269058e-05 1.61285789e-05\n",
      " 7.14997202e-03 9.99276459e-01 9.98976707e-01 3.73401526e-05\n",
      " 9.99488950e-01 9.99162912e-01 1.99877799e-01 8.17503314e-06\n",
      " 2.12200757e-04 1.79675499e-06 9.98842299e-01 9.99391794e-01\n",
      " 9.36824977e-01 1.58820185e-05 3.18115926e-05 6.30780924e-05\n",
      " 9.98496890e-01 1.46430591e-06 9.99111712e-01 1.74438725e-07\n",
      " 4.12213922e-07 9.98900533e-01 1.40106124e-06 2.52821201e-06\n",
      " 1.92573459e-08 6.58553967e-04 1.77729908e-05 9.85415518e-01\n",
      " 9.96180892e-01 9.97381508e-01 1.07743702e-07 2.91780343e-05\n",
      " 9.88446474e-01 7.10819904e-07 9.71592844e-01 9.63812649e-01\n",
      " 7.12790552e-07 1.24851963e-03 1.25074608e-03 3.43622423e-06\n",
      " 8.65194397e-06 2.24161491e-01 1.05390893e-06 3.01240440e-02]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 130 [0/106 (0%)]\tTrain Loss: 0.000920\n",
      "Train Epoch: 130 [10/106 (9%)]\tTrain Loss: 0.000097\n",
      "Train Epoch: 130 [20/106 (19%)]\tTrain Loss: 0.000785\n",
      "Train Epoch: 130 [30/106 (28%)]\tTrain Loss: 0.000010\n",
      "Train Epoch: 130 [40/106 (38%)]\tTrain Loss: 0.000789\n",
      "Train Epoch: 130 [50/106 (47%)]\tTrain Loss: 0.000123\n",
      "Train Epoch: 130 [60/106 (57%)]\tTrain Loss: 0.000103\n",
      "Train Epoch: 130 [70/106 (66%)]\tTrain Loss: 0.000324\n",
      "Train Epoch: 130 [80/106 (75%)]\tTrain Loss: 0.000352\n",
      "Train Epoch: 130 [90/106 (85%)]\tTrain Loss: 0.000163\n",
      "Train Epoch: 130 [100/106 (94%)]\tTrain Loss: 0.000158\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 424/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.33598576e-06 4.12752888e-05 4.23038443e-07 3.82724011e-07\n",
      " 3.61003003e-07 4.97014298e-07 1.17476366e-05 3.95623800e-07\n",
      " 1.41284303e-07 8.38062775e-08 1.22851830e-07 3.44623885e-08\n",
      " 2.73142041e-06 2.12184275e-07 1.40990358e-06 1.02393315e-07\n",
      " 1.29357386e-05 1.91545746e-06 1.03883201e-06 9.99930739e-01\n",
      " 9.99563515e-01 8.45671833e-01 9.99729216e-01 9.99871135e-01\n",
      " 5.64094575e-04 9.99962091e-01 9.99945879e-01 1.91328490e-06\n",
      " 7.53956897e-07 1.06400284e-06 3.45986991e-05 6.28421867e-06\n",
      " 2.68050917e-06 3.60246361e-07 8.30494713e-08 8.16631029e-08\n",
      " 2.01236432e-07 4.07726156e-06 3.86056968e-07 2.31103286e-06\n",
      " 7.50033796e-01 1.83155876e-07 2.65685799e-06 1.68872930e-06\n",
      " 8.80002403e-07 1.15194190e-07 1.86169154e-06 7.69345934e-06\n",
      " 2.07667836e-05 1.04368247e-07 4.14959277e-06 6.52636558e-08\n",
      " 1.56615087e-07 5.88094125e-08 6.34372827e-06 8.58354241e-08\n",
      " 9.98365223e-01 3.64782835e-08 1.48028974e-07 3.56474771e-07\n",
      " 1.25012066e-05 2.57968713e-06 9.28540766e-01 9.72668350e-01\n",
      " 9.98983085e-01 3.62691185e-06 3.64252423e-06 9.99393582e-01\n",
      " 9.89292741e-01 9.99628425e-01 5.11238904e-05 3.75974887e-05\n",
      " 5.10294503e-03 9.99653339e-01 9.99450028e-01 2.91459655e-05\n",
      " 9.99707282e-01 9.99419570e-01 4.72624630e-01 9.64264927e-06\n",
      " 2.35908595e-03 2.68564690e-06 9.99448597e-01 9.99691010e-01\n",
      " 9.80204880e-01 2.87651874e-05 1.82112090e-05 3.09902971e-05\n",
      " 9.99147892e-01 2.03328477e-06 9.99746621e-01 3.61017101e-07\n",
      " 7.38077063e-07 9.99298334e-01 2.67575365e-06 4.49051095e-06\n",
      " 4.36336443e-08 9.95255075e-04 5.52898455e-05 9.92910564e-01\n",
      " 9.95743930e-01 9.97760177e-01 1.74088385e-07 8.43879025e-05\n",
      " 9.93641913e-01 1.49935818e-06 9.69894826e-01 9.83246744e-01\n",
      " 1.90748415e-06 2.05658772e-03 8.16088263e-03 9.21047831e-06\n",
      " 1.78224673e-05 5.97466111e-01 1.69160319e-06 7.36760736e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 24 TN= 51 FN= 32 FP= 9\n",
      "TP+FP 33\n",
      "precision 0.7272727272727273\n",
      "recall 0.42857142857142855\n",
      "F1 0.5393258426966292\n",
      "acc 0.646551724137931\n",
      "AUCp 0.6392857142857142\n",
      "AUC 0.7818452380952381\n",
      "\n",
      " The epoch is 130, average recall: 0.4286, average precision: 0.7273,average F1: 0.5393, average accuracy: 0.6466, average AUC: 0.7818\n",
      "Train Epoch: 131 [0/106 (0%)]\tTrain Loss: 0.000011\n",
      "Train Epoch: 131 [10/106 (9%)]\tTrain Loss: 0.008343\n",
      "Train Epoch: 131 [20/106 (19%)]\tTrain Loss: 0.000111\n",
      "Train Epoch: 131 [30/106 (28%)]\tTrain Loss: 0.000520\n",
      "Train Epoch: 131 [40/106 (38%)]\tTrain Loss: 0.001232\n",
      "Train Epoch: 131 [50/106 (47%)]\tTrain Loss: 0.000035\n",
      "Train Epoch: 131 [60/106 (57%)]\tTrain Loss: 0.079286\n",
      "Train Epoch: 131 [70/106 (66%)]\tTrain Loss: 0.000014\n",
      "Train Epoch: 131 [80/106 (75%)]\tTrain Loss: 0.000019\n",
      "Train Epoch: 131 [90/106 (85%)]\tTrain Loss: 0.000018\n",
      "Train Epoch: 131 [100/106 (94%)]\tTrain Loss: 0.000105\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 419/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.43374905e-06 5.72748650e-05 6.22282357e-07 6.53124744e-07\n",
      " 5.91985781e-07 6.38729944e-07 9.77934087e-06 6.02433147e-07\n",
      " 2.34006734e-07 1.94683395e-07 2.30270075e-07 5.16455430e-08\n",
      " 1.53678193e-05 5.33893228e-07 3.20848130e-06 2.21195904e-07\n",
      " 5.41539448e-05 2.16908597e-06 4.16013290e-06 9.99953508e-01\n",
      " 9.99568880e-01 9.77795482e-01 9.99830961e-01 9.99897242e-01\n",
      " 1.18928682e-02 9.99981403e-01 9.99965668e-01 2.61240166e-06\n",
      " 1.55597684e-06 1.90289256e-06 5.44214890e-05 1.98208691e-05\n",
      " 3.66576432e-06 6.48227513e-07 1.15813236e-07 1.33660990e-07\n",
      " 3.62631994e-07 1.22829360e-05 7.68476184e-07 2.76868241e-06\n",
      " 9.29782271e-01 3.06113208e-07 4.46983177e-06 3.29726277e-06\n",
      " 1.99293459e-06 1.36903765e-07 2.77515892e-06 1.52638531e-05\n",
      " 6.17243059e-05 2.31691743e-07 8.28849352e-06 9.73613439e-08\n",
      " 2.93686583e-07 1.00500984e-07 4.36331429e-05 1.37855722e-07\n",
      " 9.98976946e-01 5.82758943e-08 2.52023625e-07 6.77001083e-07\n",
      " 9.31413088e-05 9.37071854e-06 9.96765256e-01 9.97364581e-01\n",
      " 9.98805285e-01 7.84923213e-06 1.00484376e-05 9.99586642e-01\n",
      " 9.96097505e-01 9.99611676e-01 1.19700118e-04 1.97449161e-04\n",
      " 3.35472584e-01 9.99668956e-01 9.99413967e-01 5.61684137e-05\n",
      " 9.99670386e-01 9.99518394e-01 8.69490683e-01 3.73669391e-05\n",
      " 5.84072828e-01 7.18715273e-06 9.99510646e-01 9.99749482e-01\n",
      " 9.94747102e-01 2.72795209e-04 4.35349248e-05 6.10501156e-05\n",
      " 9.99008119e-01 6.66671349e-06 9.99830127e-01 5.91124831e-07\n",
      " 1.62096364e-06 9.99315262e-01 6.04563093e-06 9.61393653e-06\n",
      " 7.24753164e-08 2.55374913e-03 3.92647344e-04 9.97754991e-01\n",
      " 9.97638106e-01 9.98931348e-01 2.73484545e-07 1.97624974e-03\n",
      " 9.98023987e-01 3.60056492e-06 9.97168243e-01 9.95709181e-01\n",
      " 4.48043238e-06 1.12060830e-02 3.02051026e-02 1.96206329e-05\n",
      " 5.68124196e-05 9.87994313e-01 4.83318581e-06 9.99201238e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 132 [0/106 (0%)]\tTrain Loss: 0.000089\n",
      "Train Epoch: 132 [10/106 (9%)]\tTrain Loss: 0.000253\n",
      "Train Epoch: 132 [20/106 (19%)]\tTrain Loss: 0.118821\n",
      "Train Epoch: 132 [30/106 (28%)]\tTrain Loss: 0.000007\n",
      "Train Epoch: 132 [40/106 (38%)]\tTrain Loss: 0.000011\n",
      "Train Epoch: 132 [50/106 (47%)]\tTrain Loss: 0.000076\n",
      "Train Epoch: 132 [60/106 (57%)]\tTrain Loss: 0.000137\n",
      "Train Epoch: 132 [70/106 (66%)]\tTrain Loss: 0.011449\n",
      "Train Epoch: 132 [80/106 (75%)]\tTrain Loss: 0.000036\n",
      "Train Epoch: 132 [90/106 (85%)]\tTrain Loss: 0.000063\n",
      "Train Epoch: 132 [100/106 (94%)]\tTrain Loss: 0.012127\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 421/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.79460415e-06 1.70406609e-04 1.04375852e-06 1.05582137e-06\n",
      " 1.24346604e-06 7.02984266e-07 2.46805266e-05 1.20126117e-06\n",
      " 4.62295986e-07 5.00669785e-07 4.66300094e-07 1.02671400e-07\n",
      " 1.31638653e-05 8.14213138e-07 6.87113697e-06 4.35214389e-07\n",
      " 2.32848452e-05 3.87517548e-06 2.65653875e-06 9.99992609e-01\n",
      " 9.99903679e-01 9.22658384e-01 9.99955654e-01 9.99985099e-01\n",
      " 1.03328878e-03 9.99996543e-01 9.99998450e-01 3.57657473e-06\n",
      " 1.66803989e-06 3.25905694e-06 1.09948676e-04 2.81065768e-05\n",
      " 1.31620827e-05 8.72440864e-07 1.17071401e-07 3.27593540e-07\n",
      " 8.87754027e-07 2.11417791e-05 1.19842684e-06 3.45162130e-06\n",
      " 9.51105297e-01 8.28482143e-07 8.05279888e-06 6.13440898e-06\n",
      " 3.29433669e-06 2.79991127e-07 5.89427600e-06 3.88993794e-05\n",
      " 1.34298680e-04 7.12278108e-07 2.16211611e-05 1.45215537e-07\n",
      " 6.34362209e-07 1.73848974e-07 1.96382657e-01 2.35868825e-07\n",
      " 9.99730766e-01 1.02550842e-07 4.86027261e-07 1.91349659e-06\n",
      " 8.94740660e-05 9.48871093e-06 9.89998400e-01 9.98275399e-01\n",
      " 9.99848485e-01 4.96558414e-06 8.91072978e-06 9.99882102e-01\n",
      " 9.97131824e-01 9.99921679e-01 2.34817009e-04 3.48802365e-04\n",
      " 9.95481730e-01 9.99924064e-01 9.99785602e-01 1.49532105e-04\n",
      " 9.99769986e-01 9.99226093e-01 8.30356598e-01 4.04229395e-05\n",
      " 2.26131827e-01 2.85887363e-05 9.99898434e-01 9.99907851e-01\n",
      " 9.90763724e-01 9.77463904e-04 4.44392877e-04 3.16949852e-04\n",
      " 9.99561489e-01 4.68475810e-06 9.99965668e-01 1.31985325e-06\n",
      " 4.83037502e-06 9.99822557e-01 7.56931149e-06 1.33952844e-05\n",
      " 1.24903849e-07 3.70664359e-03 3.96891177e-04 9.94528234e-01\n",
      " 9.98685181e-01 9.99465525e-01 3.50331533e-07 5.47924647e-05\n",
      " 9.99370635e-01 6.69425526e-06 9.99756038e-01 9.88297403e-01\n",
      " 3.23963673e-06 3.03176418e-03 6.24368433e-03 2.26640514e-05\n",
      " 6.24196400e-05 6.52170300e-01 3.60755257e-06 9.99743402e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "Train Epoch: 133 [0/106 (0%)]\tTrain Loss: 0.000102\n",
      "Train Epoch: 133 [10/106 (9%)]\tTrain Loss: 0.000005\n",
      "Train Epoch: 133 [20/106 (19%)]\tTrain Loss: 0.000102\n",
      "Train Epoch: 133 [30/106 (28%)]\tTrain Loss: 0.000030\n",
      "Train Epoch: 133 [40/106 (38%)]\tTrain Loss: 0.000022\n",
      "Train Epoch: 133 [50/106 (47%)]\tTrain Loss: 0.000132\n",
      "Train Epoch: 133 [60/106 (57%)]\tTrain Loss: 0.000319\n",
      "Train Epoch: 133 [70/106 (66%)]\tTrain Loss: 0.000105\n",
      "Train Epoch: 133 [80/106 (75%)]\tTrain Loss: 0.000464\n",
      "Train Epoch: 133 [90/106 (85%)]\tTrain Loss: 0.001902\n",
      "Train Epoch: 133 [100/106 (94%)]\tTrain Loss: 0.000378\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [8.01914837e-07 1.07090178e-04 1.70583189e-06 8.46498381e-07\n",
      " 1.60891250e-06 3.50604608e-07 1.92679872e-05 1.04767082e-06\n",
      " 5.59422404e-07 6.55814802e-07 2.89373389e-07 1.66721449e-07\n",
      " 1.34176105e-06 1.30060334e-06 7.66872381e-06 3.44285581e-07\n",
      " 4.64603636e-06 4.58165096e-06 1.32174011e-06 9.99991655e-01\n",
      " 9.99735177e-01 2.10925878e-04 9.99872327e-01 9.99990702e-01\n",
      " 1.55615617e-05 9.99997139e-01 9.99999046e-01 2.65726339e-06\n",
      " 1.06963716e-06 1.53587337e-06 1.03134444e-05 6.83958615e-06\n",
      " 1.65375714e-05 5.54009091e-07 1.06294863e-07 3.65074499e-07\n",
      " 1.11241536e-06 5.12809675e-06 8.74932198e-07 9.52676203e-07\n",
      " 1.25103304e-03 4.96823361e-07 8.32461683e-06 4.62329990e-06\n",
      " 1.86995146e-06 3.76432325e-07 5.05458138e-06 2.84504076e-05\n",
      " 4.97403307e-05 7.56085967e-07 1.26895247e-05 1.17856793e-07\n",
      " 7.29485180e-07 1.65519495e-07 5.52715537e-05 3.94241567e-07\n",
      " 9.91471946e-01 1.73438295e-07 5.60877538e-07 1.91048616e-06\n",
      " 1.39912163e-05 5.42356383e-06 4.76828869e-03 1.25403553e-02\n",
      " 9.43130910e-01 2.71108433e-06 5.04789023e-06 9.99340594e-01\n",
      " 7.75874376e-01 9.99803960e-01 1.04759156e-05 1.87750629e-05\n",
      " 2.17797628e-04 9.99412656e-01 9.22535717e-01 3.78020159e-05\n",
      " 9.99231339e-01 9.94530737e-01 4.67855978e-04 1.17385216e-05\n",
      " 4.40197800e-05 5.35646814e-06 9.99748528e-01 9.96896505e-01\n",
      " 1.40227064e-01 4.26296538e-05 2.96651888e-05 3.76299467e-05\n",
      " 9.97676313e-01 2.95203972e-06 9.99989510e-01 4.56499379e-07\n",
      " 2.89122045e-06 9.99753654e-01 5.72834097e-06 1.25543729e-05\n",
      " 1.71721851e-07 3.09115130e-04 1.25851311e-05 2.94717238e-03\n",
      " 4.75810975e-01 8.04294832e-03 2.56954394e-07 4.04468756e-06\n",
      " 1.09449285e-03 4.85517148e-06 9.98553097e-01 3.20988059e-01\n",
      " 1.27322824e-06 1.49084779e-04 2.77909003e-05 8.13607585e-06\n",
      " 1.43257093e-05 3.05389665e-04 1.92499124e-06 4.76139430e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 134 [0/106 (0%)]\tTrain Loss: 0.000012\n",
      "Train Epoch: 134 [10/106 (9%)]\tTrain Loss: 0.000058\n",
      "Train Epoch: 134 [20/106 (19%)]\tTrain Loss: 0.000011\n",
      "Train Epoch: 134 [30/106 (28%)]\tTrain Loss: 0.000310\n",
      "Train Epoch: 134 [40/106 (38%)]\tTrain Loss: 0.000775\n",
      "Train Epoch: 134 [50/106 (47%)]\tTrain Loss: 0.000051\n",
      "Train Epoch: 134 [60/106 (57%)]\tTrain Loss: 0.000246\n",
      "Train Epoch: 134 [70/106 (66%)]\tTrain Loss: 0.000113\n",
      "Train Epoch: 134 [80/106 (75%)]\tTrain Loss: 0.000084\n",
      "Train Epoch: 134 [90/106 (85%)]\tTrain Loss: 0.000254\n",
      "Train Epoch: 134 [100/106 (94%)]\tTrain Loss: 0.008468\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.96093816e-08 6.00556596e-05 5.72746650e-08 5.55362334e-08\n",
      " 2.83396080e-08 1.78600725e-08 2.85999295e-06 6.23658352e-08\n",
      " 1.61386779e-08 1.57683733e-08 1.16539338e-08 3.59926777e-09\n",
      " 3.23623198e-07 2.18088701e-07 4.58185650e-06 6.60944579e-08\n",
      " 7.18546016e-06 6.77507953e-04 7.92924766e-08 9.99963164e-01\n",
      " 9.99692798e-01 9.99216914e-01 9.99964833e-01 9.99833107e-01\n",
      " 2.23972616e-04 9.99995589e-01 9.99986529e-01 1.66988798e-07\n",
      " 4.27430365e-07 1.55413929e-07 1.13801518e-03 5.63113972e-05\n",
      " 2.75539583e-06 4.36030305e-08 7.55110019e-09 5.06365527e-09\n",
      " 6.99730673e-09 1.42452491e-05 2.04443282e-07 5.88864623e-07\n",
      " 6.38577580e-01 4.01740508e-08 1.45874837e-05 4.05977914e-07\n",
      " 3.70180516e-07 7.59382424e-09 5.15427075e-07 1.13446167e-05\n",
      " 2.55588111e-05 4.33060627e-08 2.01162493e-06 3.24826432e-09\n",
      " 6.23820142e-08 7.34930117e-09 3.37040365e-05 9.35641520e-09\n",
      " 4.40444909e-02 2.95510283e-09 6.58415544e-08 2.76196829e-07\n",
      " 1.46670791e-05 1.25764961e-06 4.68203127e-01 9.80021298e-01\n",
      " 9.99903202e-01 1.19994218e-06 3.10640553e-06 9.99776423e-01\n",
      " 9.52306747e-01 9.99943256e-01 1.31297993e-05 1.95404518e-05\n",
      " 9.86828506e-01 9.99908924e-01 9.99816000e-01 9.95955229e-01\n",
      " 9.99909520e-01 9.99774158e-01 9.97408211e-01 6.02777442e-03\n",
      " 9.99517083e-01 7.19151518e-04 9.99884844e-01 9.99920726e-01\n",
      " 9.94053185e-01 2.55237770e-04 9.39213642e-05 2.03834679e-05\n",
      " 9.99725878e-01 1.70166902e-06 9.99960184e-01 6.07137238e-08\n",
      " 9.65778440e-07 9.99236584e-01 1.21704977e-06 3.17289800e-06\n",
      " 6.91111790e-09 6.71102107e-03 1.68562183e-05 9.97680128e-01\n",
      " 9.95442152e-01 9.99744356e-01 2.46530814e-08 3.20564359e-05\n",
      " 9.99193132e-01 3.46465140e-06 9.99790967e-01 9.94134486e-01\n",
      " 4.81631844e-07 3.38990736e-04 8.73932458e-06 8.96772872e-06\n",
      " 6.54682590e-05 1.99032039e-03 6.42936470e-07 9.88229275e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 135 [0/106 (0%)]\tTrain Loss: 0.000018\n",
      "Train Epoch: 135 [10/106 (9%)]\tTrain Loss: 0.007317\n",
      "Train Epoch: 135 [20/106 (19%)]\tTrain Loss: 0.000076\n",
      "Train Epoch: 135 [30/106 (28%)]\tTrain Loss: 0.101018\n",
      "Train Epoch: 135 [40/106 (38%)]\tTrain Loss: 0.000140\n",
      "Train Epoch: 135 [50/106 (47%)]\tTrain Loss: 0.000239\n",
      "Train Epoch: 135 [60/106 (57%)]\tTrain Loss: 0.007229\n",
      "Train Epoch: 135 [70/106 (66%)]\tTrain Loss: 0.000379\n",
      "Train Epoch: 135 [80/106 (75%)]\tTrain Loss: 0.000080\n",
      "Train Epoch: 135 [90/106 (85%)]\tTrain Loss: 0.000102\n",
      "Train Epoch: 135 [100/106 (94%)]\tTrain Loss: 0.000015\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 421/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [7.56039867e-07 3.98113334e-05 1.02122931e-06 9.96140216e-07\n",
      " 1.33798642e-06 7.84225051e-07 1.78364098e-05 1.36602659e-06\n",
      " 5.20425658e-07 5.37059293e-07 3.42879275e-07 1.18968096e-07\n",
      " 3.29591567e-05 1.30232104e-06 2.46924174e-05 1.76817025e-06\n",
      " 3.27655463e-03 6.83783855e-06 1.61558728e-05 9.99998212e-01\n",
      " 9.99940753e-01 9.99884844e-01 9.99997377e-01 9.99995708e-01\n",
      " 4.00475353e-01 9.99999404e-01 9.99999404e-01 1.03945831e-06\n",
      " 4.37410745e-06 1.55492069e-06 1.06784966e-04 2.72345023e-05\n",
      " 2.09399168e-05 2.36919459e-06 2.07993779e-07 4.11606976e-07\n",
      " 2.15082309e-06 4.84295934e-03 1.28286183e-06 1.85070719e-04\n",
      " 9.98578310e-01 1.04915325e-05 1.57268805e-05 3.25705550e-05\n",
      " 9.90555823e-01 2.04124930e-07 4.21331970e-06 5.90146883e-05\n",
      " 8.08605051e-04 4.88532010e-07 4.44257639e-05 1.43006588e-07\n",
      " 7.50570234e-06 3.98802428e-07 1.68161096e-05 9.96511631e-07\n",
      " 1.81132025e-04 2.19205106e-07 2.08139136e-06 2.65882354e-05\n",
      " 1.15530620e-05 9.36351807e-06 6.98137935e-03 9.39802110e-01\n",
      " 9.99986410e-01 7.96354550e-04 2.14943739e-05 9.99961734e-01\n",
      " 7.31232110e-04 9.99967217e-01 5.53419668e-05 2.01490591e-04\n",
      " 9.72899139e-01 9.99973416e-01 9.99838710e-01 7.41340482e-05\n",
      " 9.99964595e-01 9.99916673e-01 5.18772140e-05 4.83698008e-04\n",
      " 9.99632239e-01 5.01839211e-03 9.99975085e-01 9.99938846e-01\n",
      " 9.98835146e-01 7.80607661e-05 1.38558957e-04 2.10298531e-05\n",
      " 9.99956489e-01 3.79981284e-05 9.99996901e-01 6.01063903e-07\n",
      " 7.54183657e-06 9.99973536e-01 2.51675101e-05 6.11694122e-04\n",
      " 1.69109194e-06 7.19187839e-04 8.38126580e-05 9.99921441e-01\n",
      " 9.99244928e-01 9.99163270e-01 3.34318003e-07 9.97375607e-01\n",
      " 9.59284067e-01 2.77293220e-05 9.99965668e-01 9.99960423e-01\n",
      " 2.44868552e-05 7.52550986e-05 6.57936335e-02 1.00145047e-03\n",
      " 9.98844981e-01 9.99933362e-01 6.48973764e-06 7.36801012e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      "Train Epoch: 136 [0/106 (0%)]\tTrain Loss: 0.003380\n",
      "Train Epoch: 136 [10/106 (9%)]\tTrain Loss: 0.000701\n",
      "Train Epoch: 136 [20/106 (19%)]\tTrain Loss: 0.000182\n",
      "Train Epoch: 136 [30/106 (28%)]\tTrain Loss: 0.000006\n",
      "Train Epoch: 136 [40/106 (38%)]\tTrain Loss: 0.000015\n",
      "Train Epoch: 136 [50/106 (47%)]\tTrain Loss: 0.000236\n",
      "Train Epoch: 136 [60/106 (57%)]\tTrain Loss: 0.000007\n",
      "Train Epoch: 136 [70/106 (66%)]\tTrain Loss: 0.000063\n",
      "Train Epoch: 136 [80/106 (75%)]\tTrain Loss: 0.000211\n",
      "Train Epoch: 136 [90/106 (85%)]\tTrain Loss: 0.000107\n",
      "Train Epoch: 136 [100/106 (94%)]\tTrain Loss: 0.000043\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 419/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.56473101e-05 2.05180824e-01 8.72191595e-06 1.22216388e-05\n",
      " 4.06155596e-06 1.83712891e-05 6.46956861e-01 2.92333771e-05\n",
      " 2.21207802e-06 2.33974652e-06 1.33300773e-06 8.23959340e-07\n",
      " 1.12214948e-04 1.72776145e-05 8.24511633e-04 3.34056131e-06\n",
      " 1.03562772e-04 6.92449138e-03 6.34189519e-06 9.99803841e-01\n",
      " 9.67289925e-01 9.97412384e-01 9.99534011e-01 9.98634756e-01\n",
      " 3.92294303e-03 9.99947786e-01 9.99398708e-01 6.21338722e-06\n",
      " 7.50235849e-05 1.28467918e-05 9.97252882e-01 3.72244082e-02\n",
      " 3.47144523e-04 2.12864165e-06 9.58950295e-07 6.33212551e-07\n",
      " 4.64121712e-07 1.09825497e-02 5.88625953e-05 1.87905054e-04\n",
      " 3.48785222e-01 1.86279449e-05 2.50658544e-04 2.96589369e-05\n",
      " 2.01415805e-05 1.63140555e-06 6.85832492e-05 2.46042036e-03\n",
      " 9.43968743e-02 2.83275563e-06 1.59066985e-03 4.66975195e-07\n",
      " 4.79661730e-06 5.07782261e-07 4.29053442e-04 3.29268005e-07\n",
      " 9.88109052e-01 2.45732792e-07 3.83664701e-05 4.13186826e-05\n",
      " 2.21414752e-02 2.78927479e-03 9.95649278e-01 9.96632040e-01\n",
      " 9.99827206e-01 5.03425654e-05 7.39180832e-04 9.99929309e-01\n",
      " 9.80614960e-01 9.99204218e-01 8.71495426e-01 8.07007432e-01\n",
      " 9.95556891e-01 9.99562204e-01 9.99283373e-01 9.99522924e-01\n",
      " 9.99643445e-01 9.99394774e-01 9.99309897e-01 9.96883690e-01\n",
      " 9.99332130e-01 9.97913539e-01 9.99353468e-01 9.98979509e-01\n",
      " 9.93599772e-01 9.79011118e-01 9.98448133e-01 1.06375918e-01\n",
      " 9.98661518e-01 2.82029243e-04 9.99868274e-01 1.72583132e-05\n",
      " 1.68445753e-04 9.97997820e-01 5.84305148e-04 1.07227603e-03\n",
      " 3.31754723e-06 9.44269001e-01 3.41791217e-03 4.68885809e-01\n",
      " 9.96683657e-01 9.99261200e-01 1.40060968e-06 9.17339146e-01\n",
      " 9.97708082e-01 1.09616239e-02 9.99733031e-01 5.82086435e-03\n",
      " 5.72442877e-05 1.21716736e-02 2.14707502e-03 5.60750719e-03\n",
      " 2.02494804e-04 9.83366966e-01 2.14541127e-04 9.96773779e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "Train Epoch: 137 [0/106 (0%)]\tTrain Loss: 0.000059\n",
      "Train Epoch: 137 [10/106 (9%)]\tTrain Loss: 0.000068\n",
      "Train Epoch: 137 [20/106 (19%)]\tTrain Loss: 0.000053\n",
      "Train Epoch: 137 [30/106 (28%)]\tTrain Loss: 0.000049\n",
      "Train Epoch: 137 [40/106 (38%)]\tTrain Loss: 0.000588\n",
      "Train Epoch: 137 [50/106 (47%)]\tTrain Loss: 0.000174\n",
      "Train Epoch: 137 [60/106 (57%)]\tTrain Loss: 0.000158\n",
      "Train Epoch: 137 [70/106 (66%)]\tTrain Loss: 0.000143\n",
      "Train Epoch: 137 [80/106 (75%)]\tTrain Loss: 0.000105\n",
      "Train Epoch: 137 [90/106 (85%)]\tTrain Loss: 0.000010\n",
      "Train Epoch: 137 [100/106 (94%)]\tTrain Loss: 0.000073\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 421/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.87807725e-05 2.22511613e-03 8.63197693e-06 1.17630691e-04\n",
      " 5.64151342e-05 2.45388455e-05 2.74989265e-03 4.91226441e-04\n",
      " 9.99325857e-06 1.75755704e-05 2.67000019e-06 3.52697271e-06\n",
      " 1.36346571e-04 7.69508115e-05 1.77295804e-02 2.64092523e-04\n",
      " 1.38914408e-02 3.66929290e-03 4.42216606e-06 9.99949694e-01\n",
      " 9.39872026e-01 9.98525560e-01 9.96875763e-01 9.99634862e-01\n",
      " 1.22887606e-03 9.99944210e-01 9.99879599e-01 3.30081275e-05\n",
      " 2.19196983e-04 1.96548681e-05 9.56798866e-02 8.78208037e-03\n",
      " 2.31627448e-04 1.09958764e-05 6.96052393e-06 6.24866107e-06\n",
      " 2.25444501e-06 1.75838768e-02 1.84813980e-04 1.31804391e-03\n",
      " 4.39644784e-01 6.94751798e-05 1.37386657e-03 3.61675542e-04\n",
      " 1.71055683e-04 5.14999419e-06 1.72847227e-04 2.23931950e-03\n",
      " 3.26381773e-02 1.32649329e-05 1.70941604e-03 4.55613963e-06\n",
      " 5.73080251e-05 4.21637833e-06 4.20535961e-03 2.15079012e-06\n",
      " 9.98903751e-01 1.46969080e-06 6.55435084e-04 2.51597870e-04\n",
      " 8.06174427e-03 9.19823442e-03 9.99306798e-01 9.99402285e-01\n",
      " 9.99835610e-01 1.98864311e-01 1.37177289e-01 9.99992728e-01\n",
      " 9.98767138e-01 9.99788940e-01 9.58409965e-01 2.82006115e-01\n",
      " 9.95491326e-01 9.99637961e-01 9.98520315e-01 9.99184906e-01\n",
      " 9.99885321e-01 9.99813020e-01 9.98345256e-01 9.99179780e-01\n",
      " 9.98923957e-01 9.53766704e-01 9.99969125e-01 9.99676824e-01\n",
      " 9.98450875e-01 9.89422381e-01 3.32603827e-02 1.13848643e-03\n",
      " 9.99382138e-01 2.01259251e-03 9.99970555e-01 5.90575874e-05\n",
      " 1.50716072e-03 9.99038458e-01 6.03562628e-04 9.46538872e-04\n",
      " 1.37781944e-05 9.96271491e-01 5.93954206e-01 3.13431293e-01\n",
      " 9.97772038e-01 9.99227643e-01 2.52824502e-05 9.99008894e-01\n",
      " 9.93281960e-01 2.99456646e-03 9.99839664e-01 9.96662915e-01\n",
      " 3.04553483e-04 1.20400619e-02 9.94590949e-03 2.01852284e-02\n",
      " 1.43059925e-03 8.18289995e-01 1.75542303e-03 6.11449638e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 138 [0/106 (0%)]\tTrain Loss: 0.000021\n",
      "Train Epoch: 138 [10/106 (9%)]\tTrain Loss: 0.233434\n",
      "Train Epoch: 138 [20/106 (19%)]\tTrain Loss: 0.000354\n",
      "Train Epoch: 138 [30/106 (28%)]\tTrain Loss: 0.000077\n",
      "Train Epoch: 138 [40/106 (38%)]\tTrain Loss: 0.002169\n",
      "Train Epoch: 138 [50/106 (47%)]\tTrain Loss: 0.000032\n",
      "Train Epoch: 138 [60/106 (57%)]\tTrain Loss: 0.007520\n",
      "Train Epoch: 138 [70/106 (66%)]\tTrain Loss: 0.000369\n",
      "Train Epoch: 138 [80/106 (75%)]\tTrain Loss: 0.000225\n",
      "Train Epoch: 138 [90/106 (85%)]\tTrain Loss: 0.000039\n",
      "Train Epoch: 138 [100/106 (94%)]\tTrain Loss: 0.000059\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 416/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.28824368e-07 3.39961960e-04 7.63331116e-07 3.92256140e-07\n",
      " 1.87672825e-07 1.91966905e-08 5.36190018e-05 8.73357635e-08\n",
      " 1.86229681e-08 1.37623616e-07 3.36563812e-06 2.04733919e-08\n",
      " 9.89599175e-06 3.18457069e-07 2.36241681e-06 6.96136624e-07\n",
      " 8.55955805e-05 8.36944523e-08 5.51675100e-07 9.99988794e-01\n",
      " 3.26908223e-04 1.27651765e-05 9.60036099e-01 9.99911189e-01\n",
      " 1.47901142e-06 9.99990463e-01 1.00000000e+00 6.37016910e-07\n",
      " 4.42997759e-07 6.26162546e-07 4.48093815e-05 5.20052754e-06\n",
      " 6.45464206e-06 6.92090737e-07 1.68462449e-07 2.77186587e-07\n",
      " 1.81387549e-07 1.49896351e-07 2.23452673e-07 1.26241503e-07\n",
      " 4.04102195e-07 9.32792545e-08 1.16937279e-07 5.18187107e-07\n",
      " 4.56976750e-07 7.02797962e-08 3.63990239e-06 4.59760959e-05\n",
      " 1.16827287e-04 1.49563209e-06 1.56344795e-05 1.74106400e-08\n",
      " 7.55108971e-08 4.33670628e-08 9.60118473e-01 3.46297583e-07\n",
      " 2.35441681e-02 7.52083338e-08 1.12458345e-07 2.02678820e-07\n",
      " 7.26574353e-06 7.09396181e-06 1.65113626e-04 2.64707942e-05\n",
      " 9.99996305e-01 1.25041993e-07 8.16095223e-07 9.99999285e-01\n",
      " 5.69128955e-04 9.99964356e-01 3.18799317e-01 1.82576507e-01\n",
      " 2.13882513e-02 9.99981761e-01 9.99976754e-01 3.75021991e-05\n",
      " 9.99963045e-01 9.99893069e-01 9.87816155e-01 9.79691684e-01\n",
      " 9.98241663e-01 9.71901596e-01 9.99999523e-01 9.94834423e-01\n",
      " 7.68786413e-05 9.96899128e-01 2.57078995e-04 2.23143652e-04\n",
      " 9.99209106e-01 3.48410458e-06 1.00000000e+00 9.15368688e-08\n",
      " 3.27270016e-07 9.99751389e-01 1.05720642e-03 1.43232501e-06\n",
      " 7.63114301e-08 5.85821308e-02 9.98917341e-01 1.98564023e-01\n",
      " 9.99833822e-01 8.09969530e-02 3.88464834e-08 1.10567853e-01\n",
      " 9.99109328e-01 1.31378665e-05 9.99990821e-01 9.59620476e-01\n",
      " 2.26229963e-06 2.33986005e-02 8.52160096e-01 4.39064141e-04\n",
      " 8.86070848e-05 3.02167406e-04 5.90775983e-07 9.68899371e-07]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 139 [0/106 (0%)]\tTrain Loss: 0.000241\n",
      "Train Epoch: 139 [10/106 (9%)]\tTrain Loss: 0.000052\n",
      "Train Epoch: 139 [20/106 (19%)]\tTrain Loss: 0.001570\n",
      "Train Epoch: 139 [30/106 (28%)]\tTrain Loss: 0.000102\n",
      "Train Epoch: 139 [40/106 (38%)]\tTrain Loss: 0.009196\n",
      "Train Epoch: 139 [50/106 (47%)]\tTrain Loss: 0.000555\n",
      "Train Epoch: 139 [60/106 (57%)]\tTrain Loss: 0.313330\n",
      "Train Epoch: 139 [70/106 (66%)]\tTrain Loss: 0.047348\n",
      "Train Epoch: 139 [80/106 (75%)]\tTrain Loss: 0.001152\n",
      "Train Epoch: 139 [90/106 (85%)]\tTrain Loss: 0.054695\n",
      "Train Epoch: 139 [100/106 (94%)]\tTrain Loss: 0.000422\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 412/424 (97%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.55685354e-05 2.80206203e-02 1.29382504e-04 3.21023581e-05\n",
      " 1.42606341e-05 2.54362330e-05 1.83914055e-03 9.13104850e-06\n",
      " 4.02322075e-06 1.69870418e-05 1.15681047e-04 4.38231564e-06\n",
      " 2.10883270e-04 5.18765228e-05 4.14439535e-04 4.37196068e-05\n",
      " 1.31293957e-03 5.42655180e-04 5.28409510e-05 9.99737322e-01\n",
      " 6.31980540e-04 9.99206126e-01 1.18225366e-02 9.99578655e-01\n",
      " 2.23764963e-03 9.99656796e-01 9.99953508e-01 2.96763374e-05\n",
      " 1.29744248e-03 1.00802665e-03 1.94875652e-03 5.07184677e-03\n",
      " 7.15305679e-04 1.95329456e-04 2.17175620e-05 1.86038724e-05\n",
      " 2.05334636e-05 4.69458500e-05 2.89805845e-04 5.34928586e-05\n",
      " 1.08861517e-04 3.96503019e-05 5.28150558e-05 4.62966636e-05\n",
      " 3.74929041e-05 1.75001878e-05 1.55582631e-04 6.75063522e-04\n",
      " 1.46108707e-02 3.29961731e-05 5.96059428e-04 4.00153158e-06\n",
      " 9.03160326e-05 4.63558263e-06 4.75460887e-02 2.29652524e-05\n",
      " 1.24117196e-01 9.18827300e-06 4.46363119e-05 1.24321465e-04\n",
      " 9.97378469e-01 1.52892500e-01 9.99804080e-01 9.99696255e-01\n",
      " 9.99457657e-01 5.35831714e-05 3.34683433e-03 9.99681115e-01\n",
      " 5.90661705e-01 9.99770939e-01 9.23385844e-03 6.81095719e-01\n",
      " 7.60518551e-01 9.99581993e-01 9.99771655e-01 9.94988382e-01\n",
      " 9.99590814e-01 9.99586165e-01 1.08805113e-02 3.58886480e-01\n",
      " 9.34351206e-01 8.25600326e-02 9.99874115e-01 9.99203742e-01\n",
      " 3.56852822e-02 1.69028506e-01 9.81077552e-04 1.44377921e-03\n",
      " 9.92443502e-01 6.01178908e-04 9.99958634e-01 8.36316613e-05\n",
      " 1.04967716e-04 9.99356329e-01 9.33133718e-03 3.36850004e-04\n",
      " 3.84187915e-05 9.89805877e-01 1.03180727e-03 9.98500586e-01\n",
      " 9.99200165e-01 9.99358714e-01 7.34773039e-06 2.98671005e-03\n",
      " 9.80459601e-02 2.13521154e-04 9.99724686e-01 1.73367898e-03\n",
      " 2.22560600e-04 7.36638997e-03 8.47455084e-01 6.87977374e-02\n",
      " 6.57338575e-02 9.97681022e-01 2.34255727e-04 6.85132109e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 140 [0/106 (0%)]\tTrain Loss: 0.000119\n",
      "Train Epoch: 140 [10/106 (9%)]\tTrain Loss: 0.000015\n",
      "Train Epoch: 140 [20/106 (19%)]\tTrain Loss: 0.005136\n",
      "Train Epoch: 140 [30/106 (28%)]\tTrain Loss: 0.000176\n",
      "Train Epoch: 140 [40/106 (38%)]\tTrain Loss: 0.000089\n",
      "Train Epoch: 140 [50/106 (47%)]\tTrain Loss: 0.000074\n",
      "Train Epoch: 140 [60/106 (57%)]\tTrain Loss: 0.000042\n",
      "Train Epoch: 140 [70/106 (66%)]\tTrain Loss: 0.000497\n",
      "Train Epoch: 140 [80/106 (75%)]\tTrain Loss: 0.000989\n",
      "Train Epoch: 140 [90/106 (85%)]\tTrain Loss: 0.000618\n",
      "Train Epoch: 140 [100/106 (94%)]\tTrain Loss: 0.000133\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 418/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.19233812e-06 3.67952347e-01 9.07356753e-06 3.85724263e-07\n",
      " 2.60923457e-06 8.55048583e-08 1.47528120e-03 5.97054509e-07\n",
      " 2.97060069e-07 2.99899648e-06 2.32074194e-06 4.69756145e-07\n",
      " 2.67131018e-05 1.20553045e-06 1.27067266e-04 7.14920702e-07\n",
      " 2.69374344e-04 6.46406625e-06 7.35815847e-05 9.99998808e-01\n",
      " 9.99937057e-01 9.98169184e-01 9.99973416e-01 9.99998689e-01\n",
      " 1.28039054e-03 1.00000000e+00 1.00000000e+00 3.85874318e-06\n",
      " 8.35873708e-02 1.54442150e-05 9.99892116e-01 2.68407399e-03\n",
      " 1.01185404e-03 4.12195914e-05 2.62755316e-06 5.37093911e-06\n",
      " 1.05133086e-05 2.43515660e-06 6.15649697e-06 6.61105901e-07\n",
      " 3.44533669e-06 6.32331478e-07 1.24453236e-05 1.84752639e-06\n",
      " 3.86444144e-02 5.74418266e-07 2.46405234e-05 5.49599121e-04\n",
      " 6.27850354e-01 2.72627985e-05 2.59589285e-01 2.36771882e-07\n",
      " 5.48560965e-05 3.66639028e-07 6.28820539e-01 6.22926973e-06\n",
      " 9.99516129e-01 2.61212250e-07 1.99875876e-06 2.83697760e-03\n",
      " 9.92614269e-01 9.91449878e-03 9.99957800e-01 9.99935508e-01\n",
      " 1.00000000e+00 2.66962161e-05 3.79352387e-05 9.99999642e-01\n",
      " 9.97518420e-01 9.99997020e-01 1.69785097e-04 1.61413811e-02\n",
      " 9.99392033e-01 9.99967694e-01 9.99999881e-01 9.99949813e-01\n",
      " 9.99995351e-01 9.99988437e-01 9.99979973e-01 9.99797642e-01\n",
      " 9.99930978e-01 9.99984980e-01 1.00000000e+00 9.99988198e-01\n",
      " 8.28002870e-01 9.82745230e-01 9.94418144e-01 9.99368250e-01\n",
      " 9.99927402e-01 1.49989937e-04 1.00000000e+00 4.93414609e-06\n",
      " 9.34001037e-06 9.99998808e-01 9.52086985e-01 9.84201610e-01\n",
      " 9.88482407e-06 4.35649276e-01 1.57769886e-04 9.99830484e-01\n",
      " 9.99269307e-01 9.98276472e-01 3.63786484e-07 1.00307996e-04\n",
      " 9.42112565e-01 1.27597872e-04 9.99505043e-01 9.99644279e-01\n",
      " 1.53954570e-05 6.61407039e-03 9.99962211e-01 1.32526597e-03\n",
      " 5.93858421e-01 9.97183084e-01 3.72190625e-05 1.66020039e-02]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 26 TN= 52 FN= 30 FP= 8\n",
      "TP+FP 34\n",
      "precision 0.7647058823529411\n",
      "recall 0.4642857142857143\n",
      "F1 0.5777777777777777\n",
      "acc 0.6724137931034483\n",
      "AUCp 0.6654761904761906\n",
      "AUC 0.8083333333333333\n",
      "\n",
      " The epoch is 140, average recall: 0.4643, average precision: 0.7647,average F1: 0.5778, average accuracy: 0.6724, average AUC: 0.8083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 141 [0/106 (0%)]\tTrain Loss: 0.000066\n",
      "Train Epoch: 141 [10/106 (9%)]\tTrain Loss: 0.000044\n",
      "Train Epoch: 141 [20/106 (19%)]\tTrain Loss: 0.000174\n",
      "Train Epoch: 141 [30/106 (28%)]\tTrain Loss: 0.000031\n",
      "Train Epoch: 141 [40/106 (38%)]\tTrain Loss: 0.000940\n",
      "Train Epoch: 141 [50/106 (47%)]\tTrain Loss: 0.000084\n",
      "Train Epoch: 141 [60/106 (57%)]\tTrain Loss: 0.001583\n",
      "Train Epoch: 141 [70/106 (66%)]\tTrain Loss: 0.000016\n",
      "Train Epoch: 141 [80/106 (75%)]\tTrain Loss: 0.000630\n",
      "Train Epoch: 141 [90/106 (85%)]\tTrain Loss: 0.000149\n",
      "Train Epoch: 141 [100/106 (94%)]\tTrain Loss: 0.000068\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [8.07538618e-06 4.56847541e-04 1.36789677e-05 8.91698710e-06\n",
      " 6.53722454e-05 6.57023008e-07 1.16404713e-04 1.73190692e-05\n",
      " 1.32457683e-06 2.80917857e-06 8.43126236e-06 6.31031469e-07\n",
      " 1.29935643e-05 5.69452141e-06 2.84567068e-04 2.11431907e-05\n",
      " 6.65169209e-05 1.47738683e-05 7.75259468e-06 7.55846947e-02\n",
      " 1.00377467e-04 6.28610898e-04 2.92527293e-05 9.99644995e-01\n",
      " 3.50845985e-05 9.96735394e-01 9.99999523e-01 9.92709101e-06\n",
      " 7.33500246e-06 1.65675337e-05 8.69893730e-01 1.27005245e-04\n",
      " 3.32264579e-03 2.04805019e-05 5.03048432e-06 1.50540063e-05\n",
      " 2.78329662e-05 2.98994437e-05 2.89418913e-05 1.21701842e-05\n",
      " 3.79511985e-05 8.29777036e-06 2.12999832e-04 2.14373013e-05\n",
      " 4.57207607e-05 8.22121024e-07 7.00363307e-05 3.08678078e-04\n",
      " 9.41915691e-01 1.24388444e-05 2.33398424e-03 8.86228918e-06\n",
      " 2.57189276e-06 1.21735627e-06 4.21191326e-05 5.99188979e-07\n",
      " 2.12911174e-01 5.74864316e-07 8.77989896e-06 2.83804166e-05\n",
      " 7.07810977e-03 5.94815414e-04 9.99622107e-01 9.99576390e-01\n",
      " 9.97548521e-01 1.14742397e-04 4.08666237e-05 9.99996781e-01\n",
      " 9.94903684e-01 9.99853134e-01 1.86626232e-04 7.14425929e-04\n",
      " 8.06503557e-03 9.97972786e-01 9.98946130e-01 6.56178892e-02\n",
      " 9.99887705e-01 9.99880075e-01 9.98646677e-01 9.97427404e-01\n",
      " 1.55304670e-01 9.59583044e-01 9.99991417e-01 9.98861074e-01\n",
      " 2.04234780e-03 6.59356499e-03 2.19980211e-04 6.68311783e-04\n",
      " 8.22322350e-03 6.59273646e-04 9.99908566e-01 8.61098488e-06\n",
      " 4.29467073e-05 9.96188104e-01 8.92818716e-05 1.50068472e-05\n",
      " 1.25183419e-06 1.66904682e-03 2.32549082e-05 9.79426444e-01\n",
      " 9.85462308e-01 9.97153163e-01 5.99526675e-06 1.15790754e-04\n",
      " 1.37012437e-04 5.75479426e-05 9.74304736e-01 8.38290974e-02\n",
      " 2.15962864e-05 1.85311856e-05 9.96373594e-01 7.25789927e-04\n",
      " 2.21405178e-04 3.60943581e-04 5.76029634e-05 4.17940527e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 142 [0/106 (0%)]\tTrain Loss: 0.000029\n",
      "Train Epoch: 142 [10/106 (9%)]\tTrain Loss: 0.000186\n",
      "Train Epoch: 142 [20/106 (19%)]\tTrain Loss: 0.000217\n",
      "Train Epoch: 142 [30/106 (28%)]\tTrain Loss: 0.000042\n",
      "Train Epoch: 142 [40/106 (38%)]\tTrain Loss: 0.000164\n",
      "Train Epoch: 142 [50/106 (47%)]\tTrain Loss: 0.189921\n",
      "Train Epoch: 142 [60/106 (57%)]\tTrain Loss: 0.000011\n",
      "Train Epoch: 142 [70/106 (66%)]\tTrain Loss: 0.000154\n",
      "Train Epoch: 142 [80/106 (75%)]\tTrain Loss: 0.009277\n",
      "Train Epoch: 142 [90/106 (85%)]\tTrain Loss: 0.037961\n",
      "Train Epoch: 142 [100/106 (94%)]\tTrain Loss: 0.000225\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 414/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.04794135e-05 5.91021264e-04 1.39492759e-05 9.97387269e-06\n",
      " 3.61651364e-05 6.70154179e-07 1.13063579e-04 1.19119959e-05\n",
      " 9.14045984e-07 5.32205149e-06 4.37500130e-05 7.17701710e-07\n",
      " 7.77968089e-05 5.76059210e-06 1.72940534e-04 8.65645870e-06\n",
      " 6.13034281e-05 5.03360825e-05 3.46790985e-05 1.82483241e-01\n",
      " 3.77766090e-04 1.36208991e-02 4.49612562e-05 9.37336624e-01\n",
      " 5.15220454e-05 1.06699076e-02 9.99468029e-01 2.39184465e-06\n",
      " 2.52152695e-05 8.93523975e-05 3.93618757e-05 3.22684355e-05\n",
      " 1.16599032e-04 6.40337457e-05 8.08619825e-06 1.52010980e-05\n",
      " 6.70733643e-05 6.75499687e-05 4.85603632e-05 1.33612975e-05\n",
      " 4.49067447e-05 1.05018044e-05 7.74164291e-05 2.96256658e-05\n",
      " 2.21261726e-05 5.88411012e-06 1.14012510e-04 6.47074601e-04\n",
      " 8.95611942e-03 2.23497409e-05 2.39669491e-04 5.57880185e-06\n",
      " 1.62060348e-06 2.39544920e-06 3.11460535e-05 1.21140465e-06\n",
      " 6.45756372e-05 2.66961320e-06 9.96833842e-06 1.68202168e-05\n",
      " 5.98086219e-04 8.62533692e-03 9.93889213e-01 9.94065166e-01\n",
      " 3.06872302e-04 2.64258979e-05 1.65816346e-05 9.89235222e-01\n",
      " 9.69008625e-01 9.95180249e-01 2.60522036e-04 1.18305825e-03\n",
      " 5.67661540e-04 1.60335854e-01 2.39687233e-05 2.81112407e-05\n",
      " 9.97198224e-01 9.94262338e-01 1.86885518e-04 7.73650780e-03\n",
      " 1.54945429e-03 1.61202363e-04 9.99213099e-01 9.78614151e-01\n",
      " 1.50063233e-02 1.01828206e-04 6.00395142e-05 9.62191552e-05\n",
      " 8.41681063e-02 1.97475520e-03 9.79294717e-01 4.07352236e-05\n",
      " 4.90236525e-05 9.34281707e-01 1.52736160e-04 1.82663844e-05\n",
      " 3.30186958e-06 5.72094345e-04 2.95921509e-05 5.95987253e-02\n",
      " 1.51002511e-01 9.47431326e-01 5.74521391e-06 4.17670672e-05\n",
      " 1.33598747e-04 4.75606248e-05 1.42803285e-02 8.10600668e-02\n",
      " 4.61740892e-05 6.22077932e-05 9.78598058e-01 1.83680118e-03\n",
      " 4.04643221e-03 3.57439071e-02 1.13460337e-05 7.87959434e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 143 [0/106 (0%)]\tTrain Loss: 0.000063\n",
      "Train Epoch: 143 [10/106 (9%)]\tTrain Loss: 0.000140\n",
      "Train Epoch: 143 [20/106 (19%)]\tTrain Loss: 0.009127\n",
      "Train Epoch: 143 [30/106 (28%)]\tTrain Loss: 0.015062\n",
      "Train Epoch: 143 [40/106 (38%)]\tTrain Loss: 0.000021\n",
      "Train Epoch: 143 [50/106 (47%)]\tTrain Loss: 0.000100\n",
      "Train Epoch: 143 [60/106 (57%)]\tTrain Loss: 0.012237\n",
      "Train Epoch: 143 [70/106 (66%)]\tTrain Loss: 0.000023\n",
      "Train Epoch: 143 [80/106 (75%)]\tTrain Loss: 0.000011\n",
      "Train Epoch: 143 [90/106 (85%)]\tTrain Loss: 0.000187\n",
      "Train Epoch: 143 [100/106 (94%)]\tTrain Loss: 0.000006\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 418/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.67909070e-06 1.16758456e-04 1.50305950e-05 3.93479331e-06\n",
      " 1.04728388e-05 6.29987653e-07 9.57443917e-05 7.57407679e-06\n",
      " 2.30983332e-07 6.53171037e-06 6.14287319e-06 6.66763299e-07\n",
      " 7.11267967e-06 5.81970835e-06 8.94778204e-05 4.76810328e-06\n",
      " 1.34128468e-05 5.69419426e-05 5.43533424e-05 2.55222946e-01\n",
      " 3.06829989e-01 2.20922872e-01 8.82717669e-01 9.24913347e-01\n",
      " 4.27684681e-05 9.92117226e-01 9.89547610e-01 9.08146194e-06\n",
      " 2.05415708e-05 1.74078396e-05 9.24920314e-04 5.78283980e-05\n",
      " 1.19041397e-04 1.94967633e-05 2.93600260e-06 3.28585543e-06\n",
      " 7.75755780e-06 2.85401966e-05 4.77014291e-05 8.57887881e-06\n",
      " 2.39326364e-05 4.66302845e-06 4.01828438e-05 1.12537246e-05\n",
      " 6.80414378e-05 9.18187015e-06 9.73826609e-05 4.82128351e-04\n",
      " 4.85586256e-01 1.17626796e-05 1.83742133e-03 1.35323262e-06\n",
      " 9.34537366e-06 2.09222935e-06 7.18778829e-05 1.51410802e-06\n",
      " 1.78780771e-04 7.08895243e-07 6.45188447e-06 6.23667584e-05\n",
      " 1.52483379e-04 1.67104357e-04 9.82469320e-01 9.76184368e-01\n",
      " 9.93123114e-01 2.99115945e-05 3.01761029e-05 9.99857903e-01\n",
      " 7.53854275e-01 9.96239066e-01 8.76275881e-05 9.78800017e-05\n",
      " 1.58560730e-03 8.77800405e-01 8.69417369e-01 1.59617310e-04\n",
      " 9.97611165e-01 9.94718552e-01 9.03910339e-01 4.42235798e-01\n",
      " 3.78083676e-01 9.25620317e-01 9.99643564e-01 9.65586603e-01\n",
      " 1.25231236e-01 1.21512567e-03 1.31432098e-04 2.73646525e-04\n",
      " 5.42083979e-01 9.62934864e-05 9.96376812e-01 1.74221059e-05\n",
      " 4.74956760e-05 9.44719017e-01 2.85932074e-05 1.79360432e-05\n",
      " 7.53624363e-06 2.47131765e-01 1.22195179e-05 1.18504453e-04\n",
      " 1.71062397e-03 8.64252567e-01 2.33057585e-06 2.66078951e-05\n",
      " 1.84766774e-04 1.13732807e-04 5.64782500e-01 2.39182591e-05\n",
      " 8.91452055e-06 1.21192006e-05 1.77671146e-02 9.10216331e-05\n",
      " 2.67513416e-04 1.09865869e-04 1.08735048e-05 8.87675851e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 144 [0/106 (0%)]\tTrain Loss: 0.000093\n",
      "Train Epoch: 144 [10/106 (9%)]\tTrain Loss: 0.000005\n",
      "Train Epoch: 144 [20/106 (19%)]\tTrain Loss: 0.000023\n",
      "Train Epoch: 144 [30/106 (28%)]\tTrain Loss: 0.000084\n",
      "Train Epoch: 144 [40/106 (38%)]\tTrain Loss: 0.000048\n",
      "Train Epoch: 144 [50/106 (47%)]\tTrain Loss: 0.000015\n",
      "Train Epoch: 144 [60/106 (57%)]\tTrain Loss: 0.000287\n",
      "Train Epoch: 144 [70/106 (66%)]\tTrain Loss: 0.000012\n",
      "Train Epoch: 144 [80/106 (75%)]\tTrain Loss: 0.000019\n",
      "Train Epoch: 144 [90/106 (85%)]\tTrain Loss: 0.000021\n",
      "Train Epoch: 144 [100/106 (94%)]\tTrain Loss: 0.000166\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 420/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.01013677e-06 8.92738163e-01 6.44937472e-06 5.18828210e-06\n",
      " 1.09775619e-05 3.65407487e-07 1.28503380e-04 5.76907996e-06\n",
      " 8.67963834e-08 3.50119171e-06 4.04401590e-06 3.80081843e-07\n",
      " 8.23743903e-06 9.72875478e-06 3.62104326e-02 6.48386094e-06\n",
      " 5.89244919e-05 1.38180010e-04 3.83089646e-05 9.95823860e-01\n",
      " 9.40111458e-01 9.95648563e-01 9.96365905e-01 9.93890882e-01\n",
      " 2.22087675e-03 9.99538541e-01 9.99975204e-01 1.73891567e-06\n",
      " 1.60270557e-02 5.76899292e-05 5.88012099e-01 6.03629334e-04\n",
      " 2.35056490e-04 2.59217104e-05 1.54425106e-06 1.28637055e-06\n",
      " 2.04713820e-06 2.45738775e-05 1.41929218e-03 3.75117488e-05\n",
      " 4.64495446e-04 3.31638284e-06 3.88636363e-05 5.25113182e-06\n",
      " 5.03384836e-05 1.42494730e-06 1.97301444e-04 2.38440149e-02\n",
      " 9.87928987e-01 1.00111911e-05 9.89555120e-01 5.34157095e-07\n",
      " 8.68139341e-06 1.36166159e-06 5.19511476e-03 2.68662319e-07\n",
      " 8.83913815e-01 3.47865864e-07 2.25572262e-06 1.43131925e-04\n",
      " 9.97478902e-01 9.98834312e-01 9.99943137e-01 9.99810278e-01\n",
      " 9.98653769e-01 1.60269083e-05 1.30216160e-03 9.99922633e-01\n",
      " 9.57978010e-01 9.99525189e-01 8.92710030e-01 9.54357743e-01\n",
      " 9.86552298e-01 9.98831332e-01 9.90338862e-01 6.59560487e-02\n",
      " 9.99886394e-01 9.99972224e-01 9.99989510e-01 9.98050451e-01\n",
      " 9.95851278e-01 9.98081207e-01 9.99983311e-01 9.99186099e-01\n",
      " 9.80186999e-01 9.56672072e-01 4.01325116e-04 4.33848763e-04\n",
      " 9.98875439e-01 2.96930462e-04 9.99694109e-01 3.86964748e-05\n",
      " 5.96097598e-05 9.97163355e-01 9.47295339e-05 6.83723119e-05\n",
      " 6.35699098e-06 9.96604085e-01 8.22602630e-01 4.34955955e-02\n",
      " 9.97034550e-01 9.98597085e-01 6.29899023e-06 8.88285984e-04\n",
      " 9.96410549e-01 5.06056997e-04 9.97974098e-01 9.65125582e-05\n",
      " 2.05500364e-05 3.30206560e-04 9.94214237e-01 6.27246723e-02\n",
      " 2.54105441e-02 8.72692645e-01 9.86497267e-04 9.97961521e-01]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
      "Train Epoch: 145 [0/106 (0%)]\tTrain Loss: 0.007853\n",
      "Train Epoch: 145 [10/106 (9%)]\tTrain Loss: 0.000009\n",
      "Train Epoch: 145 [20/106 (19%)]\tTrain Loss: 0.000005\n",
      "Train Epoch: 145 [30/106 (28%)]\tTrain Loss: 0.000005\n",
      "Train Epoch: 145 [40/106 (38%)]\tTrain Loss: 0.000090\n",
      "Train Epoch: 145 [50/106 (47%)]\tTrain Loss: 0.000033\n",
      "Train Epoch: 145 [60/106 (57%)]\tTrain Loss: 0.000083\n",
      "Train Epoch: 145 [70/106 (66%)]\tTrain Loss: 0.000010\n",
      "Train Epoch: 145 [80/106 (75%)]\tTrain Loss: 0.000080\n",
      "Train Epoch: 145 [90/106 (85%)]\tTrain Loss: 0.000085\n",
      "Train Epoch: 145 [100/106 (94%)]\tTrain Loss: 0.003652\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 421/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.05383765e-06 2.50361889e-04 3.17178456e-06 1.82431882e-06\n",
      " 5.57502517e-06 1.06166596e-07 6.14074597e-05 2.80513609e-06\n",
      " 9.34584179e-08 3.12309930e-06 2.48119477e-06 2.09049119e-07\n",
      " 1.17105742e-06 1.70350063e-06 3.00520387e-05 1.11397162e-06\n",
      " 7.93090203e-06 3.69942677e-06 2.52835343e-05 9.95846570e-01\n",
      " 8.72183621e-01 9.80935693e-01 9.57882285e-01 9.95513856e-01\n",
      " 4.97218862e-05 9.98013735e-01 9.99938846e-01 3.03112046e-07\n",
      " 1.04630553e-05 8.03088096e-06 1.69957129e-04 2.90965636e-05\n",
      " 1.51023214e-05 8.37799325e-06 7.53504708e-07 1.44086437e-06\n",
      " 1.92334142e-06 5.13822033e-06 4.12607260e-05 2.60239563e-06\n",
      " 9.48148408e-06 1.34305276e-06 1.10137908e-05 1.62562799e-06\n",
      " 9.39168876e-06 5.01162788e-07 3.42329549e-05 4.68282320e-04\n",
      " 9.74928558e-01 3.00506599e-06 1.22850247e-01 4.58050522e-07\n",
      " 3.27917815e-06 4.61433984e-07 1.97864356e-04 1.74272770e-07\n",
      " 2.17278255e-04 1.39460496e-07 1.72358102e-06 4.30614964e-05\n",
      " 7.93841124e-01 7.34660566e-01 9.99773800e-01 9.99530792e-01\n",
      " 9.98637497e-01 8.88159684e-06 2.95557111e-05 9.99881506e-01\n",
      " 9.91865456e-01 9.99403834e-01 8.63266084e-03 3.41955125e-02\n",
      " 6.48940980e-01 9.98967409e-01 2.08183806e-02 3.30249459e-05\n",
      " 9.99903083e-01 9.99946356e-01 7.76280765e-04 6.64528966e-01\n",
      " 9.71663475e-01 9.94016886e-01 9.99977112e-01 9.96990681e-01\n",
      " 2.47406617e-01 8.76771733e-02 4.41653865e-05 4.43502904e-05\n",
      " 9.97860968e-01 5.90681739e-05 9.99608815e-01 6.45080218e-06\n",
      " 1.18843673e-05 9.97850180e-01 1.38815440e-05 1.57200866e-05\n",
      " 2.44875673e-06 9.85952973e-01 2.45839110e-05 1.13373972e-04\n",
      " 8.66036355e-01 9.96548951e-01 1.04142305e-06 8.27025451e-06\n",
      " 5.81389785e-01 7.77457099e-05 9.96708632e-01 1.59538031e-05\n",
      " 5.94990479e-06 6.83409644e-06 3.38729441e-01 4.37124370e-04\n",
      " 3.69213696e-04 1.39055162e-04 4.64546065e-06 3.34808174e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 146 [0/106 (0%)]\tTrain Loss: 0.000017\n",
      "Train Epoch: 146 [10/106 (9%)]\tTrain Loss: 0.000123\n",
      "Train Epoch: 146 [20/106 (19%)]\tTrain Loss: 0.000191\n",
      "Train Epoch: 146 [30/106 (28%)]\tTrain Loss: 0.000113\n",
      "Train Epoch: 146 [40/106 (38%)]\tTrain Loss: 0.016197\n",
      "Train Epoch: 146 [50/106 (47%)]\tTrain Loss: 0.000205\n",
      "Train Epoch: 146 [60/106 (57%)]\tTrain Loss: 0.009864\n",
      "Train Epoch: 146 [70/106 (66%)]\tTrain Loss: 0.000097\n",
      "Train Epoch: 146 [80/106 (75%)]\tTrain Loss: 0.000065\n",
      "Train Epoch: 146 [90/106 (85%)]\tTrain Loss: 0.000032\n",
      "Train Epoch: 146 [100/106 (94%)]\tTrain Loss: 0.000140\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.38547523e-06 1.84834679e-03 5.20199092e-06 4.46477907e-06\n",
      " 9.43146824e-06 1.75840896e-07 2.02086827e-04 7.60986450e-06\n",
      " 8.57519709e-08 1.35978471e-05 4.62383377e-06 5.55068425e-07\n",
      " 2.25623467e-06 3.93418213e-06 1.27397987e-04 3.90413516e-06\n",
      " 4.12508489e-05 1.26953828e-05 1.08827364e-04 9.98464346e-01\n",
      " 9.82186675e-01 9.96835947e-01 9.97428119e-01 9.97733951e-01\n",
      " 2.14248872e-03 9.99174178e-01 9.99888301e-01 2.24853920e-07\n",
      " 1.86494127e-01 6.59422003e-05 1.10153761e-03 5.83623769e-04\n",
      " 9.92863279e-05 2.09507016e-05 1.21641972e-06 1.95100438e-06\n",
      " 3.46892307e-06 1.87868627e-05 1.37329771e-04 5.94194989e-06\n",
      " 4.21918921e-05 2.64852724e-06 4.28712883e-05 3.10617429e-06\n",
      " 6.65463667e-05 1.13613441e-06 2.03950563e-04 1.53682129e-02\n",
      " 9.90856171e-01 8.42710597e-06 9.10093129e-01 8.53439360e-07\n",
      " 1.16338615e-05 1.18326932e-06 1.21256302e-03 2.39514009e-07\n",
      " 9.89403576e-03 1.03718925e-07 3.36323501e-06 2.17133289e-04\n",
      " 4.63876456e-01 1.49489075e-01 9.99737561e-01 9.99525428e-01\n",
      " 9.99032021e-01 4.08761371e-05 8.10394995e-04 9.99861479e-01\n",
      " 9.96927083e-01 9.99278724e-01 5.59419356e-02 5.57704754e-02\n",
      " 9.78643477e-01 9.99133527e-01 9.36309457e-01 4.04500577e-04\n",
      " 9.99871254e-01 9.99956369e-01 9.98983562e-01 9.96698022e-01\n",
      " 9.95452225e-01 9.97003734e-01 9.99963403e-01 9.98648345e-01\n",
      " 9.86568809e-01 9.81334925e-01 1.14519076e-04 1.36294722e-04\n",
      " 9.98710632e-01 3.30290903e-04 9.99366224e-01 5.53197497e-05\n",
      " 5.91687531e-05 9.98907685e-01 7.55328219e-05 9.13993717e-05\n",
      " 7.47969943e-06 9.92404997e-01 9.12976320e-05 8.06028314e-04\n",
      " 9.63895380e-01 9.96441782e-01 2.39591986e-06 7.64422148e-05\n",
      " 9.91749287e-01 3.30921932e-04 9.98161256e-01 6.15610770e-05\n",
      " 1.91530853e-05 1.60149757e-05 9.69292462e-01 9.78206517e-04\n",
      " 1.67644257e-03 9.16017830e-01 3.26414774e-05 9.64876115e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 147 [0/106 (0%)]\tTrain Loss: 0.000303\n",
      "Train Epoch: 147 [10/106 (9%)]\tTrain Loss: 0.000136\n",
      "Train Epoch: 147 [20/106 (19%)]\tTrain Loss: 0.000059\n",
      "Train Epoch: 147 [30/106 (28%)]\tTrain Loss: 0.000077\n",
      "Train Epoch: 147 [40/106 (38%)]\tTrain Loss: 0.000019\n",
      "Train Epoch: 147 [50/106 (47%)]\tTrain Loss: 0.014883\n",
      "Train Epoch: 147 [60/106 (57%)]\tTrain Loss: 0.000214\n",
      "Train Epoch: 147 [70/106 (66%)]\tTrain Loss: 0.000056\n",
      "Train Epoch: 147 [80/106 (75%)]\tTrain Loss: 0.000010\n",
      "Train Epoch: 147 [90/106 (85%)]\tTrain Loss: 0.000009\n",
      "Train Epoch: 147 [100/106 (94%)]\tTrain Loss: 0.000140\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 424/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [4.37370682e-06 4.68077837e-03 2.22699982e-05 1.04437759e-05\n",
      " 2.53744292e-05 4.56024196e-07 5.53446589e-04 1.70246349e-05\n",
      " 3.54405017e-07 1.78602204e-05 1.39379463e-05 9.97782081e-07\n",
      " 6.62468074e-06 1.19121896e-05 3.74937867e-04 1.21588855e-05\n",
      " 1.44873164e-04 7.03445112e-05 2.66743038e-04 9.99373972e-01\n",
      " 9.87435579e-01 9.98987615e-01 9.99282658e-01 9.98722732e-01\n",
      " 7.57071096e-03 9.99684930e-01 9.99957204e-01 1.33263404e-06\n",
      " 5.35898609e-03 1.27065941e-04 1.74648850e-03 1.59484649e-03\n",
      " 2.53162900e-04 6.59345969e-05 3.97942222e-06 7.76865545e-06\n",
      " 1.93295346e-05 4.25377148e-05 2.55257939e-04 3.10967225e-05\n",
      " 1.29403474e-04 1.02138420e-05 9.51066977e-05 1.51328277e-05\n",
      " 1.20432633e-04 3.16641103e-06 2.33842467e-04 5.50465658e-03\n",
      " 9.90231216e-01 1.38038413e-05 5.01419902e-01 3.08963877e-06\n",
      " 2.52734098e-05 2.95393784e-06 2.84218509e-03 1.41940541e-06\n",
      " 3.71043265e-01 7.77945559e-07 9.44650219e-06 3.98776407e-04\n",
      " 8.76969337e-01 6.28138185e-01 9.99881268e-01 9.99778450e-01\n",
      " 9.99709666e-01 1.15314084e-04 1.88661274e-03 9.99934196e-01\n",
      " 9.98029172e-01 9.99686956e-01 3.15228216e-02 2.38429800e-01\n",
      " 9.89678144e-01 9.99546945e-01 9.94614780e-01 8.72457749e-04\n",
      " 9.99960184e-01 9.99979973e-01 9.99562681e-01 9.98545170e-01\n",
      " 9.98179674e-01 9.98712540e-01 9.99979973e-01 9.99465048e-01\n",
      " 9.96264160e-01 9.89257991e-01 3.99346725e-04 5.64151327e-04\n",
      " 9.99346077e-01 9.12255025e-04 9.99703109e-01 6.68847570e-05\n",
      " 1.16877316e-04 9.99430001e-01 2.25482741e-04 2.11180028e-04\n",
      " 1.69177365e-05 9.97277081e-01 1.58042740e-03 3.97675112e-03\n",
      " 9.94167686e-01 9.98897910e-01 7.86571854e-06 1.73309483e-04\n",
      " 9.92726564e-01 9.73855436e-04 9.99343455e-01 2.24066156e-04\n",
      " 6.12045405e-05 5.34205756e-05 9.96106088e-01 4.01315140e-03\n",
      " 8.50716699e-03 9.75072384e-01 8.85309855e-05 9.96020854e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
      "Train Epoch: 148 [0/106 (0%)]\tTrain Loss: 0.000020\n",
      "Train Epoch: 148 [10/106 (9%)]\tTrain Loss: 0.000012\n",
      "Train Epoch: 148 [20/106 (19%)]\tTrain Loss: 0.057569\n",
      "Train Epoch: 148 [30/106 (28%)]\tTrain Loss: 0.000079\n",
      "Train Epoch: 148 [40/106 (38%)]\tTrain Loss: 0.000125\n",
      "Train Epoch: 148 [50/106 (47%)]\tTrain Loss: 0.000126\n",
      "Train Epoch: 148 [60/106 (57%)]\tTrain Loss: 0.016713\n",
      "Train Epoch: 148 [70/106 (66%)]\tTrain Loss: 0.000011\n",
      "Train Epoch: 148 [80/106 (75%)]\tTrain Loss: 0.000064\n",
      "Train Epoch: 148 [90/106 (85%)]\tTrain Loss: 0.000014\n",
      "Train Epoch: 148 [100/106 (94%)]\tTrain Loss: 0.000281\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 421/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.17894217e-06 2.16280972e-03 1.51033591e-05 5.11051076e-06\n",
      " 1.26480463e-05 2.49680369e-07 1.79157578e-04 8.05186210e-06\n",
      " 2.35322631e-07 7.42804286e-06 1.28617730e-05 4.92271113e-07\n",
      " 6.17693422e-06 5.62198329e-06 1.13513081e-04 6.24135919e-06\n",
      " 5.05667304e-05 3.08549679e-05 1.11629233e-04 9.99300480e-01\n",
      " 9.77878571e-01 9.97808993e-01 9.98236895e-01 9.97610807e-01\n",
      " 1.67434185e-03 9.99603212e-01 9.99959588e-01 9.93666617e-07\n",
      " 4.89906920e-03 6.51367154e-05 3.99775978e-04 4.61727759e-04\n",
      " 9.85423103e-05 3.97899275e-05 3.00830038e-06 6.48468358e-06\n",
      " 1.47120900e-05 2.03821073e-05 1.14347866e-04 1.62159176e-05\n",
      " 6.38184938e-05 8.08215100e-06 5.13045015e-05 1.22019119e-05\n",
      " 4.94547967e-05 1.98312682e-06 1.04650004e-04 2.52111838e-03\n",
      " 9.81566787e-01 1.02158874e-05 3.15091789e-01 1.94640847e-06\n",
      " 1.23633672e-05 2.06980985e-06 1.83624798e-03 1.01107764e-06\n",
      " 1.30884815e-02 4.87557372e-07 4.27907389e-06 1.77883485e-04\n",
      " 9.34180379e-01 7.51519501e-01 9.99900222e-01 9.99719560e-01\n",
      " 9.99686480e-01 3.92954898e-05 4.33183624e-04 9.99947071e-01\n",
      " 9.93179917e-01 9.99592602e-01 1.88446622e-02 1.00855179e-01\n",
      " 9.77951407e-01 9.99320388e-01 9.74506319e-01 1.81963100e-04\n",
      " 9.99959707e-01 9.99985456e-01 9.98314023e-01 9.96064246e-01\n",
      " 9.94466245e-01 9.97179508e-01 9.99984622e-01 9.99210000e-01\n",
      " 9.90555465e-01 9.55301642e-01 1.87459460e-04 2.31903658e-04\n",
      " 9.99391079e-01 2.05013348e-04 9.99666214e-01 3.24598586e-05\n",
      " 4.47097591e-05 9.99100804e-01 7.91207567e-05 8.61472436e-05\n",
      " 8.84995006e-06 9.93027151e-01 1.56504393e-04 1.55503396e-03\n",
      " 9.86914694e-01 9.97204185e-01 5.69645499e-06 6.15666577e-05\n",
      " 9.90498722e-01 1.97001034e-04 9.98364866e-01 1.23447287e-04\n",
      " 3.91803915e-05 2.69449283e-05 9.93969321e-01 1.23132358e-03\n",
      " 2.67436146e-03 9.57065046e-01 2.98462583e-05 9.24813747e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
      "Train Epoch: 149 [0/106 (0%)]\tTrain Loss: 0.000067\n",
      "Train Epoch: 149 [10/106 (9%)]\tTrain Loss: 0.000070\n",
      "Train Epoch: 149 [20/106 (19%)]\tTrain Loss: 0.000008\n",
      "Train Epoch: 149 [30/106 (28%)]\tTrain Loss: 0.000125\n",
      "Train Epoch: 149 [40/106 (38%)]\tTrain Loss: 0.000005\n",
      "Train Epoch: 149 [50/106 (47%)]\tTrain Loss: 0.009999\n",
      "Train Epoch: 149 [60/106 (57%)]\tTrain Loss: 0.000008\n",
      "Train Epoch: 149 [70/106 (66%)]\tTrain Loss: 0.000521\n",
      "Train Epoch: 149 [80/106 (75%)]\tTrain Loss: 0.000162\n",
      "Train Epoch: 149 [90/106 (85%)]\tTrain Loss: 0.000006\n",
      "Train Epoch: 149 [100/106 (94%)]\tTrain Loss: 0.000008\n",
      "\n",
      "Train set: Average loss: 0.0002, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.01425291e-06 2.63765827e-03 7.05522962e-06 2.93272001e-06\n",
      " 7.95033429e-06 1.60068311e-07 5.08817611e-04 6.17288288e-06\n",
      " 1.18892565e-07 7.00840837e-06 3.23462928e-06 3.60263527e-07\n",
      " 1.82572865e-06 3.55477732e-06 1.99014918e-04 2.97278211e-06\n",
      " 3.35453769e-05 3.21920270e-05 9.64637948e-05 9.96464849e-01\n",
      " 1.50812879e-01 9.91604149e-01 9.91834641e-01 9.93434668e-01\n",
      " 6.92528149e-04 9.97364700e-01 9.99379396e-01 3.61655623e-07\n",
      " 1.18283977e-04 6.27844565e-05 8.21483321e-04 7.45917379e-04\n",
      " 2.88854731e-04 2.07734793e-05 1.25765564e-06 1.26617840e-06\n",
      " 2.95085738e-06 1.51100139e-05 9.32273251e-05 7.54228950e-06\n",
      " 3.94460840e-05 2.23616485e-06 3.32436648e-05 3.59262117e-06\n",
      " 3.80564052e-05 1.40742100e-06 1.46585226e-04 2.37660808e-03\n",
      " 8.05179894e-01 4.20678725e-06 4.45967261e-03 5.39303187e-07\n",
      " 5.57094882e-06 4.95964116e-07 3.95156821e-04 2.67461559e-07\n",
      " 1.23011565e-03 1.43408386e-07 2.28656450e-06 1.07218300e-04\n",
      " 3.15121487e-02 7.69175962e-03 9.98835504e-01 9.98188555e-01\n",
      " 9.97485280e-01 3.07350056e-05 3.05622088e-04 9.99573648e-01\n",
      " 9.92790222e-01 9.98360932e-01 1.59445312e-02 1.36885464e-01\n",
      " 9.69177246e-01 9.97400522e-01 9.62458670e-01 3.63916042e-04\n",
      " 9.99643087e-01 9.99684215e-01 9.98273492e-01 9.92167652e-01\n",
      " 9.91059721e-01 9.93470669e-01 9.99786079e-01 9.95808721e-01\n",
      " 9.51888740e-01 9.78588343e-01 1.64488505e-04 2.61224690e-04\n",
      " 9.95605648e-01 4.78016940e-04 9.97937441e-01 4.49865147e-05\n",
      " 7.45088182e-05 9.96709228e-01 9.78189782e-05 4.63325632e-05\n",
      " 3.91735648e-06 9.85038221e-01 4.27880077e-05 1.98986125e-03\n",
      " 9.43486273e-01 9.93230939e-01 2.08253914e-06 8.10049314e-05\n",
      " 9.64943409e-01 3.31698015e-04 9.96819854e-01 8.78496357e-05\n",
      " 1.10379633e-05 2.15710916e-05 9.03113544e-01 8.14829662e-04\n",
      " 1.37174397e-03 1.74627766e-01 2.87830771e-05 8.96732390e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 150 [0/106 (0%)]\tTrain Loss: 0.010873\n",
      "Train Epoch: 150 [10/106 (9%)]\tTrain Loss: 0.000169\n",
      "Train Epoch: 150 [20/106 (19%)]\tTrain Loss: 0.249118\n",
      "Train Epoch: 150 [30/106 (28%)]\tTrain Loss: 0.010923\n",
      "Train Epoch: 150 [40/106 (38%)]\tTrain Loss: 0.082646\n",
      "Train Epoch: 150 [50/106 (47%)]\tTrain Loss: 0.000004\n",
      "Train Epoch: 150 [60/106 (57%)]\tTrain Loss: 0.000085\n",
      "Train Epoch: 150 [70/106 (66%)]\tTrain Loss: 0.000062\n",
      "Train Epoch: 150 [80/106 (75%)]\tTrain Loss: 0.000006\n",
      "Train Epoch: 150 [90/106 (85%)]\tTrain Loss: 0.009028\n",
      "Train Epoch: 150 [100/106 (94%)]\tTrain Loss: 0.000191\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.57803095e-06 1.71572831e-03 9.78406115e-06 3.31817046e-06\n",
      " 8.64294634e-06 1.92605469e-07 2.00694310e-04 5.83961173e-06\n",
      " 1.25093635e-07 5.20122194e-06 4.87036868e-06 3.74749874e-07\n",
      " 2.60746333e-06 4.39573523e-06 1.16708710e-04 4.01326452e-06\n",
      " 3.35421137e-05 5.75053818e-05 6.98724398e-05 9.96877193e-01\n",
      " 4.81418133e-01 9.94849265e-01 9.95600104e-01 9.94305074e-01\n",
      " 1.19344483e-03 9.98869240e-01 9.99814928e-01 3.49234710e-07\n",
      " 1.18595688e-03 4.01813086e-05 2.38915440e-04 2.57025298e-04\n",
      " 1.10576373e-04 2.51168221e-05 1.51450081e-06 2.50744165e-06\n",
      " 6.48481318e-06 1.69512623e-05 9.60064281e-05 9.35256867e-06\n",
      " 4.42794262e-05 4.26022507e-06 4.08252599e-05 6.94135952e-06\n",
      " 3.84278828e-05 1.45708168e-06 1.15362462e-04 1.92113628e-03\n",
      " 9.60658908e-01 6.79323693e-06 6.52669221e-02 1.02999797e-06\n",
      " 8.44959777e-06 1.14484442e-06 2.93131539e-04 4.35264212e-07\n",
      " 4.72828746e-04 2.12224364e-07 2.76266087e-06 1.31193578e-04\n",
      " 6.92679524e-01 2.69608349e-01 9.99465764e-01 9.98785794e-01\n",
      " 9.98309374e-01 3.19688806e-05 4.40357864e-04 9.99775469e-01\n",
      " 9.84782994e-01 9.99019623e-01 5.43037523e-03 4.81144488e-02\n",
      " 9.17718232e-01 9.98273730e-01 9.04023707e-01 1.44605132e-04\n",
      " 9.99912024e-01 9.99952555e-01 9.98791873e-01 9.94382143e-01\n",
      " 9.89629328e-01 9.93073046e-01 9.99952674e-01 9.97426212e-01\n",
      " 9.78627264e-01 9.47273612e-01 1.25938328e-04 2.26927790e-04\n",
      " 9.96897340e-01 2.21597380e-04 9.98663425e-01 2.49119566e-05\n",
      " 5.16396540e-05 9.97633219e-01 5.31642327e-05 3.55450320e-05\n",
      " 5.37061669e-06 9.83242929e-01 5.16245345e-05 4.12043242e-04\n",
      " 9.56622005e-01 9.94161427e-01 3.08415133e-06 5.38078166e-05\n",
      " 9.74563956e-01 1.30015338e-04 9.95578945e-01 1.04245824e-04\n",
      " 2.02618921e-05 1.48887302e-05 9.62216914e-01 6.17267448e-04\n",
      " 1.16932858e-03 9.44342971e-01 2.55370287e-05 9.04226542e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 29 TN= 53 FN= 27 FP= 7\n",
      "TP+FP 36\n",
      "precision 0.8055555555555556\n",
      "recall 0.5178571428571429\n",
      "F1 0.6304347826086957\n",
      "acc 0.7068965517241379\n",
      "AUCp 0.700595238095238\n",
      "AUC 0.8104166666666667\n",
      "\n",
      " The epoch is 150, average recall: 0.5179, average precision: 0.8056,average F1: 0.6304, average accuracy: 0.7069, average AUC: 0.8104\n",
      "Train Epoch: 151 [0/106 (0%)]\tTrain Loss: 0.000082\n",
      "Train Epoch: 151 [10/106 (9%)]\tTrain Loss: 0.008703\n",
      "Train Epoch: 151 [20/106 (19%)]\tTrain Loss: 0.000018\n",
      "Train Epoch: 151 [30/106 (28%)]\tTrain Loss: 0.000075\n",
      "Train Epoch: 151 [40/106 (38%)]\tTrain Loss: 0.000284\n",
      "Train Epoch: 151 [50/106 (47%)]\tTrain Loss: 0.000022\n",
      "Train Epoch: 151 [60/106 (57%)]\tTrain Loss: 0.000020\n",
      "Train Epoch: 151 [70/106 (66%)]\tTrain Loss: 0.000008\n",
      "Train Epoch: 151 [80/106 (75%)]\tTrain Loss: 0.113777\n",
      "Train Epoch: 151 [90/106 (85%)]\tTrain Loss: 0.000036\n",
      "Train Epoch: 151 [100/106 (94%)]\tTrain Loss: 0.000069\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [4.34972094e-07 2.80809891e-03 7.57083171e-07 6.46032959e-07\n",
      " 1.11749353e-06 3.71427333e-08 7.22511031e-05 1.24051326e-06\n",
      " 1.83925994e-08 1.15462035e-06 4.93330390e-07 7.06277064e-08\n",
      " 2.68989453e-07 6.36948869e-07 2.65328636e-05 4.58463518e-07\n",
      " 4.95485529e-06 7.06751280e-06 1.33518834e-05 9.97912586e-01\n",
      " 1.77237280e-02 9.95956123e-01 9.90009606e-01 9.96663392e-01\n",
      " 2.11882711e-04 9.98685181e-01 9.99705732e-01 4.63560852e-08\n",
      " 4.27697341e-05 1.01789728e-05 1.57154893e-04 9.87752428e-05\n",
      " 3.30926632e-05 2.65837366e-06 1.47240371e-07 1.44756484e-07\n",
      " 3.09154672e-07 3.63404433e-06 2.65272975e-05 1.41253474e-06\n",
      " 1.57173890e-05 5.49370782e-07 6.47959541e-06 5.40897361e-07\n",
      " 7.89819114e-06 1.44056600e-07 2.86999712e-05 1.22342608e-03\n",
      " 9.69153166e-01 1.28554905e-06 9.45008695e-02 7.19313391e-08\n",
      " 1.60451020e-06 1.33175192e-07 1.01995458e-04 3.76695510e-08\n",
      " 1.20091456e-04 1.66982979e-08 4.10224828e-07 2.97878232e-05\n",
      " 8.08773160e-01 4.37845588e-01 9.99611676e-01 9.99356568e-01\n",
      " 9.98951674e-01 7.09014148e-06 9.57284283e-05 9.99730885e-01\n",
      " 9.95492220e-01 9.99160886e-01 4.55492875e-03 2.45120581e-02\n",
      " 9.70096409e-01 9.98778641e-01 9.26295936e-01 8.65306647e-05\n",
      " 9.99754965e-01 9.99801099e-01 9.97773826e-01 9.93641913e-01\n",
      " 9.90694404e-01 9.95263577e-01 9.99901891e-01 9.98403251e-01\n",
      " 9.79021907e-01 8.62107992e-01 4.01155303e-05 4.47869534e-05\n",
      " 9.98317599e-01 8.29645069e-05 9.98720050e-01 7.36507263e-06\n",
      " 1.05952977e-05 9.98425126e-01 1.17104119e-05 1.07271244e-05\n",
      " 1.07913945e-06 9.91008341e-01 1.19869446e-05 3.34705546e-04\n",
      " 9.61444378e-01 9.96320724e-01 2.87838674e-07 1.38819951e-05\n",
      " 9.81521726e-01 3.81230930e-05 9.97232497e-01 2.34767103e-05\n",
      " 2.51787742e-06 3.20116419e-06 9.82611239e-01 3.04714951e-04\n",
      " 3.03184905e-04 4.98475254e-01 5.06728929e-06 9.18863714e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "Train Epoch: 152 [0/106 (0%)]\tTrain Loss: 0.056413\n",
      "Train Epoch: 152 [10/106 (9%)]\tTrain Loss: 0.009660\n",
      "Train Epoch: 152 [20/106 (19%)]\tTrain Loss: 0.000016\n",
      "Train Epoch: 152 [30/106 (28%)]\tTrain Loss: 0.000067\n",
      "Train Epoch: 152 [40/106 (38%)]\tTrain Loss: 0.000064\n",
      "Train Epoch: 152 [50/106 (47%)]\tTrain Loss: 0.000091\n",
      "Train Epoch: 152 [60/106 (57%)]\tTrain Loss: 0.000983\n",
      "Train Epoch: 152 [70/106 (66%)]\tTrain Loss: 0.002362\n",
      "Train Epoch: 152 [80/106 (75%)]\tTrain Loss: 0.000163\n",
      "Train Epoch: 152 [90/106 (85%)]\tTrain Loss: 0.000051\n",
      "Train Epoch: 152 [100/106 (94%)]\tTrain Loss: 0.000146\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.50087897e-06 1.74954802e-01 1.10614747e-05 3.21843777e-06\n",
      " 9.68767654e-06 1.16895805e-07 4.56254551e-04 7.38020390e-06\n",
      " 7.48702718e-08 2.82095334e-06 5.60708759e-06 1.55033931e-07\n",
      " 2.86541513e-06 5.21899528e-06 1.84096396e-04 3.36205471e-06\n",
      " 4.74533008e-05 4.94968845e-05 8.18879998e-05 9.99626875e-01\n",
      " 1.83510911e-02 9.99017477e-01 9.98735607e-01 9.98174429e-01\n",
      " 1.90138514e-03 9.99757349e-01 9.99984384e-01 1.00105706e-06\n",
      " 1.16430252e-04 4.78858783e-05 1.53545011e-03 6.85081119e-04\n",
      " 1.44468548e-04 2.47341304e-05 1.15228067e-06 2.15152863e-06\n",
      " 6.83629969e-06 2.11511560e-05 1.25171733e-04 2.78121115e-05\n",
      " 2.21355600e-04 9.18085425e-06 5.17331537e-05 5.52825031e-06\n",
      " 3.34145043e-05 6.28415478e-07 7.05364146e-05 9.77695920e-04\n",
      " 9.73030865e-01 3.03698494e-06 1.94034204e-02 7.48911816e-07\n",
      " 7.72076510e-06 8.13298129e-07 5.70626697e-04 2.06672766e-07\n",
      " 7.88172111e-02 1.36502607e-07 1.85059264e-06 1.10847905e-04\n",
      " 9.90715265e-01 9.77883220e-01 9.99927282e-01 9.99825537e-01\n",
      " 9.99891400e-01 6.59383659e-05 7.36136921e-04 9.99971747e-01\n",
      " 9.98258889e-01 9.99657869e-01 3.16581992e-03 4.28192541e-02\n",
      " 9.75253046e-01 9.99495864e-01 9.97671783e-01 7.24822574e-04\n",
      " 9.99981284e-01 9.99993920e-01 9.99837518e-01 9.96958017e-01\n",
      " 9.94909823e-01 9.97722805e-01 9.99992371e-01 9.99588668e-01\n",
      " 9.97143805e-01 9.58385229e-01 3.01059190e-04 4.17153089e-04\n",
      " 9.99503255e-01 3.48543253e-04 9.99745667e-01 2.86374470e-05\n",
      " 4.67449572e-05 9.99491453e-01 5.46324991e-05 3.69016198e-05\n",
      " 4.47194589e-06 9.96994376e-01 5.35380241e-05 4.82951524e-03\n",
      " 9.95117784e-01 9.98727620e-01 2.28611316e-06 4.44620491e-05\n",
      " 9.12910819e-01 1.31228851e-04 9.98231351e-01 1.72227999e-04\n",
      " 3.28390561e-05 2.89128147e-05 9.96393859e-01 1.40014605e-03\n",
      " 9.00813553e-04 7.14389324e-01 3.49353577e-05 9.96329129e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 153 [0/106 (0%)]\tTrain Loss: 0.008620\n",
      "Train Epoch: 153 [10/106 (9%)]\tTrain Loss: 0.000144\n",
      "Train Epoch: 153 [20/106 (19%)]\tTrain Loss: 0.000229\n",
      "Train Epoch: 153 [30/106 (28%)]\tTrain Loss: 0.008877\n",
      "Train Epoch: 153 [40/106 (38%)]\tTrain Loss: 0.000091\n",
      "Train Epoch: 153 [50/106 (47%)]\tTrain Loss: 0.000015\n",
      "Train Epoch: 153 [60/106 (57%)]\tTrain Loss: 0.000090\n",
      "Train Epoch: 153 [70/106 (66%)]\tTrain Loss: 0.000392\n",
      "Train Epoch: 153 [80/106 (75%)]\tTrain Loss: 0.000077\n",
      "Train Epoch: 153 [90/106 (85%)]\tTrain Loss: 0.000029\n",
      "Train Epoch: 153 [100/106 (94%)]\tTrain Loss: 0.000051\n",
      "\n",
      "Train set: Average loss: 0.0004, Accuracy: 418/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.06848097e-06 1.39568299e-01 1.15581215e-05 2.89248896e-06\n",
      " 5.02585226e-06 1.33271755e-07 1.32646051e-03 5.08398898e-06\n",
      " 5.38357767e-08 1.94558447e-06 1.22736878e-06 1.00742810e-07\n",
      " 1.32070443e-06 2.98772602e-06 4.52579145e-04 6.63331491e-07\n",
      " 1.75584482e-05 6.44666096e-03 8.83557295e-05 9.98137355e-01\n",
      " 7.30341475e-04 9.97627914e-01 9.96993184e-01 9.97723520e-01\n",
      " 3.05347610e-03 9.98358905e-01 9.99446213e-01 2.14694978e-06\n",
      " 6.89968467e-04 4.23189049e-05 1.35954574e-03 1.00124935e-02\n",
      " 4.28279483e-04 1.72433083e-05 3.40051997e-07 8.21356196e-07\n",
      " 4.58545810e-06 3.50140690e-05 2.86026596e-04 2.65993949e-05\n",
      " 1.84712280e-03 3.45213152e-06 3.68899142e-04 3.67905909e-06\n",
      " 4.28945605e-05 2.20434430e-07 1.08605513e-04 9.96082323e-04\n",
      " 9.86801922e-01 1.48515130e-06 1.46524003e-02 1.22667785e-07\n",
      " 2.59863100e-06 2.90516510e-07 3.85637308e-04 4.56452334e-08\n",
      " 8.89796466e-02 2.51753534e-08 1.12166299e-06 4.80375493e-05\n",
      " 9.97122228e-01 9.96103048e-01 9.99373257e-01 9.99208510e-01\n",
      " 9.98773992e-01 1.19039243e-04 3.33917029e-02 9.99545991e-01\n",
      " 9.98164594e-01 9.98459697e-01 2.50995997e-02 3.59576702e-01\n",
      " 9.96308208e-01 9.98600185e-01 9.97987151e-01 1.98957905e-01\n",
      " 9.99268234e-01 9.99380350e-01 9.99324560e-01 9.97268856e-01\n",
      " 9.96058345e-01 9.96830285e-01 9.99487042e-01 9.98493910e-01\n",
      " 9.97853041e-01 9.95006025e-01 8.02857161e-04 1.22180849e-03\n",
      " 9.97728527e-01 1.38846319e-03 9.98232722e-01 4.35957918e-05\n",
      " 2.16380155e-04 9.98118937e-01 2.44711682e-05 2.34325762e-05\n",
      " 9.36159211e-07 9.96442139e-01 1.67876369e-05 6.99902773e-01\n",
      " 9.96244729e-01 9.97537851e-01 1.00555985e-06 1.40093951e-04\n",
      " 9.95386899e-01 1.24896556e-04 9.98162806e-01 2.27074954e-03\n",
      " 3.61757266e-05 5.77070423e-05 9.98273015e-01 3.72800813e-03\n",
      " 2.27255654e-03 7.70804584e-01 1.63726436e-04 9.97095108e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
      "Train Epoch: 154 [0/106 (0%)]\tTrain Loss: 0.000344\n",
      "Train Epoch: 154 [10/106 (9%)]\tTrain Loss: 0.000026\n",
      "Train Epoch: 154 [20/106 (19%)]\tTrain Loss: 0.000089\n",
      "Train Epoch: 154 [30/106 (28%)]\tTrain Loss: 0.000013\n",
      "Train Epoch: 154 [40/106 (38%)]\tTrain Loss: 0.000039\n",
      "Train Epoch: 154 [50/106 (47%)]\tTrain Loss: 0.000139\n",
      "Train Epoch: 154 [60/106 (57%)]\tTrain Loss: 0.000016\n",
      "Train Epoch: 154 [70/106 (66%)]\tTrain Loss: 0.049557\n",
      "Train Epoch: 154 [80/106 (75%)]\tTrain Loss: 0.000126\n",
      "Train Epoch: 154 [90/106 (85%)]\tTrain Loss: 0.000004\n",
      "Train Epoch: 154 [100/106 (94%)]\tTrain Loss: 0.000007\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.59837214e-06 9.96876974e-04 5.33548837e-06 7.15210490e-07\n",
      " 1.69410407e-06 1.98077629e-07 5.84387053e-05 1.52037774e-06\n",
      " 7.18585227e-08 4.98252007e-07 1.01699766e-06 7.74855167e-08\n",
      " 8.40210475e-07 2.14403212e-06 4.67928076e-05 1.01475780e-06\n",
      " 8.75552541e-06 7.11166561e-01 9.86631858e-06 9.97552216e-01\n",
      " 7.69599064e-05 9.99867797e-01 9.99725521e-01 9.99872208e-01\n",
      " 5.64343035e-02 9.99992728e-01 9.99999881e-01 1.20897755e-06\n",
      " 1.17086252e-04 1.63599416e-05 5.42827998e-04 2.45332078e-04\n",
      " 6.01924257e-04 3.58459920e-06 3.29026932e-07 5.18924423e-07\n",
      " 1.30139745e-06 9.26296889e-06 1.45456552e-05 4.57614442e-06\n",
      " 5.27903867e-05 1.13940700e-06 7.01982935e-05 4.31601256e-06\n",
      " 1.52536086e-05 5.28077408e-07 3.32671661e-05 1.57025323e-04\n",
      " 9.91291821e-01 6.59356772e-07 8.65591839e-02 1.17641747e-07\n",
      " 1.21403343e-06 1.88397465e-07 6.01538068e-06 6.45511946e-08\n",
      " 9.37862635e-01 4.85514846e-08 2.07901076e-06 1.51042086e-05\n",
      " 9.12405431e-01 7.30452716e-01 9.99992847e-01 9.99961376e-01\n",
      " 9.99987245e-01 2.72185844e-05 3.10104154e-02 9.99999881e-01\n",
      " 9.99287903e-01 9.99973536e-01 7.51604885e-02 7.58042112e-02\n",
      " 9.75318491e-01 9.99898076e-01 9.99034047e-01 6.53982803e-04\n",
      " 9.99979377e-01 9.99995708e-01 9.99994516e-01 9.99545515e-01\n",
      " 9.99577582e-01 9.99058187e-01 9.99999762e-01 9.99930382e-01\n",
      " 9.98550594e-01 9.57645416e-01 6.27555492e-05 9.08819202e-05\n",
      " 9.97531652e-01 1.94267050e-04 9.99994278e-01 1.14261029e-05\n",
      " 3.09598072e-05 9.99896169e-01 9.65095751e-06 1.17680438e-05\n",
      " 8.13918859e-07 9.99096751e-01 7.09491633e-06 1.16357149e-03\n",
      " 9.91035581e-01 9.99706686e-01 8.22668483e-07 7.33198322e-05\n",
      " 9.99077559e-01 4.33973510e-05 9.99796212e-01 2.68603690e-05\n",
      " 2.12380473e-05 6.66701862e-06 9.99552667e-01 3.51648399e-04\n",
      " 1.54083522e-04 8.38878512e-01 7.09629458e-05 9.51859355e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
      "Train Epoch: 155 [0/106 (0%)]\tTrain Loss: 0.000007\n",
      "Train Epoch: 155 [10/106 (9%)]\tTrain Loss: 0.000043\n",
      "Train Epoch: 155 [20/106 (19%)]\tTrain Loss: 0.023312\n",
      "Train Epoch: 155 [30/106 (28%)]\tTrain Loss: 0.000035\n",
      "Train Epoch: 155 [40/106 (38%)]\tTrain Loss: 0.000020\n",
      "Train Epoch: 155 [50/106 (47%)]\tTrain Loss: 0.001091\n",
      "Train Epoch: 155 [60/106 (57%)]\tTrain Loss: 0.000004\n",
      "Train Epoch: 155 [70/106 (66%)]\tTrain Loss: 0.000086\n",
      "Train Epoch: 155 [80/106 (75%)]\tTrain Loss: 0.000011\n",
      "Train Epoch: 155 [90/106 (85%)]\tTrain Loss: 0.000034\n",
      "Train Epoch: 155 [100/106 (94%)]\tTrain Loss: 0.000764\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 421/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.46503766e-05 4.96038139e-01 1.72649554e-04 4.84633965e-05\n",
      " 7.91831117e-05 1.72062537e-05 1.04209012e-03 5.98777042e-05\n",
      " 3.23050767e-06 3.20415529e-05 2.79267115e-04 5.26611211e-06\n",
      " 1.18137876e-04 1.23732723e-04 1.05112500e-03 7.99495829e-05\n",
      " 4.14201786e-04 1.10019103e-03 4.76802554e-04 9.99265373e-01\n",
      " 1.68958474e-02 9.98555720e-01 9.86688316e-01 9.98572111e-01\n",
      " 3.37228877e-03 9.96265471e-01 9.99981880e-01 1.91927247e-05\n",
      " 1.28270057e-03 7.51805142e-04 1.10335974e-03 1.44534959e-02\n",
      " 1.46840781e-03 3.06424452e-04 2.22042800e-05 7.18352530e-05\n",
      " 8.45338509e-05 1.58914336e-04 2.37082175e-04 2.08217345e-04\n",
      " 1.04971221e-02 1.09781671e-04 2.78615014e-04 2.70554214e-04\n",
      " 1.13922847e-03 3.42978783e-05 5.53790131e-04 1.01458491e-03\n",
      " 9.96419787e-01 2.83121026e-05 2.34336436e-01 1.10703604e-05\n",
      " 7.90337363e-05 1.78759674e-05 5.25381474e-04 6.82290874e-06\n",
      " 1.65566653e-01 9.07339381e-06 4.79988994e-05 5.14358864e-04\n",
      " 9.98548806e-01 9.97391224e-01 9.99877572e-01 9.99696255e-01\n",
      " 9.99804437e-01 9.79923876e-04 8.19579791e-03 9.99946594e-01\n",
      " 9.97556806e-01 9.98918295e-01 9.84772027e-01 9.95610774e-01\n",
      " 9.97486234e-01 9.99575794e-01 9.98210430e-01 2.10674554e-02\n",
      " 9.99322057e-01 9.99544084e-01 9.28762496e-01 9.97548878e-01\n",
      " 9.98982012e-01 9.97897863e-01 9.99901056e-01 9.98471558e-01\n",
      " 9.79753375e-01 9.92149115e-01 1.32916239e-03 1.40459207e-03\n",
      " 9.97301400e-01 1.40083162e-03 9.99538064e-01 2.22334740e-04\n",
      " 3.24370252e-04 9.97556329e-01 7.30258704e-04 6.70937472e-04\n",
      " 5.81158165e-05 9.97069299e-01 2.74934177e-03 9.82488990e-01\n",
      " 9.96307731e-01 9.99231696e-01 5.10695645e-05 3.14556807e-03\n",
      " 9.97177839e-01 1.37290708e-03 9.99150872e-01 1.33445831e-02\n",
      " 1.56415638e-03 6.55868556e-04 9.99273241e-01 4.48797550e-03\n",
      " 2.74790684e-03 9.93874490e-01 1.22562167e-03 9.05484557e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 156 [0/106 (0%)]\tTrain Loss: 0.024075\n",
      "Train Epoch: 156 [10/106 (9%)]\tTrain Loss: 0.001419\n",
      "Train Epoch: 156 [20/106 (19%)]\tTrain Loss: 0.000252\n",
      "Train Epoch: 156 [30/106 (28%)]\tTrain Loss: 0.000208\n",
      "Train Epoch: 156 [40/106 (38%)]\tTrain Loss: 0.001008\n",
      "Train Epoch: 156 [50/106 (47%)]\tTrain Loss: 0.000044\n",
      "Train Epoch: 156 [60/106 (57%)]\tTrain Loss: 0.000161\n",
      "Train Epoch: 156 [70/106 (66%)]\tTrain Loss: 0.000155\n",
      "Train Epoch: 156 [80/106 (75%)]\tTrain Loss: 0.000357\n",
      "Train Epoch: 156 [90/106 (85%)]\tTrain Loss: 0.000245\n",
      "Train Epoch: 156 [100/106 (94%)]\tTrain Loss: 0.000232\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 418/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.85630244e-06 9.99728620e-01 8.97150370e-04 3.23630479e-06\n",
      " 1.27453416e-06 5.37566109e-07 4.24983144e-01 6.74165722e-06\n",
      " 1.41871112e-07 1.95759276e-06 2.02776464e-05 1.61973844e-07\n",
      " 7.88554025e-05 6.46186963e-05 1.34281325e-03 1.07118683e-06\n",
      " 1.20471884e-03 8.18513072e-05 1.96829140e-02 9.99881625e-01\n",
      " 9.99821484e-01 9.99920130e-01 9.99793947e-01 9.99843240e-01\n",
      " 5.81729114e-01 9.99950051e-01 9.99999166e-01 8.73465888e-06\n",
      " 2.61200150e-03 1.35136244e-04 1.20494433e-03 5.40216267e-03\n",
      " 4.05478850e-03 2.13399471e-04 1.04556079e-06 3.96593441e-06\n",
      " 2.21757764e-05 9.51839320e-06 2.92841310e-06 2.91655961e-05\n",
      " 1.29712874e-03 1.39897757e-05 4.10773027e-05 2.07149424e-06\n",
      " 1.12869754e-03 1.14565466e-06 2.88820564e-04 1.76413963e-03\n",
      " 3.32038403e-02 8.71846441e-06 2.49356590e-02 1.31725372e-07\n",
      " 4.09549557e-06 3.36121047e-07 1.16063943e-02 3.41191878e-07\n",
      " 9.94323015e-01 1.93098742e-07 1.18232856e-06 2.22392380e-04\n",
      " 9.99963999e-01 9.99934554e-01 9.99964833e-01 9.99929070e-01\n",
      " 9.99983430e-01 2.99166720e-02 4.64878161e-04 9.99994159e-01\n",
      " 9.99906421e-01 9.99834418e-01 1.53236547e-02 9.80455637e-01\n",
      " 9.98802781e-01 9.99919176e-01 9.99972463e-01 9.99926329e-01\n",
      " 9.99929428e-01 9.99964237e-01 9.99932170e-01 9.99932170e-01\n",
      " 9.99892473e-01 9.99896288e-01 9.99964118e-01 9.99748290e-01\n",
      " 8.86912167e-01 9.99835372e-01 5.94014022e-03 5.66857681e-03\n",
      " 9.99603093e-01 6.07560715e-03 9.99906421e-01 6.78824381e-06\n",
      " 6.33944592e-06 9.99401331e-01 1.18389587e-04 9.36907527e-05\n",
      " 6.04222441e-06 9.99208152e-01 7.35173002e-04 9.99530792e-01\n",
      " 9.99547064e-01 9.99872208e-01 4.72132479e-06 3.92874586e-04\n",
      " 8.33615568e-03 6.34901633e-04 9.99848604e-01 9.99507666e-01\n",
      " 2.12228447e-02 2.86794943e-03 9.99958158e-01 8.26862059e-04\n",
      " 1.05112651e-03 9.99575078e-01 3.04202527e-01 9.95045528e-03]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 157 [0/106 (0%)]\tTrain Loss: 0.000041\n",
      "Train Epoch: 157 [10/106 (9%)]\tTrain Loss: 0.000069\n",
      "Train Epoch: 157 [20/106 (19%)]\tTrain Loss: 0.054048\n",
      "Train Epoch: 157 [30/106 (28%)]\tTrain Loss: 0.000108\n",
      "Train Epoch: 157 [40/106 (38%)]\tTrain Loss: 0.080554\n",
      "Train Epoch: 157 [50/106 (47%)]\tTrain Loss: 0.050805\n",
      "Train Epoch: 157 [60/106 (57%)]\tTrain Loss: 0.000102\n",
      "Train Epoch: 157 [70/106 (66%)]\tTrain Loss: 0.000479\n",
      "Train Epoch: 157 [80/106 (75%)]\tTrain Loss: 0.000524\n",
      "Train Epoch: 157 [90/106 (85%)]\tTrain Loss: 0.001278\n",
      "Train Epoch: 157 [100/106 (94%)]\tTrain Loss: 0.000208\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 416/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.23788459e-05 9.97431099e-01 6.43372245e-04 9.06673358e-06\n",
      " 1.35029086e-06 9.39358870e-06 7.77628049e-02 3.21113912e-05\n",
      " 7.90568777e-07 5.26844196e-05 2.06507389e-06 3.90766672e-06\n",
      " 8.30647241e-06 1.67152015e-04 3.71069252e-01 5.54472649e-07\n",
      " 3.15607249e-05 1.17823165e-02 6.43570861e-03 9.99231339e-01\n",
      " 9.90613103e-01 9.99436080e-01 9.99525428e-01 9.99640822e-01\n",
      " 6.51711300e-02 9.99972820e-01 9.99989629e-01 9.97967184e-01\n",
      " 6.70932770e-01 1.01850682e-03 9.99522209e-01 3.52967739e-01\n",
      " 2.76854057e-02 1.32398327e-05 5.40527594e-07 8.61025114e-07\n",
      " 4.61681429e-06 3.19236096e-05 1.53419256e-04 4.02942787e-05\n",
      " 3.14377295e-03 8.06520347e-06 1.45480182e-04 4.73588671e-06\n",
      " 2.11060350e-03 1.35991313e-05 5.33443177e-04 2.97711254e-03\n",
      " 3.94494757e-02 1.61040283e-04 1.55165806e-01 9.89764573e-08\n",
      " 6.05171135e-06 4.71941632e-07 8.93712103e-01 1.10116059e-06\n",
      " 9.99589741e-01 9.12461644e-07 3.55057814e-03 2.84340844e-04\n",
      " 9.99933243e-01 9.99807298e-01 9.99970317e-01 9.99962211e-01\n",
      " 9.99982119e-01 1.32335899e-02 6.74405992e-02 9.99988675e-01\n",
      " 9.99556124e-01 9.99883056e-01 5.45094199e-02 9.88998353e-01\n",
      " 9.99592483e-01 9.99946833e-01 9.99983311e-01 9.99951482e-01\n",
      " 9.99954104e-01 9.99962568e-01 9.99969959e-01 9.99886751e-01\n",
      " 9.99706686e-01 9.99819219e-01 9.99942064e-01 9.99958038e-01\n",
      " 9.90901351e-01 9.99458253e-01 9.98392284e-01 9.99807060e-01\n",
      " 9.99283969e-01 1.41016260e-01 9.99826133e-01 2.46506061e-05\n",
      " 3.81443351e-05 9.99685168e-01 1.58504365e-04 6.02661050e-04\n",
      " 8.07419292e-06 9.98806477e-01 9.68967319e-01 9.99200761e-01\n",
      " 9.99212384e-01 9.99858141e-01 9.78286771e-05 3.29597807e-03\n",
      " 9.92057025e-01 1.10518998e-02 9.99222875e-01 5.17458003e-03\n",
      " 3.27509874e-03 2.74353148e-03 9.92008328e-01 9.32952040e-04\n",
      " 4.99151135e-03 9.95183170e-01 9.98724520e-01 9.99653459e-01]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1.]\n",
      "Train Epoch: 158 [0/106 (0%)]\tTrain Loss: 0.000127\n",
      "Train Epoch: 158 [10/106 (9%)]\tTrain Loss: 0.118205\n",
      "Train Epoch: 158 [20/106 (19%)]\tTrain Loss: 0.000005\n",
      "Train Epoch: 158 [30/106 (28%)]\tTrain Loss: 0.000686\n",
      "Train Epoch: 158 [40/106 (38%)]\tTrain Loss: 0.018180\n",
      "Train Epoch: 158 [50/106 (47%)]\tTrain Loss: 0.000051\n",
      "Train Epoch: 158 [60/106 (57%)]\tTrain Loss: 0.000179\n",
      "Train Epoch: 158 [70/106 (66%)]\tTrain Loss: 0.000022\n",
      "Train Epoch: 158 [80/106 (75%)]\tTrain Loss: 0.000294\n",
      "Train Epoch: 158 [90/106 (85%)]\tTrain Loss: 0.010046\n",
      "Train Epoch: 158 [100/106 (94%)]\tTrain Loss: 0.000133\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 419/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [4.40284657e-06 9.98643100e-01 8.10982237e-05 2.79189917e-05\n",
      " 3.11928920e-06 2.36945425e-06 9.22761634e-02 4.37163108e-05\n",
      " 1.85375629e-05 9.34917596e-04 1.82718998e-06 3.72302820e-05\n",
      " 9.99708027e-06 9.74684775e-01 9.99590695e-01 3.63802815e-06\n",
      " 8.24004601e-05 9.96040106e-01 1.18736923e-03 9.99181092e-01\n",
      " 9.99428928e-01 9.99908209e-01 9.99482512e-01 9.99971390e-01\n",
      " 9.99137998e-01 9.99956489e-01 9.99999881e-01 6.23737986e-04\n",
      " 4.97674162e-04 3.30349489e-04 9.99943614e-01 9.95942056e-01\n",
      " 7.84185708e-01 2.34431282e-05 1.04801563e-06 2.17975276e-06\n",
      " 2.78995067e-05 4.60697338e-03 1.68246764e-03 6.95156911e-03\n",
      " 9.94653821e-01 1.08757653e-04 9.90304291e-01 5.66899544e-05\n",
      " 4.13358444e-04 2.89804466e-05 5.06094657e-03 8.34548920e-02\n",
      " 9.94933426e-01 3.07566533e-03 9.05001938e-01 3.88277101e-07\n",
      " 8.61239969e-05 1.97841382e-06 3.84748587e-03 4.61375066e-06\n",
      " 9.99712646e-01 1.46244702e-06 3.95749463e-04 2.10357830e-03\n",
      " 9.99994159e-01 9.99966621e-01 9.99995232e-01 9.99994516e-01\n",
      " 9.99996424e-01 9.99671459e-01 9.99284208e-01 9.99999762e-01\n",
      " 9.99820054e-01 9.99973893e-01 9.97766972e-01 9.90842700e-01\n",
      " 9.99945760e-01 9.99996543e-01 9.99999404e-01 9.99998212e-01\n",
      " 9.99989390e-01 9.99987364e-01 9.99986053e-01 9.99993086e-01\n",
      " 9.99936342e-01 9.99929309e-01 9.99998808e-01 9.99999166e-01\n",
      " 9.99966145e-01 9.99936342e-01 9.99769390e-01 9.99994636e-01\n",
      " 9.99235988e-01 1.32956550e-01 9.99991179e-01 6.00848987e-04\n",
      " 1.98411383e-03 9.99980807e-01 1.86198580e-04 1.29808500e-01\n",
      " 1.67366416e-05 9.99731123e-01 2.05813019e-04 9.99276698e-01\n",
      " 9.99549806e-01 9.99994755e-01 1.37490033e-05 9.99484420e-01\n",
      " 9.99976277e-01 3.22270673e-03 9.99856830e-01 9.96168554e-01\n",
      " 3.30852121e-02 2.38706416e-04 9.99900579e-01 2.03549176e-01\n",
      " 9.11573827e-01 9.97794628e-01 9.96897817e-01 9.99989152e-01]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 159 [0/106 (0%)]\tTrain Loss: 0.000210\n",
      "Train Epoch: 159 [10/106 (9%)]\tTrain Loss: 0.085312\n",
      "Train Epoch: 159 [20/106 (19%)]\tTrain Loss: 0.000042\n",
      "Train Epoch: 159 [30/106 (28%)]\tTrain Loss: 0.000036\n",
      "Train Epoch: 159 [40/106 (38%)]\tTrain Loss: 0.000101\n",
      "Train Epoch: 159 [50/106 (47%)]\tTrain Loss: 0.000041\n",
      "Train Epoch: 159 [60/106 (57%)]\tTrain Loss: 0.086358\n",
      "Train Epoch: 159 [70/106 (66%)]\tTrain Loss: 0.000239\n",
      "Train Epoch: 159 [80/106 (75%)]\tTrain Loss: 0.000038\n",
      "Train Epoch: 159 [90/106 (85%)]\tTrain Loss: 0.000052\n",
      "Train Epoch: 159 [100/106 (94%)]\tTrain Loss: 0.000780\n",
      "\n",
      "Train set: Average loss: 0.0015, Accuracy: 419/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.14881004e-06 9.99352753e-01 2.29176039e-05 2.63870788e-06\n",
      " 2.36204073e-06 3.54476327e-07 1.99590926e-03 3.93142136e-06\n",
      " 4.43497839e-07 1.12023481e-05 2.13110366e-06 1.11224676e-06\n",
      " 2.34037270e-05 7.37740309e-04 3.83851618e-01 1.90515345e-06\n",
      " 2.50130361e-05 3.79069934e-05 3.94390117e-05 9.99735773e-01\n",
      " 9.99760807e-01 5.57877362e-01 9.94059086e-01 9.99953628e-01\n",
      " 3.18633480e-04 9.99996305e-01 9.99999881e-01 8.85173904e-06\n",
      " 1.07094620e-05 1.31472596e-04 8.83707404e-03 1.31992623e-03\n",
      " 1.08198845e-04 8.93153174e-06 9.49719833e-07 2.27478267e-06\n",
      " 8.87132865e-06 3.96868882e-06 1.95450366e-06 7.48474440e-06\n",
      " 6.63063547e-05 2.26185716e-06 8.58252042e-06 1.24543585e-05\n",
      " 5.75451995e-05 8.33241666e-06 4.31156077e-05 9.01411186e-05\n",
      " 6.32442236e-01 1.51777485e-05 1.34076807e-04 2.73853601e-07\n",
      " 3.59468436e-06 9.85445013e-07 1.31379609e-04 1.31063030e-06\n",
      " 9.99691486e-01 2.89728473e-06 3.16915998e-06 1.47705296e-05\n",
      " 9.99995708e-01 9.99987841e-01 9.99999523e-01 9.99998450e-01\n",
      " 9.99999762e-01 4.06529980e-05 2.05238321e-05 1.00000000e+00\n",
      " 9.99298692e-01 9.99730408e-01 4.41120006e-04 9.92371589e-02\n",
      " 9.99898553e-01 9.99999166e-01 1.00000000e+00 9.99129593e-01\n",
      " 9.99997973e-01 9.99997377e-01 1.75169721e-01 9.99902010e-01\n",
      " 9.99632239e-01 9.99150634e-01 9.99999762e-01 9.99999404e-01\n",
      " 9.97717619e-01 9.98608172e-01 2.37579545e-04 8.30307254e-04\n",
      " 9.99901414e-01 1.17414515e-04 9.99994516e-01 9.24703727e-06\n",
      " 9.44992553e-06 9.99949813e-01 9.30221267e-06 2.98740215e-05\n",
      " 6.77156186e-06 1.02241070e-03 5.32043014e-05 9.97075796e-01\n",
      " 9.99772966e-01 9.99977708e-01 2.48127776e-06 3.71748647e-05\n",
      " 9.96834219e-01 6.62695529e-05 9.98991311e-01 2.36772862e-03\n",
      " 4.41245538e-05 1.66779610e-05 8.02025557e-01 2.03181175e-04\n",
      " 1.86979276e-04 9.89776254e-01 4.50752341e-05 1.16673519e-03]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 160 [0/106 (0%)]\tTrain Loss: 0.000074\n",
      "Train Epoch: 160 [10/106 (9%)]\tTrain Loss: 0.000170\n",
      "Train Epoch: 160 [20/106 (19%)]\tTrain Loss: 0.375010\n",
      "Train Epoch: 160 [30/106 (28%)]\tTrain Loss: 0.000277\n",
      "Train Epoch: 160 [40/106 (38%)]\tTrain Loss: 0.132329\n",
      "Train Epoch: 160 [50/106 (47%)]\tTrain Loss: 0.001251\n",
      "Train Epoch: 160 [60/106 (57%)]\tTrain Loss: 0.103240\n",
      "Train Epoch: 160 [70/106 (66%)]\tTrain Loss: 0.000120\n",
      "Train Epoch: 160 [80/106 (75%)]\tTrain Loss: 0.073276\n",
      "Train Epoch: 160 [90/106 (85%)]\tTrain Loss: 0.010592\n",
      "Train Epoch: 160 [100/106 (94%)]\tTrain Loss: 0.000230\n",
      "\n",
      "Train set: Average loss: 0.0001, Accuracy: 412/424 (97%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.12441625e-06 2.43001748e-04 9.94575930e-06 1.38576763e-06\n",
      " 1.75849732e-06 1.06319237e-06 4.49256768e-05 2.38821599e-06\n",
      " 2.00505724e-06 6.94516029e-06 1.33015806e-06 2.38000553e-06\n",
      " 3.37570918e-06 8.96337660e-05 2.62859365e-04 1.20057064e-06\n",
      " 3.55135890e-05 2.31734321e-01 2.65533945e-05 2.27074444e-01\n",
      " 2.65247741e-04 3.02133267e-03 2.60229991e-03 9.97953057e-01\n",
      " 1.38631498e-04 9.98640239e-01 9.99993920e-01 2.85604642e-06\n",
      " 4.68689104e-05 1.19322794e-04 5.75888960e-04 3.52183182e-04\n",
      " 6.25954926e-05 7.82747884e-06 1.69113218e-06 7.78238643e-07\n",
      " 2.12653163e-06 7.36591574e-06 4.34221283e-06 2.16285184e-06\n",
      " 1.55840316e-05 2.29104148e-06 6.75231859e-05 6.33945865e-06\n",
      " 1.62569831e-05 1.29570126e-05 8.45902105e-05 1.35051872e-04\n",
      " 3.49827809e-03 1.66951795e-05 1.37340758e-04 1.60138413e-07\n",
      " 3.59858968e-05 7.99460679e-07 6.03085864e-05 1.46498758e-06\n",
      " 1.63578530e-04 1.35795267e-06 7.91715775e-06 5.19545429e-05\n",
      " 9.99489188e-01 9.91961300e-01 9.99975801e-01 9.99908924e-01\n",
      " 9.69324708e-01 9.64826177e-06 3.26167392e-05 9.99998450e-01\n",
      " 1.03455111e-01 9.97223973e-01 2.01186561e-03 1.03980233e-03\n",
      " 9.86684859e-01 9.99727309e-01 9.99679327e-01 9.77937639e-01\n",
      " 9.99884486e-01 9.99983072e-01 5.71085751e-01 2.52614636e-03\n",
      " 9.88131881e-01 4.45943372e-03 9.99857187e-01 9.99850392e-01\n",
      " 4.88400133e-03 4.59251910e-01 4.81318739e-06 5.70444627e-06\n",
      " 6.93970323e-01 2.77938205e-04 9.99725759e-01 2.26342436e-05\n",
      " 1.80211864e-05 7.82860398e-01 6.43840976e-05 3.16025325e-05\n",
      " 2.05543492e-05 2.36436797e-04 3.59500154e-05 5.79979783e-03\n",
      " 9.48887408e-01 9.98125732e-01 1.39190126e-06 1.43193720e-05\n",
      " 5.07195946e-04 1.28806874e-04 5.89647423e-03 1.12794049e-04\n",
      " 2.94620040e-05 5.63202011e-05 5.88139053e-04 5.31797239e-04\n",
      " 8.49643897e-04 9.88151610e-01 5.48536373e-05 1.52815395e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 33 TN= 53 FN= 23 FP= 7\n",
      "TP+FP 40\n",
      "precision 0.825\n",
      "recall 0.5892857142857143\n",
      "F1 0.6875000000000001\n",
      "acc 0.7413793103448276\n",
      "AUCp 0.7363095238095239\n",
      "AUC 0.8184523809523809\n",
      "\n",
      " The epoch is 160, average recall: 0.5893, average precision: 0.8250,average F1: 0.6875, average accuracy: 0.7414, average AUC: 0.8185\n",
      "Train Epoch: 161 [0/106 (0%)]\tTrain Loss: 0.000098\n",
      "Train Epoch: 161 [10/106 (9%)]\tTrain Loss: 0.000013\n",
      "Train Epoch: 161 [20/106 (19%)]\tTrain Loss: 0.010434\n",
      "Train Epoch: 161 [30/106 (28%)]\tTrain Loss: 0.000073\n",
      "Train Epoch: 161 [40/106 (38%)]\tTrain Loss: 0.000007\n",
      "Train Epoch: 161 [50/106 (47%)]\tTrain Loss: 0.000141\n",
      "Train Epoch: 161 [60/106 (57%)]\tTrain Loss: 0.016026\n",
      "Train Epoch: 161 [70/106 (66%)]\tTrain Loss: 0.000149\n",
      "Train Epoch: 161 [80/106 (75%)]\tTrain Loss: 0.000196\n",
      "Train Epoch: 161 [90/106 (85%)]\tTrain Loss: 0.000014\n",
      "Train Epoch: 161 [100/106 (94%)]\tTrain Loss: 0.000239\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 420/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [9.95611060e-09 9.99027848e-01 4.46280239e-07 2.57817145e-08\n",
      " 9.91510074e-09 3.01174952e-09 1.38856079e-02 2.21924807e-08\n",
      " 2.30433095e-09 1.06619291e-08 1.78525106e-08 2.24957342e-09\n",
      " 2.43409488e-07 2.01551529e-06 6.63168476e-06 7.51409388e-08\n",
      " 3.12851898e-06 3.37885496e-07 4.01100493e-07 9.99909401e-01\n",
      " 3.50504852e-05 7.30797410e-06 1.23647437e-03 9.99453604e-01\n",
      " 3.42249393e-07 9.99917388e-01 9.99991536e-01 6.03159762e-08\n",
      " 4.47806485e-07 1.81308569e-05 2.97400129e-06 1.43057213e-03\n",
      " 4.52468043e-07 6.61081998e-08 9.93046889e-09 3.72994702e-09\n",
      " 7.46905204e-09 4.42653558e-08 2.30554118e-08 4.04204634e-08\n",
      " 2.32158882e-07 1.90725657e-08 7.85202587e-08 1.37188948e-07\n",
      " 9.30901251e-07 2.34781510e-07 8.33129889e-07 8.93177059e-07\n",
      " 7.17421472e-02 5.75490873e-08 6.86268891e-07 2.56678168e-09\n",
      " 1.61720251e-07 5.26155342e-09 3.48034541e-06 2.20247340e-08\n",
      " 4.57284557e-07 8.06295404e-08 5.47763435e-09 4.70732459e-07\n",
      " 9.83041286e-01 4.29084525e-02 9.99953628e-01 9.99932647e-01\n",
      " 9.99955654e-01 1.19869398e-07 2.76884379e-07 9.99999523e-01\n",
      " 4.68885351e-04 9.99952316e-01 1.81204850e-05 1.47749102e-04\n",
      " 9.99710023e-01 9.99938011e-01 9.99975681e-01 9.99968767e-01\n",
      " 9.99991536e-01 9.99997616e-01 9.68566895e-01 9.99976635e-01\n",
      " 9.99974608e-01 9.99817193e-01 9.99988675e-01 9.99975085e-01\n",
      " 7.55365356e-04 7.10951984e-01 6.42303030e-06 7.75975514e-06\n",
      " 9.99915600e-01 4.99399357e-06 9.99958277e-01 7.14448447e-08\n",
      " 2.21394700e-07 9.99648452e-01 9.90116405e-07 6.33137063e-07\n",
      " 1.18419550e-07 6.94551773e-06 7.58890337e-07 9.86662805e-01\n",
      " 8.87386315e-03 9.99203384e-01 1.39515253e-08 3.65460141e-06\n",
      " 7.34771430e-01 3.00762940e-06 1.66366830e-01 4.04239436e-05\n",
      " 3.40388794e-07 4.43942128e-07 1.63794084e-05 8.31439195e-07\n",
      " 6.39312420e-05 3.95052969e-01 1.18217406e-06 8.19390357e-01]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 162 [0/106 (0%)]\tTrain Loss: 0.206921\n",
      "Train Epoch: 162 [10/106 (9%)]\tTrain Loss: 0.000177\n",
      "Train Epoch: 162 [20/106 (19%)]\tTrain Loss: 0.010137\n",
      "Train Epoch: 162 [30/106 (28%)]\tTrain Loss: 0.054994\n",
      "Train Epoch: 162 [40/106 (38%)]\tTrain Loss: 0.000012\n",
      "Train Epoch: 162 [50/106 (47%)]\tTrain Loss: 0.000005\n",
      "Train Epoch: 162 [60/106 (57%)]\tTrain Loss: 0.000030\n",
      "Train Epoch: 162 [70/106 (66%)]\tTrain Loss: 0.000003\n",
      "Train Epoch: 162 [80/106 (75%)]\tTrain Loss: 0.000031\n",
      "Train Epoch: 162 [90/106 (85%)]\tTrain Loss: 0.000004\n",
      "Train Epoch: 162 [100/106 (94%)]\tTrain Loss: 0.000006\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 419/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.04023700e-07 9.89161313e-01 3.08839253e-06 3.00042120e-06\n",
      " 9.85632028e-07 1.56361224e-07 1.65436487e-03 4.59878447e-06\n",
      " 4.89286890e-07 1.24157486e-06 1.85531076e-06 2.08815806e-07\n",
      " 8.18318495e-06 2.31292597e-05 1.72756772e-04 8.01334681e-06\n",
      " 4.29859273e-02 2.88079586e-03 5.66175004e-06 9.99873877e-01\n",
      " 9.76576686e-01 9.98010099e-01 9.98801947e-01 9.99430597e-01\n",
      " 1.18134274e-04 9.99960423e-01 9.99985337e-01 2.42479709e-05\n",
      " 1.95492867e-05 3.05775990e-04 9.98620152e-01 9.92910802e-01\n",
      " 1.28526663e-04 1.05253666e-05 1.04414778e-06 4.86944032e-07\n",
      " 2.06986124e-06 1.81964024e-05 2.63904485e-06 2.11454540e-06\n",
      " 6.80121684e-06 1.61955597e-06 2.73593214e-05 1.60628970e-05\n",
      " 7.36922520e-05 1.35073115e-05 4.14543720e-05 8.36952458e-05\n",
      " 9.97501433e-01 2.03793505e-04 2.52711965e-04 2.06941976e-07\n",
      " 1.21674930e-05 5.43108115e-07 6.30058275e-05 1.64700054e-06\n",
      " 9.97906923e-01 5.41271311e-06 2.57141460e-06 8.05142263e-05\n",
      " 9.79089200e-01 1.98685735e-01 9.99831915e-01 9.99698520e-01\n",
      " 9.99978423e-01 9.20366438e-06 2.78233583e-05 9.99992967e-01\n",
      " 9.81087804e-01 9.99913096e-01 2.44780397e-03 1.48357525e-01\n",
      " 9.99818861e-01 9.99946713e-01 9.99988198e-01 9.99035597e-01\n",
      " 9.99912500e-01 9.99942422e-01 9.99659181e-01 9.99791920e-01\n",
      " 9.99965310e-01 9.98968005e-01 9.99965191e-01 9.99928713e-01\n",
      " 9.96454239e-01 9.97098923e-01 1.42233754e-02 7.13093281e-01\n",
      " 9.99899268e-01 7.55418062e-01 9.99932289e-01 8.36318486e-06\n",
      " 3.40562074e-05 9.99803841e-01 2.48736739e-02 9.81698751e-01\n",
      " 9.15345572e-06 9.01267827e-01 3.96253999e-05 9.98295248e-01\n",
      " 9.98982608e-01 9.99673009e-01 1.72387524e-06 7.31814623e-01\n",
      " 9.99906778e-01 3.89690627e-04 9.97179985e-01 9.98992145e-01\n",
      " 4.05421524e-05 5.74845435e-05 9.79477286e-01 8.15200183e-05\n",
      " 9.90839303e-01 9.98891890e-01 3.68404399e-05 9.87872720e-01]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1.]\n",
      "Train Epoch: 163 [0/106 (0%)]\tTrain Loss: 0.000057\n",
      "Train Epoch: 163 [10/106 (9%)]\tTrain Loss: 0.000089\n",
      "Train Epoch: 163 [20/106 (19%)]\tTrain Loss: 0.000064\n",
      "Train Epoch: 163 [30/106 (28%)]\tTrain Loss: 0.000013\n",
      "Train Epoch: 163 [40/106 (38%)]\tTrain Loss: 0.018177\n",
      "Train Epoch: 163 [50/106 (47%)]\tTrain Loss: 0.000468\n",
      "Train Epoch: 163 [60/106 (57%)]\tTrain Loss: 0.000010\n",
      "Train Epoch: 163 [70/106 (66%)]\tTrain Loss: 0.000042\n",
      "Train Epoch: 163 [80/106 (75%)]\tTrain Loss: 0.000157\n",
      "Train Epoch: 163 [90/106 (85%)]\tTrain Loss: 0.000093\n",
      "Train Epoch: 163 [100/106 (94%)]\tTrain Loss: 0.000008\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 419/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.54404710e-06 9.61216688e-01 1.09758348e-05 1.11412901e-05\n",
      " 7.07931122e-06 9.51330605e-07 5.17889392e-04 2.32562179e-05\n",
      " 1.85158831e-06 5.73532679e-06 3.66712084e-05 1.07071912e-06\n",
      " 1.16616036e-04 6.68106877e-05 1.47142855e-04 2.61490914e-05\n",
      " 1.24039976e-02 2.60082539e-04 3.19008432e-05 9.99315262e-01\n",
      " 4.73606318e-01 9.95786846e-01 8.54538441e-01 9.98652160e-01\n",
      " 5.46457130e-04 9.99813497e-01 9.99995589e-01 1.01940195e-05\n",
      " 8.92556563e-05 2.37503275e-03 7.61987329e-01 5.19726694e-01\n",
      " 2.24795411e-04 5.57825369e-05 1.36677145e-05 3.71546116e-06\n",
      " 1.07128426e-05 4.85691635e-05 1.20897321e-05 6.68622215e-06\n",
      " 2.47004245e-05 8.74192574e-06 2.35885818e-05 1.00169404e-04\n",
      " 1.66789966e-03 2.26697477e-04 3.14348639e-04 1.77730562e-03\n",
      " 9.94246185e-01 1.09782086e-04 2.83637596e-03 1.71955855e-06\n",
      " 6.67959757e-05 5.30017132e-06 5.00734721e-04 2.18500591e-05\n",
      " 9.66116965e-01 6.20218634e-05 6.37206767e-06 3.94767878e-04\n",
      " 9.66451108e-01 3.50934446e-01 9.99809921e-01 9.99386311e-01\n",
      " 9.99935389e-01 1.99828937e-05 9.86498635e-05 9.99946117e-01\n",
      " 2.22063720e-01 9.99766529e-01 6.97881505e-02 9.12075579e-01\n",
      " 9.98363435e-01 9.99809682e-01 9.99979854e-01 9.97198224e-01\n",
      " 9.99727905e-01 9.99885917e-01 9.99882221e-01 9.98558939e-01\n",
      " 9.99981642e-01 9.99098420e-01 9.99981403e-01 9.99765456e-01\n",
      " 8.49434674e-01 9.63041842e-01 3.85229418e-04 1.12969254e-03\n",
      " 9.99484181e-01 1.56823522e-03 9.99963760e-01 9.95431328e-05\n",
      " 1.54209934e-04 9.98723686e-01 2.57620215e-03 4.65975225e-01\n",
      " 4.01434700e-05 9.65373456e-01 4.97666094e-03 9.88476157e-01\n",
      " 9.97175813e-01 9.98330057e-01 1.29846703e-05 8.93378437e-01\n",
      " 9.99616981e-01 5.46867549e-01 9.94697452e-01 9.82637465e-01\n",
      " 1.76346657e-04 5.36219333e-04 4.18205917e-01 3.44113098e-04\n",
      " 1.99358985e-01 9.15961266e-01 1.64460595e-04 1.49764523e-01]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 164 [0/106 (0%)]\tTrain Loss: 0.000049\n",
      "Train Epoch: 164 [10/106 (9%)]\tTrain Loss: 0.069192\n",
      "Train Epoch: 164 [20/106 (19%)]\tTrain Loss: 0.000076\n",
      "Train Epoch: 164 [30/106 (28%)]\tTrain Loss: 0.000053\n",
      "Train Epoch: 164 [40/106 (38%)]\tTrain Loss: 0.000083\n",
      "Train Epoch: 164 [50/106 (47%)]\tTrain Loss: 0.017924\n",
      "Train Epoch: 164 [60/106 (57%)]\tTrain Loss: 0.000247\n",
      "Train Epoch: 164 [70/106 (66%)]\tTrain Loss: 0.000011\n",
      "Train Epoch: 164 [80/106 (75%)]\tTrain Loss: 0.000012\n",
      "Train Epoch: 164 [90/106 (85%)]\tTrain Loss: 0.008033\n",
      "Train Epoch: 164 [100/106 (94%)]\tTrain Loss: 0.000349\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 424/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [4.83411725e-07 4.33857553e-02 1.20312200e-06 1.79741824e-06\n",
      " 1.10341966e-06 1.68488313e-07 1.80658553e-05 2.57433931e-06\n",
      " 1.56817308e-07 4.73627011e-07 4.53138409e-06 8.96996326e-08\n",
      " 4.01851794e-05 1.00434554e-05 2.91283523e-05 7.25191194e-06\n",
      " 3.89938126e-04 1.26543702e-04 3.08054427e-06 9.98835981e-01\n",
      " 4.97903966e-05 9.99327540e-01 1.42692050e-04 9.99571383e-01\n",
      " 9.98536008e-04 9.99919653e-01 9.99999523e-01 7.34493824e-07\n",
      " 7.28991836e-06 9.07722642e-05 9.41122591e-01 8.79258871e-01\n",
      " 5.88777548e-05 4.44085435e-06 1.77247875e-06 6.73151874e-07\n",
      " 1.84627925e-06 7.17994590e-06 2.65698714e-06 2.25668441e-06\n",
      " 7.61473621e-06 1.53041537e-06 4.37953395e-06 2.56345284e-05\n",
      " 2.62591166e-05 5.79323932e-06 3.00389147e-05 4.70733736e-04\n",
      " 9.98999178e-01 4.20375909e-06 1.47745886e-03 5.09429867e-07\n",
      " 3.63414506e-06 5.36844198e-07 2.95075770e-05 7.80488563e-07\n",
      " 4.56917286e-03 2.19630419e-06 5.24771508e-07 1.60590389e-05\n",
      " 8.94375503e-01 5.52011766e-02 9.99926329e-01 9.99848962e-01\n",
      " 9.99926567e-01 2.34440336e-06 1.62230026e-05 9.99995708e-01\n",
      " 2.01114290e-03 9.99917746e-01 1.30986786e-02 3.61806899e-01\n",
      " 9.99645233e-01 9.99961734e-01 9.99994516e-01 9.99587476e-01\n",
      " 9.99965906e-01 9.99978423e-01 9.99445736e-01 9.99943614e-01\n",
      " 9.99983072e-01 9.96025085e-01 9.99988794e-01 9.99980211e-01\n",
      " 9.91511285e-01 9.96534348e-01 2.66583811e-04 1.14804563e-04\n",
      " 9.99838114e-01 1.06804781e-02 9.99988675e-01 9.31613340e-06\n",
      " 1.49906409e-05 9.96324956e-01 3.25521137e-06 2.37593940e-06\n",
      " 2.29004104e-06 9.61447716e-01 6.67959421e-06 9.99195039e-01\n",
      " 9.98601019e-01 9.99835849e-01 1.69852376e-06 9.41966236e-01\n",
      " 9.99926686e-01 2.37291461e-05 9.86292243e-01 9.96329010e-01\n",
      " 3.11218537e-05 1.05956780e-04 9.06791747e-01 8.86296402e-05\n",
      " 3.49058211e-03 3.01773157e-02 2.09887403e-05 3.00612883e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 165 [0/106 (0%)]\tTrain Loss: 0.000102\n",
      "Train Epoch: 165 [10/106 (9%)]\tTrain Loss: 0.000053\n",
      "Train Epoch: 165 [20/106 (19%)]\tTrain Loss: 0.007560\n",
      "Train Epoch: 165 [30/106 (28%)]\tTrain Loss: 0.000012\n",
      "Train Epoch: 165 [40/106 (38%)]\tTrain Loss: 0.000058\n",
      "Train Epoch: 165 [50/106 (47%)]\tTrain Loss: 0.007966\n",
      "Train Epoch: 165 [60/106 (57%)]\tTrain Loss: 0.000009\n",
      "Train Epoch: 165 [70/106 (66%)]\tTrain Loss: 0.000065\n",
      "Train Epoch: 165 [80/106 (75%)]\tTrain Loss: 0.000029\n",
      "Train Epoch: 165 [90/106 (85%)]\tTrain Loss: 0.000048\n",
      "Train Epoch: 165 [100/106 (94%)]\tTrain Loss: 0.000068\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [9.95425253e-08 7.95716711e-04 2.43854657e-07 3.31710908e-07\n",
      " 1.72344812e-07 4.15866239e-08 2.85683632e-06 4.30470237e-07\n",
      " 3.60839785e-08 7.95797419e-08 4.62079555e-07 2.05746638e-08\n",
      " 3.99587998e-06 1.03317370e-06 4.55845611e-06 1.56723866e-06\n",
      " 4.57344297e-03 9.62348422e-05 3.33141656e-07 9.98196900e-01\n",
      " 1.10919082e-05 9.99732196e-01 1.81923632e-03 9.99754369e-01\n",
      " 1.05056036e-02 9.99992251e-01 9.99999881e-01 2.25002367e-07\n",
      " 8.11383813e-07 5.53370910e-06 9.99745429e-01 9.92418528e-01\n",
      " 1.87590103e-05 1.22885740e-06 2.83680095e-07 1.49935815e-07\n",
      " 3.45153495e-07 9.53366964e-07 4.71060275e-07 5.76200819e-07\n",
      " 2.03339710e-06 3.54006715e-07 6.01761315e-07 2.08388201e-06\n",
      " 2.14583633e-06 7.91655225e-07 7.03383876e-06 1.14424329e-04\n",
      " 9.99652028e-01 6.73169836e-07 2.33318925e-01 5.72869077e-08\n",
      " 4.82684811e-07 9.16044556e-08 3.57936915e-06 1.08048482e-07\n",
      " 9.51543868e-01 2.67134993e-07 1.11385425e-07 1.96342239e-06\n",
      " 4.52978581e-01 7.04757811e-04 9.99964952e-01 9.99942064e-01\n",
      " 9.99951839e-01 5.08340918e-07 2.49758295e-06 9.99998689e-01\n",
      " 3.58108373e-04 9.99960423e-01 1.07464895e-01 3.17443997e-01\n",
      " 9.99789774e-01 9.99993563e-01 9.99999404e-01 9.99906540e-01\n",
      " 9.99981999e-01 9.99991536e-01 9.99985695e-01 9.99988317e-01\n",
      " 9.99990940e-01 9.99432147e-01 9.99994636e-01 9.99992371e-01\n",
      " 9.99239445e-01 9.94264662e-01 8.73949102e-06 1.01308215e-05\n",
      " 9.99823987e-01 8.84773582e-02 9.99996543e-01 1.14686054e-06\n",
      " 1.63880316e-06 9.98309374e-01 9.04039041e-07 5.66570804e-07\n",
      " 3.77125446e-07 9.72403407e-01 2.06030177e-06 9.99953747e-01\n",
      " 9.99387622e-01 9.99945998e-01 3.50088385e-07 9.97818351e-01\n",
      " 9.99990463e-01 6.28409862e-06 9.97099876e-01 9.99863029e-01\n",
      " 1.49922989e-05 1.38591622e-05 9.98359501e-01 3.14651952e-05\n",
      " 2.71015614e-01 1.64247349e-01 5.51571111e-06 9.16970432e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "Train Epoch: 166 [0/106 (0%)]\tTrain Loss: 0.000039\n",
      "Train Epoch: 166 [10/106 (9%)]\tTrain Loss: 0.000608\n",
      "Train Epoch: 166 [20/106 (19%)]\tTrain Loss: 0.000005\n",
      "Train Epoch: 166 [30/106 (28%)]\tTrain Loss: 0.000145\n",
      "Train Epoch: 166 [40/106 (38%)]\tTrain Loss: 0.000037\n",
      "Train Epoch: 166 [50/106 (47%)]\tTrain Loss: 0.000071\n",
      "Train Epoch: 166 [60/106 (57%)]\tTrain Loss: 0.000003\n",
      "Train Epoch: 166 [70/106 (66%)]\tTrain Loss: 0.000121\n",
      "Train Epoch: 166 [80/106 (75%)]\tTrain Loss: 0.000066\n",
      "Train Epoch: 166 [90/106 (85%)]\tTrain Loss: 0.000164\n",
      "Train Epoch: 166 [100/106 (94%)]\tTrain Loss: 0.000003\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.50382524e-08 1.19728895e-04 3.87854762e-08 8.98564849e-08\n",
      " 3.51040477e-08 8.02134981e-09 1.75549462e-06 1.10294778e-07\n",
      " 7.11377801e-09 2.05374562e-08 9.26013897e-08 7.04867764e-09\n",
      " 1.30472290e-06 1.73417743e-06 2.41267412e-06 2.79595952e-07\n",
      " 2.11171195e-04 9.08682466e-07 7.89106167e-08 9.97935653e-01\n",
      " 4.48900391e-06 9.99683976e-01 1.38633623e-04 9.99658942e-01\n",
      " 1.05206971e-03 9.99956965e-01 9.99998927e-01 3.69263020e-08\n",
      " 2.85431526e-07 4.74785611e-06 9.97141302e-01 9.54604149e-01\n",
      " 2.05398464e-06 3.06206914e-07 6.76517757e-08 2.78702448e-08\n",
      " 5.80172674e-08 2.63537544e-07 1.22762813e-07 1.24357356e-07\n",
      " 4.38630735e-07 6.43141291e-08 7.80511016e-08 5.24260315e-07\n",
      " 7.18686636e-07 3.24001206e-07 2.61576702e-06 6.82095779e-05\n",
      " 9.99229908e-01 1.83716722e-07 6.75672814e-02 8.70486794e-09\n",
      " 1.22690722e-07 1.98040180e-08 1.27965313e-06 1.81759265e-08\n",
      " 1.28267147e-05 8.83435050e-08 2.61305857e-08 7.31963951e-07\n",
      " 9.65943635e-01 2.00135827e-01 9.99954820e-01 9.99943495e-01\n",
      " 9.99797523e-01 1.38633567e-07 1.34353445e-06 9.99985456e-01\n",
      " 6.39624559e-05 9.99878645e-01 1.61435455e-02 2.44343683e-01\n",
      " 9.99691606e-01 9.99974489e-01 9.99992013e-01 9.96911347e-01\n",
      " 9.99971390e-01 9.99985456e-01 9.94091690e-01 9.99936700e-01\n",
      " 9.99971271e-01 9.95935321e-01 9.99974847e-01 9.99974489e-01\n",
      " 9.98479426e-01 9.04571474e-01 6.21729896e-06 4.99131738e-06\n",
      " 9.99769032e-01 4.49354807e-03 9.99984026e-01 8.44551778e-07\n",
      " 7.84832537e-07 9.98658299e-01 6.99750899e-07 2.93579916e-07\n",
      " 8.77758026e-08 8.94076645e-01 1.00048442e-06 9.99799907e-01\n",
      " 9.98275399e-01 9.99831915e-01 7.77404310e-08 7.59161174e-01\n",
      " 9.99930620e-01 2.62592494e-06 9.94305789e-01 9.99212623e-01\n",
      " 6.59475290e-06 1.66032023e-05 9.87542212e-01 2.89710533e-05\n",
      " 9.55865234e-02 4.39937552e-03 2.44490639e-06 4.12906110e-02]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 167 [0/106 (0%)]\tTrain Loss: 0.008429\n",
      "Train Epoch: 167 [10/106 (9%)]\tTrain Loss: 0.000083\n",
      "Train Epoch: 167 [20/106 (19%)]\tTrain Loss: 0.073792\n",
      "Train Epoch: 167 [30/106 (28%)]\tTrain Loss: 0.000297\n",
      "Train Epoch: 167 [40/106 (38%)]\tTrain Loss: 0.000008\n",
      "Train Epoch: 167 [50/106 (47%)]\tTrain Loss: 0.000193\n",
      "Train Epoch: 167 [60/106 (57%)]\tTrain Loss: 0.000012\n",
      "Train Epoch: 167 [70/106 (66%)]\tTrain Loss: 0.000041\n",
      "Train Epoch: 167 [80/106 (75%)]\tTrain Loss: 0.000074\n",
      "Train Epoch: 167 [90/106 (85%)]\tTrain Loss: 0.000060\n",
      "Train Epoch: 167 [100/106 (94%)]\tTrain Loss: 0.000098\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 421/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [8.43122763e-08 5.42556925e-04 1.21934150e-07 2.57313161e-07\n",
      " 9.73487104e-08 2.89811890e-08 2.60348293e-06 3.30819717e-07\n",
      " 2.22946444e-08 5.49800561e-08 3.00000949e-07 2.10351168e-08\n",
      " 3.04447485e-06 2.63831521e-06 5.64729817e-06 8.34232594e-07\n",
      " 8.39378312e-03 1.99596434e-05 2.53428993e-07 9.99608576e-01\n",
      " 1.68909755e-05 9.99950886e-01 4.69820291e-01 9.99855518e-01\n",
      " 1.24594286e-01 9.99994397e-01 9.99999404e-01 1.18235057e-07\n",
      " 9.17427371e-07 1.39990389e-05 9.99647737e-01 9.95849133e-01\n",
      " 3.57039862e-05 1.06123628e-06 1.92661687e-07 6.95125664e-08\n",
      " 1.55009246e-07 9.22041067e-07 3.77318287e-07 3.02804040e-07\n",
      " 1.45492106e-06 2.23417928e-07 3.15187862e-07 1.90785704e-06\n",
      " 2.35490620e-06 9.95624418e-07 1.47142928e-05 1.01745885e-03\n",
      " 9.99814451e-01 6.06956633e-07 2.08650425e-01 2.56348542e-08\n",
      " 4.51135378e-07 5.68135619e-08 4.14406850e-06 6.02079311e-08\n",
      " 2.91882306e-01 2.29618763e-07 8.19442647e-08 2.36941150e-06\n",
      " 9.94480371e-01 8.26485813e-01 9.99988198e-01 9.99983788e-01\n",
      " 9.99972820e-01 4.39539832e-07 4.08486676e-06 9.99998689e-01\n",
      " 6.41726656e-04 9.99975801e-01 5.67736141e-02 3.40132356e-01\n",
      " 9.99913573e-01 9.99994516e-01 9.99998927e-01 9.99893069e-01\n",
      " 9.99990106e-01 9.99993682e-01 9.99936819e-01 9.99990702e-01\n",
      " 9.99994397e-01 9.99651074e-01 9.99995828e-01 9.99994040e-01\n",
      " 9.99781787e-01 9.95580018e-01 1.72551190e-05 1.60267264e-05\n",
      " 9.99954104e-01 4.83067155e-01 9.99995708e-01 2.10840767e-06\n",
      " 2.37481140e-06 9.99637127e-01 2.65044014e-06 8.01511192e-07\n",
      " 2.48463493e-07 9.78131711e-01 2.82722840e-06 9.99967098e-01\n",
      " 9.99442637e-01 9.99948740e-01 2.01908492e-07 9.98408496e-01\n",
      " 9.99990702e-01 1.43483512e-05 9.99026775e-01 9.99808967e-01\n",
      " 2.03973523e-05 1.13546637e-04 9.98451829e-01 3.85283456e-05\n",
      " 8.29131126e-01 1.57432377e-01 5.84531699e-06 9.98003185e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 168 [0/106 (0%)]\tTrain Loss: 0.000036\n",
      "Train Epoch: 168 [10/106 (9%)]\tTrain Loss: 0.000275\n",
      "Train Epoch: 168 [20/106 (19%)]\tTrain Loss: 0.096712\n",
      "Train Epoch: 168 [30/106 (28%)]\tTrain Loss: 0.000993\n",
      "Train Epoch: 168 [40/106 (38%)]\tTrain Loss: 0.000004\n",
      "Train Epoch: 168 [50/106 (47%)]\tTrain Loss: 0.061259\n",
      "Train Epoch: 168 [60/106 (57%)]\tTrain Loss: 0.000014\n",
      "Train Epoch: 168 [70/106 (66%)]\tTrain Loss: 0.007752\n",
      "Train Epoch: 168 [80/106 (75%)]\tTrain Loss: 0.000003\n",
      "Train Epoch: 168 [90/106 (85%)]\tTrain Loss: 0.000046\n",
      "Train Epoch: 168 [100/106 (94%)]\tTrain Loss: 0.000012\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 419/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.02465182e-08 5.48046410e-05 3.59975694e-08 7.97275703e-08\n",
      " 3.12230775e-08 1.15174581e-08 6.06283265e-07 9.64821396e-08\n",
      " 9.48660706e-09 2.06624406e-08 7.68753381e-08 8.08356848e-09\n",
      " 8.06751245e-07 5.08765709e-07 1.27934811e-06 2.06276781e-07\n",
      " 3.53961077e-04 4.14158285e-06 7.02973608e-08 9.98732626e-01\n",
      " 2.56569228e-06 9.99661207e-01 7.72600470e-04 9.99553621e-01\n",
      " 4.53372905e-03 9.99973059e-01 9.99997258e-01 3.53124250e-08\n",
      " 2.40780963e-07 2.15112050e-06 9.98700857e-01 9.11771297e-01\n",
      " 3.63841696e-06 2.58010544e-07 6.21172447e-08 2.11678532e-08\n",
      " 4.35444356e-08 2.39464441e-07 9.90678686e-08 8.93195988e-08\n",
      " 3.55292997e-07 5.93957097e-08 1.02263137e-07 4.56879150e-07\n",
      " 4.23530167e-07 2.68128105e-07 3.16934143e-06 6.34296957e-05\n",
      " 9.99275982e-01 1.71540663e-07 2.66881916e-03 8.18302137e-09\n",
      " 1.20277505e-07 1.37158533e-08 7.44836200e-07 1.54721178e-08\n",
      " 1.02073692e-01 5.44161693e-08 3.05242374e-08 6.43291571e-07\n",
      " 8.32283258e-01 3.58100655e-03 9.99951959e-01 9.99940634e-01\n",
      " 9.99904633e-01 1.07229724e-07 9.53415110e-07 9.99991536e-01\n",
      " 1.96508947e-04 9.99927521e-01 3.61892208e-02 1.57894388e-01\n",
      " 9.99679804e-01 9.99977708e-01 9.99996066e-01 9.99529123e-01\n",
      " 9.99963284e-01 9.99965191e-01 9.99832630e-01 9.99955654e-01\n",
      " 9.99982715e-01 9.98724401e-01 9.99981523e-01 9.99974132e-01\n",
      " 9.98372138e-01 9.66145098e-01 2.14312445e-06 5.59971841e-06\n",
      " 9.99815404e-01 1.18876470e-03 9.99980807e-01 3.87107661e-07\n",
      " 6.11760242e-07 9.99011874e-01 8.31388490e-07 2.44467770e-07\n",
      " 7.30313516e-08 4.89592016e-01 5.75339300e-07 9.99831796e-01\n",
      " 9.97499526e-01 9.99731362e-01 5.17586471e-08 9.96097803e-01\n",
      " 9.99955297e-01 3.42868861e-06 9.93642211e-01 9.99650002e-01\n",
      " 3.65865708e-06 1.18896714e-05 9.85996068e-01 7.62437185e-06\n",
      " 1.15573213e-01 1.91792846e-02 1.85842396e-06 3.19891185e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 169 [0/106 (0%)]\tTrain Loss: 0.000033\n",
      "Train Epoch: 169 [10/106 (9%)]\tTrain Loss: 0.000046\n",
      "Train Epoch: 169 [20/106 (19%)]\tTrain Loss: 0.000042\n",
      "Train Epoch: 169 [30/106 (28%)]\tTrain Loss: 0.024345\n",
      "Train Epoch: 169 [40/106 (38%)]\tTrain Loss: 0.000144\n",
      "Train Epoch: 169 [50/106 (47%)]\tTrain Loss: 0.000004\n",
      "Train Epoch: 169 [60/106 (57%)]\tTrain Loss: 0.000005\n",
      "Train Epoch: 169 [70/106 (66%)]\tTrain Loss: 0.000202\n",
      "Train Epoch: 169 [80/106 (75%)]\tTrain Loss: 0.000139\n",
      "Train Epoch: 169 [90/106 (85%)]\tTrain Loss: 0.000007\n",
      "Train Epoch: 169 [100/106 (94%)]\tTrain Loss: 0.000206\n",
      "\n",
      "Train set: Average loss: 0.0018, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.50179796e-08 2.22716611e-04 4.41500063e-08 7.94234083e-08\n",
      " 3.12946824e-08 1.17249011e-08 1.10832480e-06 1.17467870e-07\n",
      " 7.99968447e-09 1.50843320e-08 6.97931455e-08 5.45504086e-09\n",
      " 7.55587166e-07 7.00246289e-07 1.91311892e-06 2.09660456e-07\n",
      " 2.61080510e-04 1.00874186e-05 7.01607306e-08 9.98478830e-01\n",
      " 4.16307012e-06 9.99839902e-01 7.59535236e-04 9.99614835e-01\n",
      " 1.20335761e-02 9.99990225e-01 9.99999523e-01 2.99080511e-08\n",
      " 3.04541686e-07 3.13013038e-06 9.98943150e-01 9.58440125e-01\n",
      " 4.70168106e-06 3.80853606e-07 6.08857036e-08 2.00911909e-08\n",
      " 3.95915123e-08 3.22509749e-07 1.38402925e-07 9.77815660e-08\n",
      " 5.74921387e-07 6.06279684e-08 1.20055873e-07 6.10507186e-07\n",
      " 5.64229026e-07 3.68323299e-07 5.24899406e-06 9.35912831e-05\n",
      " 9.99100208e-01 1.65796109e-07 2.95577710e-03 7.50350715e-09\n",
      " 1.41459040e-07 1.46042938e-08 1.48182949e-06 1.48686672e-08\n",
      " 1.22610573e-03 5.86825735e-08 2.52592880e-08 6.48917762e-07\n",
      " 1.38418272e-01 1.84010059e-04 9.99947786e-01 9.99943495e-01\n",
      " 9.99942899e-01 1.35825672e-07 1.22674624e-06 9.99997497e-01\n",
      " 2.00674796e-04 9.99956965e-01 1.68960262e-02 1.74929544e-01\n",
      " 9.99719918e-01 9.99990344e-01 9.99998927e-01 9.99692798e-01\n",
      " 9.99967456e-01 9.99975801e-01 9.99920607e-01 9.99983311e-01\n",
      " 9.99989986e-01 9.98706222e-01 9.99994040e-01 9.99981165e-01\n",
      " 9.98678148e-01 9.39476788e-01 6.31335843e-06 1.05391473e-05\n",
      " 9.99889731e-01 1.22533422e-02 9.99994040e-01 6.10400093e-07\n",
      " 7.31271086e-07 9.98379946e-01 7.39614563e-07 2.12243989e-07\n",
      " 7.32403223e-08 8.39227974e-01 7.66361836e-07 9.99907255e-01\n",
      " 9.97030973e-01 9.99825656e-01 4.26339319e-08 9.90379453e-01\n",
      " 9.99982715e-01 3.99618511e-06 9.93008494e-01 9.98891532e-01\n",
      " 6.29155829e-06 1.29135406e-05 9.89115119e-01 1.28767215e-05\n",
      " 9.71755385e-02 6.19619794e-04 2.41216344e-06 9.42442060e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "Train Epoch: 170 [0/106 (0%)]\tTrain Loss: 0.000004\n",
      "Train Epoch: 170 [10/106 (9%)]\tTrain Loss: 0.000004\n",
      "Train Epoch: 170 [20/106 (19%)]\tTrain Loss: 0.006899\n",
      "Train Epoch: 170 [30/106 (28%)]\tTrain Loss: 0.046926\n",
      "Train Epoch: 170 [40/106 (38%)]\tTrain Loss: 0.000180\n",
      "Train Epoch: 170 [50/106 (47%)]\tTrain Loss: 0.000063\n",
      "Train Epoch: 170 [60/106 (57%)]\tTrain Loss: 0.000003\n",
      "Train Epoch: 170 [70/106 (66%)]\tTrain Loss: 0.000039\n",
      "Train Epoch: 170 [80/106 (75%)]\tTrain Loss: 0.000006\n",
      "Train Epoch: 170 [90/106 (85%)]\tTrain Loss: 0.000056\n",
      "Train Epoch: 170 [100/106 (94%)]\tTrain Loss: 0.000095\n",
      "\n",
      "Train set: Average loss: 0.0002, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.90456565e-08 8.96799393e-05 3.53064316e-08 6.31452863e-08\n",
      " 1.89611757e-08 6.19818241e-09 8.73065972e-07 6.87045585e-08\n",
      " 4.95606356e-09 1.12024141e-08 4.03787013e-08 4.58630334e-09\n",
      " 3.91139992e-07 3.77644454e-07 1.23210555e-06 1.28874390e-07\n",
      " 5.95727106e-05 5.97813960e-05 5.43674119e-08 9.92114723e-01\n",
      " 1.68448605e-06 9.99781668e-01 1.73660576e-01 9.99528170e-01\n",
      " 1.52399810e-02 9.99976277e-01 9.99994278e-01 1.72134467e-08\n",
      " 1.98822733e-07 1.77409549e-06 9.85387444e-01 8.18912745e-01\n",
      " 2.00695267e-05 2.47601520e-07 3.72875384e-08 9.69706360e-09\n",
      " 2.20320864e-08 2.52500939e-07 1.07640588e-07 4.04835774e-08\n",
      " 2.48364927e-07 4.47326727e-08 7.31199989e-08 3.78192254e-07\n",
      " 3.03268479e-07 3.00664027e-07 4.58264276e-06 8.74524412e-05\n",
      " 9.99037147e-01 1.26987743e-07 2.32743390e-04 4.62548444e-09\n",
      " 7.60002976e-08 8.98016239e-09 5.95749157e-07 8.54875370e-09\n",
      " 6.80824496e-06 2.67328097e-08 2.20525624e-08 4.33459775e-07\n",
      " 6.80410936e-02 4.24548343e-04 9.99899745e-01 9.99906182e-01\n",
      " 9.99836683e-01 1.00254390e-07 8.15339831e-07 9.99991059e-01\n",
      " 1.64075274e-04 9.99923110e-01 1.13955357e-04 3.65562097e-04\n",
      " 9.99393940e-01 9.99966979e-01 9.99993682e-01 9.99239802e-01\n",
      " 9.99891400e-01 9.99886513e-01 9.99906659e-01 9.99935269e-01\n",
      " 9.99978304e-01 9.89745855e-01 9.99977708e-01 9.99967456e-01\n",
      " 9.99038696e-01 9.54629898e-01 3.30538865e-06 7.95662982e-06\n",
      " 9.99771774e-01 1.64220808e-03 9.99975085e-01 4.31993101e-07\n",
      " 7.07621382e-07 9.98631537e-01 3.99552732e-07 1.93893712e-07\n",
      " 5.32102966e-08 1.72514897e-02 2.77204038e-07 9.99647975e-01\n",
      " 9.82994080e-01 9.99710500e-01 2.51968295e-08 5.49432077e-02\n",
      " 9.99932528e-01 2.70554165e-06 9.88552451e-01 9.98802423e-01\n",
      " 3.29057480e-06 2.52090263e-06 9.37101901e-01 8.99517272e-06\n",
      " 3.64121422e-02 1.50684512e-03 9.93233584e-07 9.94265139e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 33 TN= 52 FN= 23 FP= 8\n",
      "TP+FP 41\n",
      "precision 0.8048780487804879\n",
      "recall 0.5892857142857143\n",
      "F1 0.6804123711340206\n",
      "acc 0.7327586206896551\n",
      "AUCp 0.7279761904761906\n",
      "AUC 0.8294642857142858\n",
      "\n",
      " The epoch is 170, average recall: 0.5893, average precision: 0.8049,average F1: 0.6804, average accuracy: 0.7328, average AUC: 0.8295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 171 [0/106 (0%)]\tTrain Loss: 0.000191\n",
      "Train Epoch: 171 [10/106 (9%)]\tTrain Loss: 0.000040\n",
      "Train Epoch: 171 [20/106 (19%)]\tTrain Loss: 0.000006\n",
      "Train Epoch: 171 [30/106 (28%)]\tTrain Loss: 0.007641\n",
      "Train Epoch: 171 [40/106 (38%)]\tTrain Loss: 0.000052\n",
      "Train Epoch: 171 [50/106 (47%)]\tTrain Loss: 0.007279\n",
      "Train Epoch: 171 [60/106 (57%)]\tTrain Loss: 0.029013\n",
      "Train Epoch: 171 [70/106 (66%)]\tTrain Loss: 0.000021\n",
      "Train Epoch: 171 [80/106 (75%)]\tTrain Loss: 0.000005\n",
      "Train Epoch: 171 [90/106 (85%)]\tTrain Loss: 0.000026\n",
      "Train Epoch: 171 [100/106 (94%)]\tTrain Loss: 0.000004\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 421/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.05054836e-08 1.89672828e-05 8.37535019e-09 2.53434482e-08\n",
      " 6.70210198e-09 3.38109607e-09 2.30829187e-07 2.73469336e-08\n",
      " 2.79753620e-09 5.53655033e-09 1.21773036e-08 2.52568633e-09\n",
      " 1.02561607e-07 2.75671567e-07 7.37432572e-07 3.05926662e-08\n",
      " 2.86760996e-06 3.83215934e-07 1.36387390e-08 9.87391055e-01\n",
      " 7.08064192e-07 9.98304963e-01 2.03119544e-06 9.99197900e-01\n",
      " 6.61712875e-06 9.99843121e-01 9.99986172e-01 5.09022691e-09\n",
      " 4.52947688e-08 4.97384178e-07 9.64766026e-01 1.02751413e-02\n",
      " 9.85473093e-07 7.13809598e-08 1.35225910e-08 3.36596861e-09\n",
      " 7.81763720e-09 5.81540895e-08 2.76924155e-08 1.11123324e-08\n",
      " 5.38810774e-08 1.03404805e-08 2.29221158e-08 9.63997309e-08\n",
      " 6.22242027e-08 7.33036245e-08 9.50281560e-07 8.84549536e-06\n",
      " 9.97859538e-01 4.45310029e-08 8.39853019e-05 1.79264092e-09\n",
      " 3.18927462e-08 3.51465701e-09 3.10142156e-07 3.22418514e-09\n",
      " 2.02546767e-06 1.07735856e-08 7.79392018e-09 1.97432641e-07\n",
      " 2.33629376e-01 2.54684011e-04 9.99873281e-01 9.99845624e-01\n",
      " 9.99393463e-01 3.77290874e-08 4.23918891e-07 9.99974012e-01\n",
      " 7.47870145e-05 9.99855757e-01 1.43677811e-04 1.33611576e-03\n",
      " 9.99017239e-01 9.99907613e-01 9.99969721e-01 9.95113969e-01\n",
      " 9.99922156e-01 9.99922276e-01 9.97551262e-01 9.99864101e-01\n",
      " 9.99937773e-01 9.90121245e-01 9.99952197e-01 9.99909520e-01\n",
      " 9.75449145e-01 6.35026917e-02 1.60838158e-06 2.54558063e-06\n",
      " 9.99493957e-01 2.48124252e-05 9.99934077e-01 1.21101493e-07\n",
      " 1.96680929e-07 9.96637940e-01 1.52298696e-07 9.67342118e-08\n",
      " 1.78932122e-08 2.54818611e-02 1.83450098e-07 9.99106228e-01\n",
      " 9.87740457e-01 9.99407887e-01 1.13101475e-08 4.02842183e-04\n",
      " 9.99836087e-01 8.18822173e-07 9.84456837e-01 9.95625377e-01\n",
      " 1.41168368e-06 1.15008822e-06 8.43540907e-01 2.55741134e-06\n",
      " 4.64545476e-04 2.47196549e-05 5.14192777e-07 2.41767139e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 172 [0/106 (0%)]\tTrain Loss: 0.000781\n",
      "Train Epoch: 172 [10/106 (9%)]\tTrain Loss: 0.000127\n",
      "Train Epoch: 172 [20/106 (19%)]\tTrain Loss: 0.000059\n",
      "Train Epoch: 172 [30/106 (28%)]\tTrain Loss: 0.000002\n",
      "Train Epoch: 172 [40/106 (38%)]\tTrain Loss: 0.000730\n",
      "Train Epoch: 172 [50/106 (47%)]\tTrain Loss: 0.000003\n",
      "Train Epoch: 172 [60/106 (57%)]\tTrain Loss: 0.000006\n",
      "Train Epoch: 172 [70/106 (66%)]\tTrain Loss: 0.000006\n",
      "Train Epoch: 172 [80/106 (75%)]\tTrain Loss: 0.000025\n",
      "Train Epoch: 172 [90/106 (85%)]\tTrain Loss: 0.077340\n",
      "Train Epoch: 172 [100/106 (94%)]\tTrain Loss: 0.000030\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 421/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [8.59187566e-09 3.98410066e-06 5.76109915e-09 2.02217514e-08\n",
      " 6.03541617e-09 3.00151859e-09 8.62323688e-08 2.42807783e-08\n",
      " 2.87942248e-09 5.67773517e-09 9.20154086e-09 2.66238520e-09\n",
      " 6.56190196e-08 2.98512930e-07 5.61508500e-07 3.85684480e-08\n",
      " 1.87324645e-06 2.99371585e-08 9.79289716e-09 9.23986971e-01\n",
      " 2.83493478e-07 8.79051447e-01 1.16241631e-06 9.99303937e-01\n",
      " 1.21639414e-06 9.99612868e-01 9.99980927e-01 3.12889981e-09\n",
      " 2.21054961e-08 1.15235281e-07 4.57998860e-04 7.31863081e-04\n",
      " 5.26783765e-07 7.61249126e-08 1.17348353e-08 3.41982909e-09\n",
      " 7.10996595e-09 3.95037105e-08 1.51693893e-08 7.09390102e-09\n",
      " 2.49532004e-08 8.38235081e-09 1.40774246e-08 8.63031246e-08\n",
      " 7.91797348e-08 4.10773602e-08 6.70326756e-07 2.99992348e-06\n",
      " 9.98305082e-01 3.49381892e-08 1.33842786e-05 1.89254679e-09\n",
      " 2.87668467e-08 3.62297481e-09 1.94065748e-07 3.17529447e-09\n",
      " 4.84842076e-07 6.91049795e-09 5.46524648e-09 1.69992887e-07\n",
      " 1.17583945e-03 4.11088195e-06 9.99829292e-01 9.99762237e-01\n",
      " 9.97716904e-01 7.22808124e-08 7.19537013e-07 9.99965787e-01\n",
      " 3.03978650e-05 9.99902129e-01 5.11837497e-05 9.85503895e-04\n",
      " 9.99603689e-01 9.99874353e-01 9.99923706e-01 1.79525577e-02\n",
      " 9.99937892e-01 9.99945045e-01 2.02554089e-04 9.99893904e-01\n",
      " 9.99932051e-01 9.87369597e-01 9.99954939e-01 9.99893665e-01\n",
      " 6.54327720e-02 9.22967315e-01 3.00343032e-07 5.82106793e-07\n",
      " 9.99557555e-01 1.08893200e-05 9.99890685e-01 1.01097370e-07\n",
      " 1.07961959e-07 9.97612596e-01 1.61006980e-07 1.34105832e-07\n",
      " 1.69321019e-08 1.10192079e-04 9.68254028e-08 9.98135328e-01\n",
      " 9.39613402e-01 9.99538541e-01 1.40070116e-08 1.85442681e-03\n",
      " 9.99803960e-01 1.59507988e-06 9.94994521e-01 9.94817555e-01\n",
      " 6.28528824e-07 5.15685656e-07 2.49241963e-02 2.13596036e-06\n",
      " 2.12951083e-04 1.62288525e-05 4.47637404e-07 9.28729605e-07]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 173 [0/106 (0%)]\tTrain Loss: 0.056977\n",
      "Train Epoch: 173 [10/106 (9%)]\tTrain Loss: 0.010508\n",
      "Train Epoch: 173 [20/106 (19%)]\tTrain Loss: 0.000009\n",
      "Train Epoch: 173 [30/106 (28%)]\tTrain Loss: 0.000025\n",
      "Train Epoch: 173 [40/106 (38%)]\tTrain Loss: 0.000043\n",
      "Train Epoch: 173 [50/106 (47%)]\tTrain Loss: 0.000003\n",
      "Train Epoch: 173 [60/106 (57%)]\tTrain Loss: 0.000142\n",
      "Train Epoch: 173 [70/106 (66%)]\tTrain Loss: 0.000006\n",
      "Train Epoch: 173 [80/106 (75%)]\tTrain Loss: 0.000048\n",
      "Train Epoch: 173 [90/106 (85%)]\tTrain Loss: 0.000086\n",
      "Train Epoch: 173 [100/106 (94%)]\tTrain Loss: 0.000050\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.24142886e-07 1.58447700e-04 1.87134148e-07 3.45526956e-07\n",
      " 1.31813707e-07 4.08215683e-08 3.05524827e-06 3.71754112e-07\n",
      " 3.71388396e-08 1.23099056e-07 2.83996798e-07 3.84544485e-08\n",
      " 2.31847207e-06 3.60011177e-06 9.40545669e-06 4.66302311e-07\n",
      " 9.07820431e-05 7.65039374e-07 8.03077342e-07 9.98360455e-01\n",
      " 1.60079671e-05 9.99364197e-01 1.94023363e-03 9.99259174e-01\n",
      " 4.80730878e-03 9.99701798e-01 9.99998689e-01 3.34208927e-08\n",
      " 2.59141189e-07 1.10357439e-06 2.39879819e-06 2.19522335e-05\n",
      " 2.73158957e-06 2.38939606e-06 2.01874599e-07 1.54944516e-07\n",
      " 6.77420303e-07 1.66368045e-06 2.35947340e-07 1.95800396e-07\n",
      " 1.76771164e-06 3.97840950e-07 2.74273077e-07 2.06748018e-06\n",
      " 4.02181286e-06 6.76621596e-07 9.88742704e-06 1.59813295e-04\n",
      " 9.98824418e-01 9.45800707e-07 7.67957710e-04 4.88870384e-08\n",
      " 7.44040335e-07 6.06551538e-08 3.62098490e-06 1.05612393e-07\n",
      " 3.20006848e-06 2.59756547e-07 9.11876441e-08 3.51140716e-06\n",
      " 3.85525018e-01 4.99513641e-04 9.99903321e-01 9.99898195e-01\n",
      " 9.98809576e-01 1.66649602e-06 5.77959872e-06 9.99964595e-01\n",
      " 2.91841868e-02 9.99912143e-01 3.26937210e-04 2.63703550e-04\n",
      " 9.99456942e-01 9.99905586e-01 9.99855757e-01 3.34084530e-06\n",
      " 9.99958754e-01 9.99969602e-01 1.58563944e-05 9.99778330e-01\n",
      " 9.99973416e-01 2.53389153e-04 9.99977112e-01 9.99826849e-01\n",
      " 6.23680234e-01 1.62929017e-03 3.90873993e-06 3.72145405e-06\n",
      " 9.99504685e-01 2.54535174e-04 9.99965310e-01 1.39411713e-06\n",
      " 1.78952143e-06 9.98518050e-01 1.45342324e-06 2.91797301e-06\n",
      " 4.77830895e-07 3.46345711e-04 2.04769276e-06 9.97230113e-01\n",
      " 6.85692802e-02 9.99215364e-01 2.12203304e-07 4.32156511e-02\n",
      " 9.99845147e-01 2.18157875e-05 9.93117452e-01 9.99420404e-01\n",
      " 2.10344268e-04 5.35938580e-06 9.98931825e-01 1.10887020e-04\n",
      " 2.44611636e-01 1.08474924e-03 3.16659816e-06 9.57421980e-06]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 174 [0/106 (0%)]\tTrain Loss: 0.000009\n",
      "Train Epoch: 174 [10/106 (9%)]\tTrain Loss: 0.000014\n",
      "Train Epoch: 174 [20/106 (19%)]\tTrain Loss: 0.000077\n",
      "Train Epoch: 174 [30/106 (28%)]\tTrain Loss: 0.000150\n",
      "Train Epoch: 174 [40/106 (38%)]\tTrain Loss: 0.000110\n",
      "Train Epoch: 174 [50/106 (47%)]\tTrain Loss: 0.000099\n",
      "Train Epoch: 174 [60/106 (57%)]\tTrain Loss: 0.000093\n",
      "Train Epoch: 174 [70/106 (66%)]\tTrain Loss: 0.000017\n",
      "Train Epoch: 174 [80/106 (75%)]\tTrain Loss: 0.000006\n",
      "Train Epoch: 174 [90/106 (85%)]\tTrain Loss: 0.054565\n",
      "Train Epoch: 174 [100/106 (94%)]\tTrain Loss: 0.000154\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 423/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [6.99408034e-08 1.34911919e-02 1.26158341e-07 9.19810574e-08\n",
      " 3.62763721e-08 1.37795721e-08 3.08351038e-04 1.15308275e-07\n",
      " 8.23610335e-09 5.50338832e-08 5.13918224e-08 7.60417329e-09\n",
      " 2.12274969e-07 1.97832591e-07 1.54800512e-06 1.41992658e-07\n",
      " 3.16451513e-03 6.81943391e-07 1.64250679e-07 9.99907017e-01\n",
      " 5.17653962e-06 9.99984622e-01 9.92532849e-01 9.99929905e-01\n",
      " 9.94615138e-01 9.99999881e-01 1.00000000e+00 1.74629974e-07\n",
      " 8.94849990e-08 2.41281583e-07 9.98581886e-01 2.48110358e-04\n",
      " 1.56364040e-05 1.31219997e-06 2.31872708e-08 5.79896060e-08\n",
      " 1.97095531e-07 6.19693253e-07 8.47880131e-08 7.01641341e-07\n",
      " 5.21922350e-01 1.09516168e-06 1.69577405e-07 2.74419364e-07\n",
      " 5.85994087e-07 1.74434561e-07 2.14103648e-06 5.80205278e-06\n",
      " 9.99814332e-01 6.59253658e-07 3.76965366e-02 1.31406370e-08\n",
      " 9.37072429e-08 9.52893320e-09 9.49029868e-07 3.22016724e-08\n",
      " 1.67676043e-02 2.54130548e-08 3.42754944e-08 3.86527773e-07\n",
      " 1.23548135e-03 4.13221733e-06 9.99980807e-01 9.99987602e-01\n",
      " 9.99875784e-01 2.48533865e-07 4.93409459e-07 9.99997139e-01\n",
      " 4.13466953e-02 9.99932051e-01 3.64274347e-05 1.34678103e-05\n",
      " 9.98766065e-01 9.99995232e-01 9.99999881e-01 9.89606321e-01\n",
      " 9.99974966e-01 9.99992609e-01 9.99999523e-01 9.99840379e-01\n",
      " 9.99998093e-01 8.00269544e-01 9.99995112e-01 9.99995232e-01\n",
      " 9.99806702e-01 8.65105540e-04 4.81716779e-06 3.85965803e-03\n",
      " 9.99791563e-01 9.07034986e-03 9.99999285e-01 2.40808049e-07\n",
      " 3.86490171e-07 9.99314189e-01 1.92512175e-07 2.77144579e-07\n",
      " 6.16881763e-08 1.86051650e-04 2.95671242e-07 9.99970436e-01\n",
      " 9.99580443e-01 9.99996185e-01 4.12796837e-08 9.03634136e-05\n",
      " 9.99684095e-01 2.28507133e-06 9.96488571e-01 9.99983549e-01\n",
      " 1.27076841e-04 2.25472562e-07 9.99976873e-01 2.42925758e-04\n",
      " 1.11234178e-04 3.91073854e-06 6.06265166e-06 9.99979854e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "Train Epoch: 175 [0/106 (0%)]\tTrain Loss: 0.000033\n",
      "Train Epoch: 175 [10/106 (9%)]\tTrain Loss: 0.000059\n",
      "Train Epoch: 175 [20/106 (19%)]\tTrain Loss: 0.000004\n",
      "Train Epoch: 175 [30/106 (28%)]\tTrain Loss: 0.000059\n",
      "Train Epoch: 175 [40/106 (38%)]\tTrain Loss: 0.000022\n",
      "Train Epoch: 175 [50/106 (47%)]\tTrain Loss: 0.006782\n",
      "Train Epoch: 175 [60/106 (57%)]\tTrain Loss: 0.000098\n",
      "Train Epoch: 175 [70/106 (66%)]\tTrain Loss: 0.000053\n",
      "Train Epoch: 175 [80/106 (75%)]\tTrain Loss: 0.069140\n",
      "Train Epoch: 175 [90/106 (85%)]\tTrain Loss: 0.000072\n",
      "Train Epoch: 175 [100/106 (94%)]\tTrain Loss: 0.000101\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 419/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.98131002e-05 3.28424852e-04 1.02782258e-06 9.76111460e-06\n",
      " 5.54908729e-06 1.58090495e-06 1.00511788e-05 1.42163954e-05\n",
      " 9.30040528e-07 5.89242700e-06 9.76487627e-06 1.42139936e-06\n",
      " 2.53530925e-05 1.50475189e-05 1.01383870e-04 4.17510500e-06\n",
      " 2.26332268e-04 1.38825235e-05 3.77207434e-05 9.99988914e-01\n",
      " 9.49105024e-01 9.94669020e-01 6.37722248e-03 9.99975562e-01\n",
      " 6.27071410e-03 9.99995351e-01 1.00000000e+00 7.93561526e-07\n",
      " 1.77619459e-05 4.59507683e-05 3.17907979e-04 3.48074042e-04\n",
      " 2.55940016e-04 8.23384034e-05 6.32848287e-06 7.20435037e-06\n",
      " 2.06784880e-05 2.16624569e-04 2.10464841e-05 6.41285224e-05\n",
      " 9.65623418e-04 2.19570764e-04 1.47814217e-05 3.88786866e-05\n",
      " 8.39752975e-05 3.36921476e-05 1.76927759e-04 9.35833261e-04\n",
      " 9.98852968e-01 3.14098797e-05 1.65376619e-01 9.81472112e-07\n",
      " 6.92885369e-05 5.85317707e-07 2.71836441e-04 7.76487741e-06\n",
      " 6.40574217e-01 6.94701475e-06 4.67021573e-06 3.54660267e-04\n",
      " 1.14739768e-03 2.56350431e-05 9.99563277e-01 9.99806821e-01\n",
      " 9.96724308e-01 3.15480283e-05 1.24868457e-04 9.98378754e-01\n",
      " 3.84931304e-02 9.99904156e-01 3.05051506e-01 1.05922045e-02\n",
      " 9.99206841e-01 9.99967337e-01 9.99988437e-01 6.13417476e-04\n",
      " 9.99856949e-01 9.99888062e-01 4.87638218e-03 9.95739937e-01\n",
      " 9.99992132e-01 9.94466364e-01 9.99997139e-01 9.99585688e-01\n",
      " 5.41245759e-01 5.92642045e-03 1.50701861e-04 2.30113656e-04\n",
      " 9.99168992e-01 2.86964059e-04 9.99996662e-01 3.73487012e-04\n",
      " 7.38302129e-04 9.99809563e-01 7.37058872e-05 1.58483032e-04\n",
      " 1.93692613e-05 4.49149981e-02 5.79417829e-05 9.83898938e-01\n",
      " 9.90099728e-01 9.99813259e-01 1.80592508e-06 1.01996085e-03\n",
      " 9.97209728e-01 5.22496760e-01 9.97408450e-01 9.98241663e-01\n",
      " 9.51314811e-04 1.40010452e-04 9.99964833e-01 5.73959649e-02\n",
      " 2.29449272e-01 2.50703073e-03 7.96654131e-05 1.55775444e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 176 [0/106 (0%)]\tTrain Loss: 0.000023\n",
      "Train Epoch: 176 [10/106 (9%)]\tTrain Loss: 0.000035\n",
      "Train Epoch: 176 [20/106 (19%)]\tTrain Loss: 0.368837\n",
      "Train Epoch: 176 [30/106 (28%)]\tTrain Loss: 0.121035\n",
      "Train Epoch: 176 [40/106 (38%)]\tTrain Loss: 0.000143\n",
      "Train Epoch: 176 [50/106 (47%)]\tTrain Loss: 0.000426\n",
      "Train Epoch: 176 [60/106 (57%)]\tTrain Loss: 0.007816\n",
      "Train Epoch: 176 [70/106 (66%)]\tTrain Loss: 0.000181\n",
      "Train Epoch: 176 [80/106 (75%)]\tTrain Loss: 0.000068\n",
      "Train Epoch: 176 [90/106 (85%)]\tTrain Loss: 0.000007\n",
      "Train Epoch: 176 [100/106 (94%)]\tTrain Loss: 0.000287\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 416/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.25691492e-05 1.71166062e-02 1.08362816e-04 1.00927891e-05\n",
      " 1.34501479e-05 5.25390715e-05 5.91191405e-04 1.37991947e-05\n",
      " 2.81642097e-05 2.79548694e-04 1.52024761e-04 5.70538614e-05\n",
      " 1.72515618e-04 1.51067143e-04 9.95688606e-05 4.91239325e-06\n",
      " 1.18848612e-03 3.44927385e-02 1.88711623e-04 9.73154128e-01\n",
      " 7.29460955e-01 2.21025455e-03 1.25143584e-03 9.99745309e-01\n",
      " 5.28349250e-04 9.98718977e-01 1.00000000e+00 1.41484590e-04\n",
      " 5.33718885e-05 9.21853411e-04 4.77958191e-03 3.72913480e-03\n",
      " 2.72117415e-03 6.88324144e-05 9.84944381e-06 4.63572451e-06\n",
      " 1.67707531e-05 2.15974957e-04 1.29572463e-05 7.97468892e-05\n",
      " 2.57318979e-03 4.47010643e-05 5.96099344e-05 4.93408414e-04\n",
      " 3.11182870e-04 2.86011727e-05 7.28400264e-05 2.56898929e-03\n",
      " 8.87103081e-01 2.05091565e-05 1.48614182e-03 5.38751920e-06\n",
      " 3.00713233e-04 1.98053567e-06 2.20738761e-02 3.89078014e-05\n",
      " 9.00526252e-03 2.73175829e-05 8.82691529e-05 6.90838962e-04\n",
      " 1.46997601e-01 1.07078988e-03 9.97680187e-01 9.97906804e-01\n",
      " 9.99743164e-01 7.66951730e-03 2.79788102e-04 9.99999881e-01\n",
      " 6.55505776e-01 9.99991417e-01 2.54340708e-01 3.29593793e-02\n",
      " 9.98238683e-01 9.99925733e-01 1.00000000e+00 6.39844909e-02\n",
      " 9.99999881e-01 9.99999046e-01 9.24390674e-01 9.99986649e-01\n",
      " 9.99997973e-01 9.21532214e-01 1.00000000e+00 9.63531256e-01\n",
      " 4.59268643e-03 2.19144091e-01 4.55509797e-02 3.88650559e-02\n",
      " 9.95359719e-01 1.59680583e-02 9.99997497e-01 2.88797484e-04\n",
      " 1.04749284e-03 9.99877334e-01 2.03381409e-04 1.97033305e-02\n",
      " 7.10862150e-05 6.91172361e-01 1.06133237e-04 9.33603942e-01\n",
      " 2.46648025e-02 9.99940395e-01 5.40436668e-06 1.41406041e-02\n",
      " 9.98290598e-01 4.36679512e-01 9.87891674e-01 9.75953341e-01\n",
      " 4.49190987e-03 2.06062361e-03 9.96996164e-01 3.26553285e-01\n",
      " 1.63385030e-02 6.08076811e-01 5.66083800e-05 1.04461331e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 177 [0/106 (0%)]\tTrain Loss: 0.060412\n",
      "Train Epoch: 177 [10/106 (9%)]\tTrain Loss: 0.000054\n",
      "Train Epoch: 177 [20/106 (19%)]\tTrain Loss: 0.000004\n",
      "Train Epoch: 177 [30/106 (28%)]\tTrain Loss: 0.000024\n",
      "Train Epoch: 177 [40/106 (38%)]\tTrain Loss: 0.000655\n",
      "Train Epoch: 177 [50/106 (47%)]\tTrain Loss: 0.000072\n",
      "Train Epoch: 177 [60/106 (57%)]\tTrain Loss: 0.045712\n",
      "Train Epoch: 177 [70/106 (66%)]\tTrain Loss: 0.001113\n",
      "Train Epoch: 177 [80/106 (75%)]\tTrain Loss: 0.000354\n",
      "Train Epoch: 177 [90/106 (85%)]\tTrain Loss: 0.000019\n",
      "Train Epoch: 177 [100/106 (94%)]\tTrain Loss: 0.018580\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 419/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.43184013e-06 1.06351392e-03 5.90519085e-06 3.34673791e-06\n",
      " 9.12276334e-07 2.94628080e-07 6.04017696e-05 2.94503479e-06\n",
      " 4.77005869e-07 1.02773822e-06 5.24238067e-06 2.97142805e-07\n",
      " 6.17532032e-06 1.28592956e-05 6.31907969e-05 1.70840258e-06\n",
      " 4.12044028e-04 4.36022013e-01 2.52842165e-06 1.83192504e-04\n",
      " 5.81323875e-05 6.65676926e-05 5.37016422e-05 9.99421597e-01\n",
      " 2.66227944e-05 7.57256627e-01 9.99996662e-01 1.93731194e-06\n",
      " 1.16095384e-06 9.50680806e-06 9.87952292e-01 9.76042524e-02\n",
      " 8.34866241e-03 1.71394495e-05 1.79440565e-06 1.54670647e-06\n",
      " 4.25574990e-06 2.82075198e-04 9.74802242e-05 8.79923664e-06\n",
      " 1.25426403e-03 3.39284525e-05 3.26133813e-05 4.58282339e-05\n",
      " 2.68963799e-06 3.15281568e-06 2.98226714e-05 7.08779917e-05\n",
      " 6.79254770e-01 3.31319006e-06 5.80005422e-02 3.19793116e-07\n",
      " 8.26512951e-06 2.90937379e-07 1.12737696e-04 4.46998399e-07\n",
      " 2.18673194e-05 1.06218295e-06 1.17828301e-06 2.24878731e-05\n",
      " 1.16884883e-03 4.05947249e-05 9.99360025e-01 9.99590814e-01\n",
      " 1.74418965e-03 3.06479573e-01 4.89824342e-05 9.98576403e-01\n",
      " 8.28561127e-01 9.99516726e-01 7.39054903e-02 6.89971086e-04\n",
      " 9.96286273e-01 9.98916388e-01 9.98530865e-01 9.88366544e-01\n",
      " 9.99433458e-01 9.99633431e-01 9.88105655e-01 9.99609172e-01\n",
      " 9.99746144e-01 1.64705038e-01 9.99984980e-01 9.95912075e-01\n",
      " 2.45595463e-02 9.92841959e-01 7.64796569e-04 3.41931015e-01\n",
      " 8.49474311e-01 2.62298825e-04 9.99992490e-01 8.69079158e-05\n",
      " 2.11618026e-04 9.99247193e-01 3.09014686e-05 5.20213962e-06\n",
      " 2.40663030e-06 7.59450893e-04 1.25000979e-05 8.54264319e-01\n",
      " 9.96892929e-01 9.99959946e-01 6.37297035e-07 1.37530253e-04\n",
      " 9.21429932e-01 4.76776913e-05 9.73932147e-01 9.66992557e-01\n",
      " 5.02217968e-04 6.11860378e-05 7.51675963e-01 1.37137691e-03\n",
      " 2.08206606e-04 2.03875898e-04 3.18927981e-04 9.89946663e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "Train Epoch: 178 [0/106 (0%)]\tTrain Loss: 0.039035\n",
      "Train Epoch: 178 [10/106 (9%)]\tTrain Loss: 0.000361\n",
      "Train Epoch: 178 [20/106 (19%)]\tTrain Loss: 0.000015\n",
      "Train Epoch: 178 [30/106 (28%)]\tTrain Loss: 0.007227\n",
      "Train Epoch: 178 [40/106 (38%)]\tTrain Loss: 0.329663\n",
      "Train Epoch: 178 [50/106 (47%)]\tTrain Loss: 0.000167\n",
      "Train Epoch: 178 [60/106 (57%)]\tTrain Loss: 0.009526\n",
      "Train Epoch: 178 [70/106 (66%)]\tTrain Loss: 0.000070\n",
      "Train Epoch: 178 [80/106 (75%)]\tTrain Loss: 0.000378\n",
      "Train Epoch: 178 [90/106 (85%)]\tTrain Loss: 0.076642\n",
      "Train Epoch: 178 [100/106 (94%)]\tTrain Loss: 0.000276\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 414/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [4.12300187e-06 1.34716257e-01 2.12901563e-04 8.18744127e-07\n",
      " 2.19236057e-07 4.33419274e-07 4.90988186e-03 6.90691593e-07\n",
      " 4.90045693e-08 1.42433706e-07 6.17948172e-05 9.87945992e-09\n",
      " 4.75750654e-04 1.40172315e-05 3.56987803e-05 6.53149641e-08\n",
      " 1.94618548e-03 4.68230480e-03 1.09888765e-03 9.99783576e-01\n",
      " 3.92808735e-01 9.81756866e-01 9.99496102e-01 9.99882340e-01\n",
      " 2.23940499e-02 9.99900818e-01 9.99976516e-01 3.31197066e-06\n",
      " 2.42286886e-04 7.40737072e-04 8.26642849e-03 4.65537280e-01\n",
      " 1.98147073e-03 7.96863751e-05 5.61285333e-07 3.61130105e-06\n",
      " 2.02170340e-05 7.08907328e-06 4.44869838e-05 1.71132178e-05\n",
      " 3.01510423e-01 3.96865129e-04 9.74097202e-06 9.78525713e-05\n",
      " 2.15881679e-04 7.63342769e-07 1.06090338e-04 7.02935038e-04\n",
      " 9.28429782e-01 4.70491068e-06 6.00766065e-03 4.96022512e-09\n",
      " 4.91856724e-07 5.44701395e-09 1.11998823e-02 1.24979112e-08\n",
      " 8.34364182e-05 3.19759266e-07 1.19331716e-07 3.14518729e-05\n",
      " 1.73466362e-03 2.92896037e-03 9.99180138e-01 9.96591926e-01\n",
      " 3.84120882e-01 6.28679711e-03 3.92029266e-04 9.99851823e-01\n",
      " 1.74335077e-01 9.99598920e-01 1.34592056e-02 1.69423565e-01\n",
      " 9.97811258e-01 9.99921322e-01 9.99996424e-01 9.99417663e-01\n",
      " 9.99784172e-01 9.99847770e-01 8.89828563e-01 3.09162229e-01\n",
      " 9.99537230e-01 9.87871766e-01 9.99848366e-01 9.83224034e-01\n",
      " 5.19159734e-01 8.92547220e-02 4.82563308e-04 4.43748385e-03\n",
      " 9.99762833e-01 2.60087452e-03 9.99992847e-01 5.47009986e-05\n",
      " 1.69322084e-04 9.99560654e-01 3.07654240e-03 7.36516376e-05\n",
      " 3.20195340e-07 1.68989822e-02 7.96146691e-04 9.99911189e-01\n",
      " 9.99903560e-01 9.95918691e-01 2.22781659e-06 2.33186307e-04\n",
      " 7.03989863e-01 2.75844213e-04 9.42713618e-01 9.99791563e-01\n",
      " 9.96817827e-01 2.81198556e-03 9.97473061e-01 2.73788255e-02\n",
      " 5.06030302e-03 1.64441802e-02 2.95704114e-04 4.98108044e-02]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 179 [0/106 (0%)]\tTrain Loss: 0.000091\n",
      "Train Epoch: 179 [10/106 (9%)]\tTrain Loss: 0.000141\n",
      "Train Epoch: 179 [20/106 (19%)]\tTrain Loss: 0.001075\n",
      "Train Epoch: 179 [30/106 (28%)]\tTrain Loss: 0.000137\n",
      "Train Epoch: 179 [40/106 (38%)]\tTrain Loss: 0.000052\n",
      "Train Epoch: 179 [50/106 (47%)]\tTrain Loss: 0.000072\n",
      "Train Epoch: 179 [60/106 (57%)]\tTrain Loss: 0.000056\n",
      "Train Epoch: 179 [70/106 (66%)]\tTrain Loss: 0.000024\n",
      "Train Epoch: 179 [80/106 (75%)]\tTrain Loss: 0.007801\n",
      "Train Epoch: 179 [90/106 (85%)]\tTrain Loss: 0.136305\n",
      "Train Epoch: 179 [100/106 (94%)]\tTrain Loss: 0.000109\n",
      "\n",
      "Train set: Average loss: 0.0008, Accuracy: 413/424 (97%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.51901071e-07 4.21103463e-03 1.46565560e-06 2.72448908e-08\n",
      " 5.02827824e-08 4.47420129e-10 8.30078789e-05 1.51060476e-07\n",
      " 9.84274617e-09 1.67280092e-08 5.99010718e-07 8.00685918e-09\n",
      " 5.56580881e-06 1.54473128e-06 9.02554746e-07 5.27278532e-09\n",
      " 7.91859929e-06 2.70673663e-05 4.24733298e-06 9.99998689e-01\n",
      " 9.99624014e-01 1.08734914e-03 4.30800766e-02 9.99999881e-01\n",
      " 2.61811307e-03 9.99999881e-01 1.00000000e+00 3.09036103e-08\n",
      " 7.45077045e-07 7.20483058e-06 3.30011389e-05 6.29359420e-05\n",
      " 1.00790511e-03 4.50776696e-07 4.69078820e-08 2.06956106e-08\n",
      " 4.83795930e-07 1.05676232e-08 1.68945931e-07 8.83112872e-09\n",
      " 5.61726956e-05 3.53676981e-08 3.09539672e-07 4.35494229e-07\n",
      " 5.80075259e-08 4.10742063e-09 1.82967142e-06 3.58144098e-05\n",
      " 9.97351050e-01 5.66721553e-07 3.49125601e-02 2.08438697e-10\n",
      " 1.28593678e-08 1.95431699e-10 3.61707794e-06 1.62691560e-09\n",
      " 8.23600260e-07 7.39550776e-09 6.62001867e-08 4.55430502e-08\n",
      " 4.69521672e-01 6.09672687e-04 9.99988914e-01 9.99976158e-01\n",
      " 6.34977303e-04 1.76087764e-04 1.90788905e-05 1.00000000e+00\n",
      " 9.96796668e-01 9.99991059e-01 4.57472779e-05 8.62346860e-06\n",
      " 9.99972582e-01 1.00000000e+00 9.99847054e-01 9.99997139e-01\n",
      " 9.99999762e-01 9.99999881e-01 9.83980179e-01 5.85435390e-01\n",
      " 9.99993205e-01 1.94663107e-02 1.00000000e+00 9.99999762e-01\n",
      " 9.99049604e-01 8.74028914e-03 5.84181180e-07 7.71193481e-06\n",
      " 9.99990106e-01 2.28583194e-05 1.00000000e+00 1.25314682e-06\n",
      " 3.11841006e-07 9.99999881e-01 8.35079789e-01 4.91417404e-06\n",
      " 1.81421207e-08 4.00296807e-01 6.29164242e-06 9.99996662e-01\n",
      " 9.99030948e-01 9.99996543e-01 1.79163533e-06 9.92985070e-01\n",
      " 9.99990940e-01 3.82562098e-03 7.36081004e-02 5.15507102e-01\n",
      " 1.94818145e-04 3.71665556e-06 1.34281145e-04 2.73956648e-05\n",
      " 5.28147029e-05 9.97523129e-01 1.00458897e-06 1.31281937e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 180 [0/106 (0%)]\tTrain Loss: 0.000015\n",
      "Train Epoch: 180 [10/106 (9%)]\tTrain Loss: 0.047491\n",
      "Train Epoch: 180 [20/106 (19%)]\tTrain Loss: 0.000064\n",
      "Train Epoch: 180 [30/106 (28%)]\tTrain Loss: 0.000059\n",
      "Train Epoch: 180 [40/106 (38%)]\tTrain Loss: 0.000390\n",
      "Train Epoch: 180 [50/106 (47%)]\tTrain Loss: 0.000029\n",
      "Train Epoch: 180 [60/106 (57%)]\tTrain Loss: 0.000080\n",
      "Train Epoch: 180 [70/106 (66%)]\tTrain Loss: 0.000059\n",
      "Train Epoch: 180 [80/106 (75%)]\tTrain Loss: 0.000010\n",
      "Train Epoch: 180 [90/106 (85%)]\tTrain Loss: 0.008481\n",
      "Train Epoch: 180 [100/106 (94%)]\tTrain Loss: 0.001301\n",
      "\n",
      "Train set: Average loss: 0.0001, Accuracy: 414/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.43691095e-05 1.58354043e-04 5.23719382e-06 7.25299151e-06\n",
      " 4.68394956e-06 1.44367857e-06 2.89539566e-05 2.11142942e-05\n",
      " 1.59195461e-05 1.35163182e-05 4.92803774e-05 1.63539949e-06\n",
      " 1.21705292e-04 6.90295419e-05 1.47103317e-04 9.01310977e-06\n",
      " 3.33001481e-05 2.09000020e-04 3.80630845e-05 9.99752820e-01\n",
      " 9.98929441e-01 3.06752068e-03 9.94983196e-01 9.99999881e-01\n",
      " 1.51194623e-02 9.99960423e-01 9.99999166e-01 1.84801265e-05\n",
      " 1.26989526e-05 1.07570391e-04 5.75841463e-04 7.21780816e-04\n",
      " 1.95866404e-03 9.62891791e-05 6.75619049e-06 7.42309248e-06\n",
      " 2.94106576e-05 2.24160954e-06 1.56863462e-05 8.47479350e-06\n",
      " 2.95099197e-03 1.19097140e-05 1.47827750e-05 9.16925273e-05\n",
      " 4.81993957e-05 8.68041661e-06 1.44658086e-04 1.81765342e-03\n",
      " 9.98810649e-01 5.97476492e-06 4.98670131e-01 1.64216976e-06\n",
      " 6.26627116e-06 1.99488750e-06 1.27336549e-04 2.99024600e-06\n",
      " 1.19886536e-04 7.93799245e-06 2.52329719e-05 2.01085113e-05\n",
      " 4.60174750e-04 4.71437641e-04 9.99981761e-01 9.99950409e-01\n",
      " 4.39351657e-03 9.04494524e-03 1.25982522e-04 9.99976873e-01\n",
      " 9.94528115e-01 9.99993205e-01 1.32744072e-03 1.05303549e-03\n",
      " 9.99990582e-01 9.99999404e-01 9.97875214e-01 2.52485663e-01\n",
      " 9.99996543e-01 9.99970555e-01 9.98922884e-01 1.80824444e-01\n",
      " 9.99993086e-01 9.66733634e-01 9.99994993e-01 6.17722750e-01\n",
      " 8.35778177e-01 9.14220698e-04 5.14844178e-05 4.45292913e-04\n",
      " 9.98062432e-01 1.15519892e-04 9.99999285e-01 3.74293246e-04\n",
      " 7.70522893e-05 9.99987245e-01 7.39223957e-02 8.58888263e-04\n",
      " 4.32855586e-06 2.57398505e-02 7.23631456e-05 9.99775827e-01\n",
      " 9.99974370e-01 9.99993801e-01 1.27485007e-04 1.36477053e-01\n",
      " 8.88417423e-01 2.99585104e-01 9.99361694e-01 9.15652156e-01\n",
      " 5.79982297e-04 3.43798201e-05 6.49267691e-04 4.96925146e-04\n",
      " 7.76157598e-04 9.99754131e-01 3.22524393e-05 3.24837893e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 27 TN= 54 FN= 29 FP= 6\n",
      "TP+FP 33\n",
      "precision 0.8181818181818182\n",
      "recall 0.48214285714285715\n",
      "F1 0.6067415730337079\n",
      "acc 0.6982758620689655\n",
      "AUCp 0.6910714285714287\n",
      "AUC 0.8360119047619048\n",
      "\n",
      " The epoch is 180, average recall: 0.4821, average precision: 0.8182,average F1: 0.6067, average accuracy: 0.6983, average AUC: 0.8360\n",
      "Train Epoch: 181 [0/106 (0%)]\tTrain Loss: 0.000081\n",
      "Train Epoch: 181 [10/106 (9%)]\tTrain Loss: 0.000006\n",
      "Train Epoch: 181 [20/106 (19%)]\tTrain Loss: 0.001376\n",
      "Train Epoch: 181 [30/106 (28%)]\tTrain Loss: 0.000044\n",
      "Train Epoch: 181 [40/106 (38%)]\tTrain Loss: 0.000013\n",
      "Train Epoch: 181 [50/106 (47%)]\tTrain Loss: 0.122450\n",
      "Train Epoch: 181 [60/106 (57%)]\tTrain Loss: 0.000010\n",
      "Train Epoch: 181 [70/106 (66%)]\tTrain Loss: 0.000120\n",
      "Train Epoch: 181 [80/106 (75%)]\tTrain Loss: 0.000012\n",
      "Train Epoch: 181 [90/106 (85%)]\tTrain Loss: 0.000026\n",
      "Train Epoch: 181 [100/106 (94%)]\tTrain Loss: 0.000322\n",
      "\n",
      "Train set: Average loss: 0.0006, Accuracy: 420/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.57105600e-05 1.40466465e-04 4.91439437e-07 1.74600291e-06\n",
      " 1.14157365e-06 1.40937402e-06 9.04016633e-06 4.09112408e-06\n",
      " 3.19013793e-06 2.24789483e-06 1.70273124e-05 5.70782504e-07\n",
      " 6.30735231e-05 5.03272495e-05 6.79188015e-05 2.61672517e-06\n",
      " 1.31499939e-04 2.81264610e-03 4.82082169e-06 9.99965429e-01\n",
      " 9.98784363e-01 5.92137912e-05 8.96419049e-04 9.99998212e-01\n",
      " 5.49273915e-04 9.99652147e-01 9.99997497e-01 6.49237222e-07\n",
      " 1.46315153e-06 1.60137697e-05 4.38020155e-02 2.89295171e-03\n",
      " 5.55712581e-01 4.07865809e-05 1.30481499e-06 2.14618217e-06\n",
      " 4.03963350e-06 2.84802610e-07 2.30358205e-06 5.57250096e-05\n",
      " 9.98439014e-01 8.81821688e-05 4.09397308e-06 1.09424627e-05\n",
      " 3.57211570e-06 7.91453658e-07 6.47409470e-05 4.81632818e-03\n",
      " 9.99987245e-01 2.84719704e-06 9.99963284e-01 2.23836579e-07\n",
      " 4.20697717e-07 3.59460472e-07 1.77834125e-04 3.43046707e-07\n",
      " 1.76736881e-04 3.84116049e-07 2.64997993e-06 2.30733303e-06\n",
      " 9.57726240e-01 8.60878441e-04 9.99989152e-01 9.99986529e-01\n",
      " 2.19322610e-04 2.27649766e-03 6.84172555e-05 9.99816716e-01\n",
      " 9.97533441e-01 9.99987245e-01 8.64848733e-01 2.70142598e-04\n",
      " 9.99995947e-01 9.99999404e-01 2.05052900e-03 4.25917327e-01\n",
      " 9.99948144e-01 9.99692798e-01 1.56293616e-01 9.96506095e-01\n",
      " 9.99998569e-01 9.91368353e-01 9.99970436e-01 9.98359978e-01\n",
      " 9.93395507e-01 7.05534637e-01 2.52856148e-06 4.64947107e-06\n",
      " 9.50877070e-01 1.92728170e-04 9.99999285e-01 1.83981465e-04\n",
      " 1.93310098e-05 9.99992490e-01 9.99736845e-01 8.12224578e-04\n",
      " 1.01262935e-06 9.97399330e-01 6.96724310e-05 9.99992847e-01\n",
      " 9.99992967e-01 9.99996066e-01 3.80646001e-04 8.90810549e-01\n",
      " 9.99987721e-01 9.99208629e-01 9.99999166e-01 9.97215867e-01\n",
      " 1.16539169e-02 5.50945842e-05 9.99917269e-01 4.93081333e-03\n",
      " 9.95714128e-01 9.99723613e-01 1.54975423e-04 6.66742344e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0.]\n",
      "Train Epoch: 182 [0/106 (0%)]\tTrain Loss: 0.000046\n",
      "Train Epoch: 182 [10/106 (9%)]\tTrain Loss: 0.000055\n",
      "Train Epoch: 182 [20/106 (19%)]\tTrain Loss: 0.001011\n",
      "Train Epoch: 182 [30/106 (28%)]\tTrain Loss: 0.000158\n",
      "Train Epoch: 182 [40/106 (38%)]\tTrain Loss: 0.000132\n",
      "Train Epoch: 182 [50/106 (47%)]\tTrain Loss: 0.000010\n",
      "Train Epoch: 182 [60/106 (57%)]\tTrain Loss: 0.000075\n",
      "Train Epoch: 182 [70/106 (66%)]\tTrain Loss: 0.000224\n",
      "Train Epoch: 182 [80/106 (75%)]\tTrain Loss: 0.000045\n",
      "Train Epoch: 182 [90/106 (85%)]\tTrain Loss: 0.000008\n",
      "Train Epoch: 182 [100/106 (94%)]\tTrain Loss: 0.005561\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 420/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [8.73522310e-07 4.53089771e-04 2.38787652e-06 3.29826617e-06\n",
      " 1.54079396e-06 1.52766816e-07 2.60110755e-05 8.46329203e-06\n",
      " 1.21767675e-06 1.33359902e-06 2.63434431e-05 1.58463024e-06\n",
      " 3.11508164e-04 3.44180953e-05 1.11323512e-04 3.76937373e-06\n",
      " 4.52696841e-05 9.30486654e-04 7.98988913e-06 9.97169077e-01\n",
      " 7.28153740e-04 8.79319414e-05 3.72447493e-03 9.99993443e-01\n",
      " 8.93467222e-04 9.99714315e-01 9.99997497e-01 3.27332941e-06\n",
      " 4.13774396e-05 2.53495004e-04 5.36677369e-04 9.90316644e-02\n",
      " 1.26880943e-04 4.30103182e-06 2.75319758e-06 1.42582405e-06\n",
      " 2.40077088e-06 2.71943281e-06 4.58135810e-06 3.82248663e-06\n",
      " 6.72975439e-05 1.43245200e-06 2.84477544e-04 8.95164048e-05\n",
      " 3.38359358e-04 2.22838594e-06 1.23037353e-05 5.04222990e-04\n",
      " 3.59587818e-01 2.99901376e-06 1.16780144e-03 5.42736927e-07\n",
      " 2.50174253e-05 1.00878526e-06 1.40117062e-03 8.71235329e-07\n",
      " 7.04942504e-05 1.72531441e-06 3.09322081e-06 1.37241004e-04\n",
      " 9.97588634e-01 2.77051260e-03 9.99940753e-01 9.99831080e-01\n",
      " 9.99818146e-01 7.50407489e-05 2.59339955e-04 9.99990344e-01\n",
      " 9.98603642e-01 9.99952793e-01 2.62030936e-03 3.41206382e-04\n",
      " 9.99817073e-01 9.99995828e-01 2.22280219e-01 9.89188790e-01\n",
      " 9.99979854e-01 9.99470770e-01 9.99985576e-01 9.98379111e-01\n",
      " 9.99992967e-01 6.28337681e-01 9.99996305e-01 9.99875188e-01\n",
      " 9.84774768e-01 1.79261446e-01 2.08249176e-03 5.68993331e-04\n",
      " 9.92325783e-01 1.00426099e-04 9.99998689e-01 3.30928538e-04\n",
      " 2.22518807e-04 9.99890566e-01 3.68241663e-03 6.57494820e-05\n",
      " 7.44218869e-06 9.49002922e-01 2.85259921e-05 9.98659015e-01\n",
      " 9.99913335e-01 9.99982595e-01 4.64972763e-05 9.93475378e-01\n",
      " 9.97686744e-01 9.90992069e-01 9.99816716e-01 6.21242041e-04\n",
      " 5.11880426e-05 1.15990442e-05 7.97375396e-05 3.64588523e-05\n",
      " 7.64478813e-04 9.99307275e-01 1.33073147e-04 1.16379035e-03]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 183 [0/106 (0%)]\tTrain Loss: 0.000012\n",
      "Train Epoch: 183 [10/106 (9%)]\tTrain Loss: 0.004959\n",
      "Train Epoch: 183 [20/106 (19%)]\tTrain Loss: 0.000006\n",
      "Train Epoch: 183 [30/106 (28%)]\tTrain Loss: 0.000107\n",
      "Train Epoch: 183 [40/106 (38%)]\tTrain Loss: 0.000065\n",
      "Train Epoch: 183 [50/106 (47%)]\tTrain Loss: 0.000015\n",
      "Train Epoch: 183 [60/106 (57%)]\tTrain Loss: 0.000013\n",
      "Train Epoch: 183 [70/106 (66%)]\tTrain Loss: 0.000229\n",
      "Train Epoch: 183 [80/106 (75%)]\tTrain Loss: 0.000031\n",
      "Train Epoch: 183 [90/106 (85%)]\tTrain Loss: 0.001127\n",
      "Train Epoch: 183 [100/106 (94%)]\tTrain Loss: 0.001308\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 418/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.05568306e-05 4.03429032e-04 1.79564665e-06 1.35109831e-05\n",
      " 1.75214279e-06 1.49507594e-06 3.30231778e-05 6.64048057e-05\n",
      " 8.37729021e-06 2.99051680e-06 1.98348807e-05 5.55690940e-06\n",
      " 1.44874139e-04 4.55038789e-05 2.75300088e-04 1.21520111e-06\n",
      " 2.89953750e-05 5.43232858e-01 2.69204247e-05 8.72466341e-02\n",
      " 5.21227950e-04 6.94047485e-04 1.10403270e-01 9.99986172e-01\n",
      " 2.10713269e-03 9.99700189e-01 9.99997139e-01 1.02248650e-05\n",
      " 7.96322929e-05 1.06895424e-03 4.86464575e-02 4.26669652e-03\n",
      " 6.62800064e-03 2.91522792e-06 8.39837128e-07 4.23439701e-07\n",
      " 1.73278215e-06 3.86646798e-06 1.19524047e-05 1.82205497e-06\n",
      " 2.51270048e-05 2.48094398e-06 1.18009816e-03 6.59467260e-05\n",
      " 8.98941580e-05 2.01402054e-06 2.09623278e-04 4.39761253e-03\n",
      " 9.97747958e-01 5.47149793e-05 6.41250551e-01 1.44045472e-07\n",
      " 1.20714712e-05 5.82058021e-07 9.04016010e-03 1.84580941e-07\n",
      " 3.64243489e-04 6.73160230e-07 1.58549428e-05 1.70153493e-04\n",
      " 9.99357641e-01 9.59082484e-01 9.99972463e-01 9.99872804e-01\n",
      " 9.98869956e-01 9.03385284e-04 4.57208836e-03 9.99989033e-01\n",
      " 9.64260876e-01 9.99989033e-01 3.88297513e-02 2.04042345e-03\n",
      " 9.98961210e-01 9.99829292e-01 1.05169252e-03 9.99092937e-01\n",
      " 9.99922156e-01 9.99433935e-01 9.99728858e-01 9.97517586e-01\n",
      " 9.99189913e-01 1.69033200e-01 9.99963880e-01 9.99906182e-01\n",
      " 9.99235749e-01 3.21282804e-01 1.45937502e-01 8.12422037e-02\n",
      " 9.80369389e-01 2.40224035e-04 9.99996901e-01 3.61166283e-04\n",
      " 5.32912556e-04 9.99343097e-01 5.59222605e-03 9.72792768e-05\n",
      " 4.50391781e-06 9.93628919e-01 4.39966971e-05 9.99288142e-01\n",
      " 9.98796582e-01 9.99884248e-01 2.43064787e-04 9.59202368e-03\n",
      " 3.71767819e-01 1.11162094e-02 9.99387980e-01 6.21223390e-01\n",
      " 2.82206776e-04 1.87966711e-04 2.18910098e-01 8.29440134e-04\n",
      " 2.76931971e-02 9.98930752e-01 3.92269460e-04 8.49680662e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "Train Epoch: 184 [0/106 (0%)]\tTrain Loss: 0.000012\n",
      "Train Epoch: 184 [10/106 (9%)]\tTrain Loss: 0.000006\n",
      "Train Epoch: 184 [20/106 (19%)]\tTrain Loss: 0.000020\n",
      "Train Epoch: 184 [30/106 (28%)]\tTrain Loss: 0.000071\n",
      "Train Epoch: 184 [40/106 (38%)]\tTrain Loss: 0.000014\n",
      "Train Epoch: 184 [50/106 (47%)]\tTrain Loss: 0.000054\n",
      "Train Epoch: 184 [60/106 (57%)]\tTrain Loss: 0.000073\n",
      "Train Epoch: 184 [70/106 (66%)]\tTrain Loss: 0.000359\n",
      "Train Epoch: 184 [80/106 (75%)]\tTrain Loss: 0.000159\n",
      "Train Epoch: 184 [90/106 (85%)]\tTrain Loss: 0.105977\n",
      "Train Epoch: 184 [100/106 (94%)]\tTrain Loss: 0.000027\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 417/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.79862229e-07 5.54956887e-06 8.51981596e-08 3.94074647e-07\n",
      " 1.32117520e-07 8.47765307e-08 1.25822419e-06 1.15220587e-06\n",
      " 2.25322097e-07 1.07907816e-07 1.52684220e-06 2.57248388e-07\n",
      " 1.70409276e-05 4.73079899e-06 1.53406418e-05 4.02437657e-08\n",
      " 1.27356577e-06 1.04252319e-03 9.97907705e-07 9.96919870e-01\n",
      " 6.35618853e-05 9.83740938e-06 8.36817622e-01 9.99994516e-01\n",
      " 7.58308161e-05 9.99861360e-01 9.99995828e-01 7.11206894e-06\n",
      " 5.37633559e-06 2.61323603e-05 4.75269496e-01 1.44352060e-04\n",
      " 4.60398936e-04 1.80795340e-07 7.12759345e-08 5.62460976e-08\n",
      " 1.78784333e-07 1.53345269e-07 3.86479087e-07 1.67531439e-07\n",
      " 1.99767283e-06 1.39826184e-07 2.30055648e-05 8.93896299e-07\n",
      " 5.19992773e-06 2.06588425e-07 4.17767387e-06 6.57965284e-05\n",
      " 9.89073336e-01 1.89177376e-06 2.87974975e-03 1.04748308e-08\n",
      " 1.05239144e-06 3.05280814e-08 5.01603172e-05 2.74847203e-08\n",
      " 1.64834400e-05 5.49591945e-08 3.83635353e-07 6.46913622e-06\n",
      " 9.99370992e-01 4.09680558e-03 9.99974251e-01 9.99942183e-01\n",
      " 9.99806464e-01 3.06145412e-05 6.06862923e-05 9.99991179e-01\n",
      " 9.66335773e-01 9.99994516e-01 5.18479466e-01 6.44323998e-04\n",
      " 9.99576151e-01 9.99962926e-01 9.94255602e-01 9.99210954e-01\n",
      " 9.99879122e-01 9.87128019e-01 9.98544455e-01 9.98137593e-01\n",
      " 9.99971747e-01 5.08989615e-04 9.99996901e-01 9.99868631e-01\n",
      " 9.94107842e-01 3.78343910e-01 1.01636269e-05 4.33150035e-06\n",
      " 1.14668964e-03 1.41328019e-05 9.99998093e-01 5.23554127e-06\n",
      " 4.26581937e-06 9.99820292e-01 1.88139768e-03 4.14095121e-06\n",
      " 4.79967412e-07 9.71265018e-01 1.97057670e-06 9.99592483e-01\n",
      " 9.98352766e-01 9.99980688e-01 8.69899668e-06 9.96799827e-01\n",
      " 9.93673086e-01 1.06685597e-03 9.99750555e-01 4.64197388e-03\n",
      " 2.03390009e-05 1.15723706e-05 1.93174500e-02 8.10322035e-06\n",
      " 4.27058694e-05 9.98404682e-01 9.78532989e-06 2.98501309e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 185 [0/106 (0%)]\tTrain Loss: 0.000056\n",
      "Train Epoch: 185 [10/106 (9%)]\tTrain Loss: 0.000054\n",
      "Train Epoch: 185 [20/106 (19%)]\tTrain Loss: 0.000066\n",
      "Train Epoch: 185 [30/106 (28%)]\tTrain Loss: 0.000041\n",
      "Train Epoch: 185 [40/106 (38%)]\tTrain Loss: 0.001353\n",
      "Train Epoch: 185 [50/106 (47%)]\tTrain Loss: 0.000045\n",
      "Train Epoch: 185 [60/106 (57%)]\tTrain Loss: 0.000080\n",
      "Train Epoch: 185 [70/106 (66%)]\tTrain Loss: 0.003612\n",
      "Train Epoch: 185 [80/106 (75%)]\tTrain Loss: 0.000189\n",
      "Train Epoch: 185 [90/106 (85%)]\tTrain Loss: 0.000052\n",
      "Train Epoch: 185 [100/106 (94%)]\tTrain Loss: 0.053103\n",
      "\n",
      "Train set: Average loss: 0.0001, Accuracy: 421/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.79365651e-07 5.61080515e-06 8.16621650e-08 3.88363389e-07\n",
      " 8.74157564e-08 4.84059477e-08 1.47477465e-06 1.20331595e-06\n",
      " 1.17973862e-07 5.02198141e-08 8.53719371e-07 1.22801680e-07\n",
      " 2.11391161e-05 3.39200619e-06 8.74190118e-06 3.17886553e-08\n",
      " 1.49596315e-06 1.05639501e-03 1.22115273e-06 9.97236967e-01\n",
      " 6.82449681e-05 2.24863088e-05 9.86962199e-01 9.99985099e-01\n",
      " 2.03476462e-04 9.99687791e-01 9.99986529e-01 4.93143170e-06\n",
      " 7.26301414e-06 2.42851347e-05 3.10765076e-02 2.19726446e-03\n",
      " 1.57731946e-03 1.50417023e-07 5.96642380e-08 2.80466494e-08\n",
      " 1.08027663e-07 1.37566531e-07 4.59154876e-07 1.67494704e-07\n",
      " 2.87673402e-06 1.36327756e-07 5.25672385e-06 5.73704369e-07\n",
      " 5.40072961e-06 1.47177190e-07 5.10724658e-06 4.82917312e-05\n",
      " 9.77862477e-01 9.03624482e-07 2.57712081e-02 6.37124220e-09\n",
      " 6.94077642e-07 1.67594667e-08 1.57727227e-05 1.33563587e-08\n",
      " 1.56920487e-05 3.21995834e-08 4.88742671e-07 3.51696008e-06\n",
      " 9.98995960e-01 2.19359130e-01 9.99986529e-01 9.99963403e-01\n",
      " 9.99572575e-01 3.26860572e-05 2.82225010e-05 9.99989390e-01\n",
      " 8.28502715e-01 9.99969840e-01 1.87170077e-02 5.85900852e-04\n",
      " 9.97523129e-01 9.99954343e-01 9.98557508e-01 9.97610211e-01\n",
      " 9.99923706e-01 9.96185839e-01 9.98959064e-01 9.99621034e-01\n",
      " 9.99982953e-01 6.24022400e-03 9.99996185e-01 9.99759495e-01\n",
      " 9.96904790e-01 9.06635046e-01 5.45726698e-06 3.20088316e-06\n",
      " 2.62910238e-04 1.50266251e-05 9.99983191e-01 8.46813600e-06\n",
      " 3.44214118e-06 9.98513997e-01 1.55642716e-04 2.06634854e-06\n",
      " 4.12881150e-07 5.36222756e-01 1.59916192e-06 9.97845173e-01\n",
      " 9.98087585e-01 9.99985933e-01 1.33789435e-05 7.60005534e-01\n",
      " 2.67073095e-01 4.64756376e-05 9.95095015e-01 1.46811130e-03\n",
      " 1.59103965e-05 6.25180837e-06 5.79342358e-02 9.81477024e-06\n",
      " 2.70708242e-05 9.95398223e-01 1.13494752e-05 2.43022616e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 186 [0/106 (0%)]\tTrain Loss: 0.000040\n",
      "Train Epoch: 186 [10/106 (9%)]\tTrain Loss: 0.000284\n",
      "Train Epoch: 186 [20/106 (19%)]\tTrain Loss: 0.000010\n",
      "Train Epoch: 186 [30/106 (28%)]\tTrain Loss: 0.000121\n",
      "Train Epoch: 186 [40/106 (38%)]\tTrain Loss: 0.000057\n",
      "Train Epoch: 186 [50/106 (47%)]\tTrain Loss: 0.000004\n",
      "Train Epoch: 186 [60/106 (57%)]\tTrain Loss: 0.000054\n",
      "Train Epoch: 186 [70/106 (66%)]\tTrain Loss: 0.043051\n",
      "Train Epoch: 186 [80/106 (75%)]\tTrain Loss: 0.000005\n",
      "Train Epoch: 186 [90/106 (85%)]\tTrain Loss: 0.000064\n",
      "Train Epoch: 186 [100/106 (94%)]\tTrain Loss: 0.000075\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 424/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.21279993e-06 2.62881549e-05 8.86557700e-07 2.64992445e-06\n",
      " 1.15000046e-06 3.15465741e-07 9.94223137e-06 7.40756332e-06\n",
      " 8.71472878e-07 6.39762050e-07 8.79989148e-06 1.07036783e-06\n",
      " 1.98605674e-04 3.09632342e-05 7.20549433e-05 7.04062415e-07\n",
      " 1.02202921e-05 1.07156266e-04 7.98653673e-06 9.99971032e-01\n",
      " 1.09392405e-02 7.65040531e-05 9.93797958e-01 1.00000000e+00\n",
      " 3.34066426e-04 9.99992847e-01 1.00000000e+00 1.42141784e-04\n",
      " 2.54244733e-05 6.01773718e-05 9.44461107e-01 2.07699300e-03\n",
      " 2.50229635e-03 2.31649642e-06 1.03070454e-06 5.99427892e-07\n",
      " 2.25016424e-06 1.86876593e-06 2.92390632e-06 3.18833122e-06\n",
      " 6.63887258e-05 2.15952309e-06 2.92825644e-05 7.21004108e-06\n",
      " 4.54399269e-05 1.19614208e-06 1.65904603e-05 1.08819477e-04\n",
      " 9.98237252e-01 8.16265492e-06 8.66481006e-01 1.20265355e-07\n",
      " 1.12129837e-05 3.16786839e-07 5.64172296e-05 2.82996439e-07\n",
      " 2.44053972e-05 6.24977702e-07 3.19689298e-06 2.16088119e-05\n",
      " 9.99993920e-01 9.51457322e-01 1.00000000e+00 9.99999404e-01\n",
      " 9.99993682e-01 1.95438071e-04 1.15159033e-04 9.99999881e-01\n",
      " 9.20277655e-01 9.99999881e-01 6.00688875e-01 9.76513233e-03\n",
      " 9.99794900e-01 9.99999881e-01 9.99745786e-01 9.98760581e-01\n",
      " 9.99997258e-01 9.99495029e-01 9.96518850e-01 9.99940872e-01\n",
      " 9.99999881e-01 3.78713124e-02 1.00000000e+00 9.99985814e-01\n",
      " 9.98892128e-01 9.66162443e-01 3.94260751e-05 3.69502159e-05\n",
      " 2.50201356e-02 7.64490687e-05 1.00000000e+00 3.69887857e-05\n",
      " 1.54664140e-05 9.99999285e-01 6.92033395e-03 1.40078910e-05\n",
      " 8.19461820e-06 7.25045085e-01 1.27407420e-05 9.99974489e-01\n",
      " 9.99842405e-01 9.99999881e-01 4.04630955e-05 9.94025946e-01\n",
      " 9.65390503e-01 2.10855913e-04 9.99680519e-01 9.64214385e-01\n",
      " 2.71544995e-04 2.49572568e-05 9.93425846e-01 4.13501439e-05\n",
      " 4.53645567e-04 9.99941945e-01 5.52604834e-05 2.27148368e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 187 [0/106 (0%)]\tTrain Loss: 0.000048\n",
      "Train Epoch: 187 [10/106 (9%)]\tTrain Loss: 0.000104\n",
      "Train Epoch: 187 [20/106 (19%)]\tTrain Loss: 0.000025\n",
      "Train Epoch: 187 [30/106 (28%)]\tTrain Loss: 0.000009\n",
      "Train Epoch: 187 [40/106 (38%)]\tTrain Loss: 0.000026\n",
      "Train Epoch: 187 [50/106 (47%)]\tTrain Loss: 0.000115\n",
      "Train Epoch: 187 [60/106 (57%)]\tTrain Loss: 0.000425\n",
      "Train Epoch: 187 [70/106 (66%)]\tTrain Loss: 0.000023\n",
      "Train Epoch: 187 [80/106 (75%)]\tTrain Loss: 0.000005\n",
      "Train Epoch: 187 [90/106 (85%)]\tTrain Loss: 0.000085\n",
      "Train Epoch: 187 [100/106 (94%)]\tTrain Loss: 0.000045\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.18934008e-06 3.36276935e-05 6.43221654e-07 2.10202143e-06\n",
      " 7.69652559e-07 2.93954486e-07 1.15651992e-05 8.04913725e-06\n",
      " 8.85420377e-07 4.29918401e-07 6.10780171e-06 8.10730228e-07\n",
      " 1.21549885e-04 3.35084769e-05 9.07061549e-05 3.77223301e-07\n",
      " 1.16210549e-05 1.25640727e-04 7.22374989e-06 9.99959230e-01\n",
      " 8.77733750e-04 7.29054882e-05 9.90644693e-01 9.99999881e-01\n",
      " 3.46131768e-04 9.99981284e-01 1.00000000e+00 8.17831242e-05\n",
      " 2.90648622e-05 6.32646043e-05 9.36901331e-01 5.31015405e-03\n",
      " 2.77799903e-03 1.70168857e-06 5.78941183e-07 3.32782207e-07\n",
      " 1.26358611e-06 1.61060086e-06 3.66994800e-06 2.69888756e-06\n",
      " 3.63645486e-05 1.33406843e-06 3.62236751e-05 6.14309010e-06\n",
      " 5.53456593e-05 1.16332888e-06 1.86094967e-05 1.09985049e-04\n",
      " 9.96791303e-01 8.37256175e-06 6.87651873e-01 6.79800607e-08\n",
      " 1.20711948e-05 1.75566768e-07 8.76977138e-05 1.30634106e-07\n",
      " 4.30105574e-05 3.39440277e-07 2.89172772e-06 3.05684516e-05\n",
      " 9.99991894e-01 9.72188234e-01 9.99999881e-01 9.99998450e-01\n",
      " 9.99992132e-01 1.09501474e-04 1.05328872e-04 9.99998689e-01\n",
      " 8.57195795e-01 9.99999046e-01 1.50329158e-01 1.13656949e-02\n",
      " 9.99786675e-01 9.99999642e-01 9.99728978e-01 9.98721898e-01\n",
      " 9.99988675e-01 9.98445094e-01 9.98992980e-01 9.99897718e-01\n",
      " 9.99999523e-01 2.56154805e-01 1.00000000e+00 9.99935031e-01\n",
      " 9.94298816e-01 9.65771258e-01 6.11949072e-05 4.71282328e-05\n",
      " 4.40691561e-01 9.34044001e-05 9.99999881e-01 3.53047726e-05\n",
      " 1.73659246e-05 9.99994397e-01 8.37018434e-03 1.54862646e-05\n",
      " 5.89062302e-06 8.82025421e-01 1.12067719e-05 9.99889255e-01\n",
      " 9.99242902e-01 9.99999404e-01 4.03993145e-05 9.74430621e-01\n",
      " 9.51659381e-01 1.73801149e-04 9.99506831e-01 4.85214263e-01\n",
      " 1.41623081e-04 3.42564672e-05 9.92624223e-01 5.33535240e-05\n",
      " 2.66426563e-04 9.99342740e-01 4.86663375e-05 1.58274939e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 188 [0/106 (0%)]\tTrain Loss: 0.000098\n",
      "Train Epoch: 188 [10/106 (9%)]\tTrain Loss: 0.000050\n",
      "Train Epoch: 188 [20/106 (19%)]\tTrain Loss: 0.000047\n",
      "Train Epoch: 188 [30/106 (28%)]\tTrain Loss: 0.000025\n",
      "Train Epoch: 188 [40/106 (38%)]\tTrain Loss: 0.064057\n",
      "Train Epoch: 188 [50/106 (47%)]\tTrain Loss: 0.000086\n",
      "Train Epoch: 188 [60/106 (57%)]\tTrain Loss: 0.000045\n",
      "Train Epoch: 188 [70/106 (66%)]\tTrain Loss: 0.000194\n",
      "Train Epoch: 188 [80/106 (75%)]\tTrain Loss: 0.007905\n",
      "Train Epoch: 188 [90/106 (85%)]\tTrain Loss: 0.000119\n",
      "Train Epoch: 188 [100/106 (94%)]\tTrain Loss: 0.000011\n",
      "\n",
      "Train set: Average loss: 0.0003, Accuracy: 420/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [5.38479355e-07 1.60355612e-05 3.74775624e-07 1.07375502e-06\n",
      " 3.24425429e-07 1.36045927e-07 4.85462942e-06 3.60450235e-06\n",
      " 3.67914652e-07 2.08515132e-07 3.53950077e-06 4.00826679e-07\n",
      " 5.17363587e-05 1.19013339e-05 3.16777841e-05 1.91616138e-07\n",
      " 5.26728763e-06 1.20507641e-04 2.44266926e-06 9.98896718e-01\n",
      " 2.33321975e-04 4.05181891e-05 9.83044624e-01 9.99989986e-01\n",
      " 2.12755447e-04 9.99815881e-01 9.99990702e-01 2.92226996e-05\n",
      " 1.44748747e-05 5.36204388e-05 8.50284278e-01 5.73180895e-03\n",
      " 1.62147498e-03 7.11554378e-07 2.51923893e-07 1.40916953e-07\n",
      " 4.67354340e-07 6.68230598e-07 2.23501343e-06 9.83434006e-07\n",
      " 1.29201426e-05 8.02848433e-07 1.26615751e-05 2.78758853e-06\n",
      " 2.08290221e-05 6.78885783e-07 1.03021812e-05 1.19884477e-04\n",
      " 9.87983704e-01 3.52058760e-06 5.97205609e-02 3.20045075e-08\n",
      " 3.90072637e-06 7.05738401e-08 4.87494362e-05 5.74328496e-08\n",
      " 5.27490120e-05 1.84111371e-07 1.39644976e-06 1.63813311e-05\n",
      " 9.99310374e-01 1.32815465e-01 9.99982595e-01 9.99954104e-01\n",
      " 9.99669433e-01 3.80214005e-05 3.49938382e-05 9.99969721e-01\n",
      " 4.57968503e-01 9.99952197e-01 5.77138215e-02 5.39254118e-03\n",
      " 9.98506486e-01 9.99973655e-01 9.98673677e-01 9.98266935e-01\n",
      " 9.99749601e-01 9.91895258e-01 9.99607384e-01 9.99571383e-01\n",
      " 9.99984264e-01 5.59696734e-01 9.99997616e-01 9.99659300e-01\n",
      " 9.91645992e-01 9.71851587e-01 2.40811514e-05 2.27266228e-05\n",
      " 1.65875105e-03 3.49110451e-05 9.99984980e-01 2.03650088e-05\n",
      " 1.11226373e-05 9.99628544e-01 5.97311067e-04 7.93833260e-06\n",
      " 1.73634692e-06 8.54319453e-01 4.56759017e-06 9.98321235e-01\n",
      " 9.98278618e-01 9.99981642e-01 3.19025494e-05 8.85287285e-01\n",
      " 3.92586768e-01 1.45939106e-04 9.98431385e-01 8.77205748e-04\n",
      " 3.54590375e-05 1.73611879e-05 7.49463141e-01 2.15998698e-05\n",
      " 7.87825702e-05 9.95286703e-01 3.10302203e-05 1.16439027e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 189 [0/106 (0%)]\tTrain Loss: 0.000027\n",
      "Train Epoch: 189 [10/106 (9%)]\tTrain Loss: 0.000031\n",
      "Train Epoch: 189 [20/106 (19%)]\tTrain Loss: 0.000018\n",
      "Train Epoch: 189 [30/106 (28%)]\tTrain Loss: 0.000158\n",
      "Train Epoch: 189 [40/106 (38%)]\tTrain Loss: 0.000163\n",
      "Train Epoch: 189 [50/106 (47%)]\tTrain Loss: 0.000054\n",
      "Train Epoch: 189 [60/106 (57%)]\tTrain Loss: 0.000151\n",
      "Train Epoch: 189 [70/106 (66%)]\tTrain Loss: 0.015645\n",
      "Train Epoch: 189 [80/106 (75%)]\tTrain Loss: 0.000141\n",
      "Train Epoch: 189 [90/106 (85%)]\tTrain Loss: 0.000050\n",
      "Train Epoch: 189 [100/106 (94%)]\tTrain Loss: 0.054932\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [7.54190637e-07 2.19714602e-05 2.91423362e-07 1.43631644e-06\n",
      " 3.87066308e-07 1.70007482e-07 7.95951382e-06 6.57296459e-06\n",
      " 4.19230275e-07 2.64766612e-07 3.86947841e-06 5.85562248e-07\n",
      " 1.44072634e-04 4.39047835e-05 8.57999548e-05 2.43371659e-07\n",
      " 7.30140528e-06 7.21534743e-05 4.40525309e-06 9.99945283e-01\n",
      " 1.09099771e-03 5.52183446e-05 9.89922822e-01 9.99999881e-01\n",
      " 3.02483619e-04 9.99965906e-01 1.00000000e+00 7.80762566e-05\n",
      " 1.77259008e-05 6.13995508e-05 9.67544973e-01 1.10061616e-02\n",
      " 2.44276831e-03 7.93840115e-07 2.94992986e-07 1.68604373e-07\n",
      " 6.66425763e-07 9.44594603e-07 2.14040779e-06 1.74542527e-06\n",
      " 2.75325383e-05 8.34801654e-07 2.35421321e-05 3.48515118e-06\n",
      " 5.84686386e-05 4.61913032e-07 1.62799442e-05 1.30767308e-04\n",
      " 9.97074604e-01 6.40627422e-06 8.23204994e-01 3.70503273e-08\n",
      " 1.14385975e-05 1.08446343e-07 7.21916658e-05 7.24787768e-08\n",
      " 4.28116400e-05 1.78100393e-07 2.35167204e-06 1.99814640e-05\n",
      " 9.99991536e-01 9.93254602e-01 9.99999881e-01 9.99997616e-01\n",
      " 9.99990702e-01 2.43149145e-04 1.45409722e-04 9.99999166e-01\n",
      " 9.55500364e-01 9.99998331e-01 1.83942944e-01 2.46923137e-02\n",
      " 9.99785602e-01 9.99999642e-01 9.99791205e-01 9.98324215e-01\n",
      " 9.99994516e-01 9.99347031e-01 9.79849041e-01 9.99930739e-01\n",
      " 9.99999404e-01 4.61057097e-01 1.00000000e+00 9.99948621e-01\n",
      " 9.97566342e-01 9.83002484e-01 5.25813230e-05 3.88207409e-05\n",
      " 1.75363243e-01 6.43813401e-05 9.99999881e-01 3.26836271e-05\n",
      " 1.46498414e-05 9.99993086e-01 2.16009677e-03 1.22670162e-05\n",
      " 3.64338575e-06 9.58294451e-01 9.47754233e-06 9.99444187e-01\n",
      " 9.99624252e-01 9.99999523e-01 3.02081189e-05 9.83617306e-01\n",
      " 9.33758676e-01 5.66603674e-04 9.99319196e-01 4.14049119e-01\n",
      " 1.26164901e-04 2.82872243e-05 9.86504197e-01 4.31961089e-05\n",
      " 3.19762796e-04 9.98958349e-01 6.21091094e-05 8.18544941e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 190 [0/106 (0%)]\tTrain Loss: 0.000061\n",
      "Train Epoch: 190 [10/106 (9%)]\tTrain Loss: 0.000006\n",
      "Train Epoch: 190 [20/106 (19%)]\tTrain Loss: 0.000007\n",
      "Train Epoch: 190 [30/106 (28%)]\tTrain Loss: 0.000172\n",
      "Train Epoch: 190 [40/106 (38%)]\tTrain Loss: 0.000256\n",
      "Train Epoch: 190 [50/106 (47%)]\tTrain Loss: 0.000037\n",
      "Train Epoch: 190 [60/106 (57%)]\tTrain Loss: 0.000043\n",
      "Train Epoch: 190 [70/106 (66%)]\tTrain Loss: 0.007303\n",
      "Train Epoch: 190 [80/106 (75%)]\tTrain Loss: 0.000004\n",
      "Train Epoch: 190 [90/106 (85%)]\tTrain Loss: 0.000617\n",
      "Train Epoch: 190 [100/106 (94%)]\tTrain Loss: 0.000037\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 424/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [6.28674456e-07 1.48688932e-05 2.60562388e-07 1.19309118e-06\n",
      " 3.50649088e-07 1.67713040e-07 4.74280114e-06 4.54497786e-06\n",
      " 3.58229869e-07 2.26442481e-07 3.23514450e-06 4.61141013e-07\n",
      " 7.32169137e-05 1.35918190e-05 3.69265435e-05 1.97745067e-07\n",
      " 6.07802804e-06 4.62454773e-05 2.16070362e-06 9.99753177e-01\n",
      " 4.11909510e-04 2.44810199e-05 9.45700228e-01 9.99998093e-01\n",
      " 1.57707618e-04 9.99864101e-01 9.99998808e-01 1.71905413e-05\n",
      " 1.03699567e-05 3.55775883e-05 4.76401567e-01 1.45722064e-03\n",
      " 1.19129580e-03 6.54234384e-07 2.65444442e-07 1.49383311e-07\n",
      " 5.66380606e-07 6.92827768e-07 1.77197512e-06 1.10210613e-06\n",
      " 1.17477039e-05 6.35972242e-07 1.24082653e-05 2.69351131e-06\n",
      " 2.38135890e-05 5.06222420e-07 1.01997175e-05 7.65139775e-05\n",
      " 9.97131109e-01 4.32593151e-06 4.81718838e-01 3.47633637e-08\n",
      " 3.87549699e-06 8.46350048e-08 3.91199828e-05 5.42959100e-08\n",
      " 2.34895433e-05 1.53621912e-07 1.66047710e-06 1.12144699e-05\n",
      " 9.99845386e-01 8.26216400e-01 9.99997258e-01 9.99984503e-01\n",
      " 9.99854565e-01 5.32206395e-05 6.60702863e-05 9.99987483e-01\n",
      " 9.15396869e-01 9.99985695e-01 1.40639082e-01 4.01248783e-03\n",
      " 9.99355853e-01 9.99994993e-01 9.98711586e-01 9.96920109e-01\n",
      " 9.99953270e-01 9.98314023e-01 9.95080709e-01 9.99579370e-01\n",
      " 9.99992967e-01 5.14604785e-02 9.99999404e-01 9.99804795e-01\n",
      " 9.92744088e-01 9.64218915e-01 1.91911149e-05 1.59832871e-05\n",
      " 1.12923421e-03 3.70993585e-05 9.99997735e-01 2.06371933e-05\n",
      " 1.05243125e-05 9.99927521e-01 9.39448364e-04 7.89415481e-06\n",
      " 2.36811934e-06 7.33919024e-01 5.16983528e-06 9.99219060e-01\n",
      " 9.99350846e-01 9.99992967e-01 2.51579822e-05 6.52174711e-01\n",
      " 5.57291269e-01 1.08172440e-04 9.99240160e-01 1.08790956e-02\n",
      " 4.92647268e-05 1.59414849e-05 9.21127915e-01 2.53605904e-05\n",
      " 9.29892922e-05 9.97686744e-01 3.37419879e-05 6.75358024e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 31 TN= 54 FN= 25 FP= 6\n",
      "TP+FP 37\n",
      "precision 0.8378378378378378\n",
      "recall 0.5535714285714286\n",
      "F1 0.6666666666666666\n",
      "acc 0.7327586206896551\n",
      "AUCp 0.7267857142857144\n",
      "AUC 0.8535714285714285\n",
      "\n",
      " The epoch is 190, average recall: 0.5536, average precision: 0.8378,average F1: 0.6667, average accuracy: 0.7328, average AUC: 0.8536\n",
      "Train Epoch: 191 [0/106 (0%)]\tTrain Loss: 0.000014\n",
      "Train Epoch: 191 [10/106 (9%)]\tTrain Loss: 0.000012\n",
      "Train Epoch: 191 [20/106 (19%)]\tTrain Loss: 0.000048\n",
      "Train Epoch: 191 [30/106 (28%)]\tTrain Loss: 0.000053\n",
      "Train Epoch: 191 [40/106 (38%)]\tTrain Loss: 0.000067\n",
      "Train Epoch: 191 [50/106 (47%)]\tTrain Loss: 0.000044\n",
      "Train Epoch: 191 [60/106 (57%)]\tTrain Loss: 0.000077\n",
      "Train Epoch: 191 [70/106 (66%)]\tTrain Loss: 0.000003\n",
      "Train Epoch: 191 [80/106 (75%)]\tTrain Loss: 0.000085\n",
      "Train Epoch: 191 [90/106 (85%)]\tTrain Loss: 0.000130\n",
      "Train Epoch: 191 [100/106 (94%)]\tTrain Loss: 0.000063\n",
      "\n",
      "Train set: Average loss: 0.0001, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [5.05838784e-07 1.07654132e-05 2.47524781e-07 7.26910741e-07\n",
      " 2.02520582e-07 9.32212743e-08 3.17532249e-06 2.35207790e-06\n",
      " 2.42558798e-07 1.38184774e-07 1.73931699e-06 3.09780035e-07\n",
      " 3.66679560e-05 6.80036737e-06 1.79112922e-05 1.12595274e-07\n",
      " 3.32247691e-06 1.72202213e-04 1.62550862e-06 9.97468472e-01\n",
      " 1.64709796e-04 3.47983623e-05 9.90441084e-01 9.99990225e-01\n",
      " 2.10611572e-04 9.99643803e-01 9.99977708e-01 1.89959210e-05\n",
      " 1.03564562e-05 3.60443883e-05 3.73037577e-01 1.04127533e-03\n",
      " 1.12967542e-03 3.86312905e-07 1.46017385e-07 6.83391335e-08\n",
      " 2.70042477e-07 3.14252645e-07 1.25970587e-06 4.45018003e-07\n",
      " 7.25400150e-06 4.26027611e-07 6.39413383e-06 1.45572187e-06\n",
      " 7.54070743e-06 3.09300333e-07 7.45385069e-06 9.17914658e-05\n",
      " 9.80827332e-01 2.44213834e-06 2.53685154e-02 1.50850799e-08\n",
      " 1.51661391e-06 3.42133148e-08 3.06022812e-05 3.28131904e-08\n",
      " 2.21954524e-05 7.82753276e-08 1.07132780e-06 8.21246340e-06\n",
      " 9.97379839e-01 7.57809496e-03 9.99960423e-01 9.99930739e-01\n",
      " 9.98780668e-01 2.20541933e-05 2.32469483e-05 9.99976277e-01\n",
      " 7.01506734e-01 9.99953032e-01 6.07289793e-03 4.87192796e-04\n",
      " 9.97093916e-01 9.99954820e-01 9.97817516e-01 9.96742904e-01\n",
      " 9.99792755e-01 9.91963148e-01 9.98992264e-01 9.99097943e-01\n",
      " 9.99964952e-01 4.99689467e-02 9.99995589e-01 9.99624372e-01\n",
      " 9.94804084e-01 9.51175809e-01 9.85821134e-06 1.11303825e-05\n",
      " 4.06244944e-04 3.03356628e-05 9.99978900e-01 1.57743925e-05\n",
      " 8.70182521e-06 9.99389172e-01 3.06546921e-04 6.17192882e-06\n",
      " 9.18178387e-07 6.87002122e-01 2.74880313e-06 9.96218860e-01\n",
      " 9.97864664e-01 9.99961495e-01 2.26332286e-05 9.69395563e-02\n",
      " 7.71070272e-02 1.16488118e-04 9.98582482e-01 5.33667451e-04\n",
      " 2.61351779e-05 1.03658331e-05 1.72055617e-01 1.69911400e-05\n",
      " 5.82594730e-05 9.97042835e-01 2.93713347e-05 9.63288549e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 192 [0/106 (0%)]\tTrain Loss: 0.005585\n",
      "Train Epoch: 192 [10/106 (9%)]\tTrain Loss: 0.000033\n",
      "Train Epoch: 192 [20/106 (19%)]\tTrain Loss: 0.000063\n",
      "Train Epoch: 192 [30/106 (28%)]\tTrain Loss: 0.051123\n",
      "Train Epoch: 192 [40/106 (38%)]\tTrain Loss: 0.000050\n",
      "Train Epoch: 192 [50/106 (47%)]\tTrain Loss: 0.000124\n",
      "Train Epoch: 192 [60/106 (57%)]\tTrain Loss: 0.005956\n",
      "Train Epoch: 192 [70/106 (66%)]\tTrain Loss: 0.000080\n",
      "Train Epoch: 192 [80/106 (75%)]\tTrain Loss: 0.000160\n",
      "Train Epoch: 192 [90/106 (85%)]\tTrain Loss: 0.002956\n",
      "Train Epoch: 192 [100/106 (94%)]\tTrain Loss: 0.006270\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.92272517e-07 4.34964795e-06 1.37461356e-07 5.19017021e-07\n",
      " 1.72502993e-07 7.55283267e-08 1.76993149e-06 1.33845481e-06\n",
      " 1.71051227e-07 1.13968810e-07 1.12616590e-06 2.22323976e-07\n",
      " 1.07371243e-05 2.57989359e-06 1.06076322e-05 9.33000734e-08\n",
      " 1.36013796e-06 1.35374212e-05 8.17146542e-07 9.98163760e-01\n",
      " 4.07021071e-05 6.19466164e-06 9.49441731e-01 9.99998689e-01\n",
      " 2.50910161e-05 9.99671221e-01 9.99999404e-01 6.32046649e-05\n",
      " 4.00046338e-06 1.49859379e-05 9.53309834e-01 3.41826119e-04\n",
      " 8.80646869e-04 2.51371432e-07 1.06377200e-07 7.09228019e-08\n",
      " 2.36930020e-07 1.70377206e-07 6.06146784e-07 4.13636286e-07\n",
      " 2.78481707e-06 2.51278209e-07 2.93193693e-06 1.37542020e-06\n",
      " 3.89828665e-06 2.30066590e-07 3.04953096e-06 2.09195186e-05\n",
      " 9.85499024e-01 2.36610117e-06 3.20175588e-01 1.60437175e-08\n",
      " 1.42723479e-06 4.37367973e-08 1.81802880e-05 3.04853707e-08\n",
      " 1.04770443e-05 6.51949819e-08 7.74976229e-07 3.80730808e-06\n",
      " 9.98800278e-01 1.93793932e-03 9.99995828e-01 9.99972820e-01\n",
      " 9.99535680e-01 1.48148965e-05 1.26570121e-05 9.99994278e-01\n",
      " 2.16854274e-01 9.99991655e-01 7.77914713e-04 5.94345365e-05\n",
      " 9.97270882e-01 9.99996185e-01 9.96776164e-01 9.94809389e-01\n",
      " 9.99873877e-01 9.92935836e-01 9.94592249e-01 9.99827862e-01\n",
      " 9.99994516e-01 1.04475021e-02 9.99999762e-01 9.99740064e-01\n",
      " 9.93315876e-01 6.28093600e-01 5.55023053e-06 9.96826293e-06\n",
      " 1.51534579e-04 9.59824320e-06 9.99996901e-01 7.30635793e-06\n",
      " 2.83499071e-06 9.99853253e-01 4.56277230e-05 2.17312322e-06\n",
      " 6.63833930e-07 1.24454536e-01 1.75398191e-06 9.96503949e-01\n",
      " 9.98339534e-01 9.99994397e-01 7.29690828e-06 1.64652079e-01\n",
      " 1.85588319e-02 3.84088999e-05 9.99157786e-01 2.99335661e-04\n",
      " 9.64774608e-06 6.54199675e-06 4.58320463e-03 8.39542918e-06\n",
      " 4.58531540e-05 9.95753646e-01 6.96082952e-06 3.32668824e-05]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Train Epoch: 193 [0/106 (0%)]\tTrain Loss: 0.000046\n",
      "Train Epoch: 193 [10/106 (9%)]\tTrain Loss: 0.000060\n",
      "Train Epoch: 193 [20/106 (19%)]\tTrain Loss: 0.000040\n",
      "Train Epoch: 193 [30/106 (28%)]\tTrain Loss: 0.000003\n",
      "Train Epoch: 193 [40/106 (38%)]\tTrain Loss: 0.000287\n",
      "Train Epoch: 193 [50/106 (47%)]\tTrain Loss: 0.004946\n",
      "Train Epoch: 193 [60/106 (57%)]\tTrain Loss: 0.000033\n",
      "Train Epoch: 193 [70/106 (66%)]\tTrain Loss: 0.000178\n",
      "Train Epoch: 193 [80/106 (75%)]\tTrain Loss: 0.000064\n",
      "Train Epoch: 193 [90/106 (85%)]\tTrain Loss: 0.000048\n",
      "Train Epoch: 193 [100/106 (94%)]\tTrain Loss: 0.000093\n",
      "\n",
      "Train set: Average loss: 0.0035, Accuracy: 420/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.31980050e-06 4.77267749e-05 4.78728089e-06 7.45975331e-06\n",
      " 3.09432448e-06 6.36117250e-07 2.15501386e-05 1.00107709e-05\n",
      " 2.60601428e-06 2.10591202e-06 1.28612819e-05 3.16991577e-06\n",
      " 1.23743244e-04 1.76666163e-05 4.85390192e-05 1.03391983e-06\n",
      " 2.06741497e-05 6.66791893e-05 1.36307081e-05 9.99998689e-01\n",
      " 8.04496884e-01 1.22150348e-04 9.99847174e-01 1.00000000e+00\n",
      " 1.65383521e-04 1.00000000e+00 1.00000000e+00 3.75843346e-01\n",
      " 4.52872555e-05 5.99676459e-05 9.99677896e-01 1.90361799e-03\n",
      " 7.12018344e-04 7.48250341e-06 2.26995212e-06 1.65896699e-06\n",
      " 4.33896730e-06 4.81028655e-06 9.12953328e-06 6.53624693e-06\n",
      " 4.16409166e-05 5.77634182e-06 2.43494414e-05 1.33308004e-05\n",
      " 6.92507529e-05 5.42051293e-06 3.17069826e-05 1.30686050e-04\n",
      " 9.80785251e-01 1.10261339e-04 3.51020217e-01 2.56710933e-07\n",
      " 3.69128829e-05 6.35549043e-07 4.29820418e-02 1.60067248e-06\n",
      " 1.94851141e-02 2.07682683e-06 8.36311301e-06 7.83020150e-05\n",
      " 9.98204827e-01 6.66163687e-04 9.99999881e-01 9.99998927e-01\n",
      " 9.99998331e-01 5.34872997e-05 4.99046764e-05 1.00000000e+00\n",
      " 5.47847413e-02 9.99999881e-01 6.69261499e-04 2.54681596e-04\n",
      " 9.99813974e-01 1.00000000e+00 1.00000000e+00 9.99982357e-01\n",
      " 9.99919415e-01 9.25887048e-01 9.99999166e-01 9.99949574e-01\n",
      " 1.00000000e+00 8.88495982e-01 1.00000000e+00 9.99972105e-01\n",
      " 9.98187244e-01 4.24953848e-01 2.95840611e-04 4.35255497e-04\n",
      " 5.33430398e-01 9.50486865e-05 1.00000000e+00 3.21785556e-05\n",
      " 3.44642795e-05 9.99999881e-01 4.93292755e-04 2.79872856e-05\n",
      " 2.30268579e-05 5.29654980e-01 1.65394649e-05 9.99916911e-01\n",
      " 9.99473631e-01 9.99999881e-01 5.74950682e-05 9.06287968e-01\n",
      " 5.83052576e-01 2.02215582e-01 9.99991417e-01 4.42151539e-03\n",
      " 5.65899718e-05 4.15942377e-05 1.43649685e-03 5.62188870e-05\n",
      " 2.66424759e-04 9.99631166e-01 2.80008480e-05 5.80559790e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "Train Epoch: 194 [0/106 (0%)]\tTrain Loss: 0.000073\n",
      "Train Epoch: 194 [10/106 (9%)]\tTrain Loss: 0.000070\n",
      "Train Epoch: 194 [20/106 (19%)]\tTrain Loss: 0.000011\n",
      "Train Epoch: 194 [30/106 (28%)]\tTrain Loss: 0.000066\n",
      "Train Epoch: 194 [40/106 (38%)]\tTrain Loss: 0.000044\n",
      "Train Epoch: 194 [50/106 (47%)]\tTrain Loss: 0.096231\n",
      "Train Epoch: 194 [60/106 (57%)]\tTrain Loss: 0.000041\n",
      "Train Epoch: 194 [70/106 (66%)]\tTrain Loss: 0.000004\n",
      "Train Epoch: 194 [80/106 (75%)]\tTrain Loss: 0.008217\n",
      "Train Epoch: 194 [90/106 (85%)]\tTrain Loss: 0.000855\n",
      "Train Epoch: 194 [100/106 (94%)]\tTrain Loss: 0.000005\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.18975273e-07 6.43345220e-06 1.44065126e-07 5.01408579e-07\n",
      " 2.18095565e-07 4.57309817e-08 2.08674146e-06 1.02060335e-06\n",
      " 9.51398960e-08 1.25270077e-07 7.95633298e-07 9.57218163e-08\n",
      " 6.29151646e-06 3.14524482e-06 8.63377136e-06 9.25768404e-08\n",
      " 3.88622266e-06 9.22243180e-06 7.33774812e-07 9.99699235e-01\n",
      " 7.98447509e-05 8.27353942e-05 9.97490406e-01 9.99992251e-01\n",
      " 1.42117933e-04 9.99992251e-01 9.99999404e-01 1.67735081e-04\n",
      " 1.01322030e-05 8.53648817e-06 9.99723256e-01 9.75776136e-01\n",
      " 3.06877191e-03 3.40898850e-07 1.23506140e-07 7.42649533e-08\n",
      " 2.94927730e-07 3.87854101e-07 1.72586067e-06 1.18854291e-06\n",
      " 2.44240709e-05 4.60567009e-07 2.39509745e-06 5.59152511e-07\n",
      " 3.42637122e-06 2.07180122e-07 1.82856718e-06 1.10871279e-05\n",
      " 9.99062359e-01 3.45600506e-06 9.94524837e-01 1.50921302e-08\n",
      " 7.35567028e-07 3.42907320e-08 7.23581412e-04 3.81665437e-08\n",
      " 9.31907296e-01 4.28092868e-08 4.46333019e-07 2.03982995e-06\n",
      " 9.99958277e-01 9.53817010e-01 9.99999166e-01 9.99997258e-01\n",
      " 9.99813497e-01 1.27045978e-05 1.05369065e-04 9.99996901e-01\n",
      " 9.71517086e-01 9.99992728e-01 6.31637266e-03 1.28609349e-03\n",
      " 9.99902725e-01 9.99996066e-01 9.99862909e-01 9.99982953e-01\n",
      " 9.99973536e-01 9.99244213e-01 9.99999523e-01 9.99985218e-01\n",
      " 9.99994159e-01 9.99241352e-01 9.99999404e-01 9.99966502e-01\n",
      " 9.99898791e-01 9.99651909e-01 1.87444821e-04 3.77531367e-04\n",
      " 4.21946079e-01 4.73818545e-05 9.99999404e-01 3.65628875e-06\n",
      " 2.20803963e-06 9.99743521e-01 1.04257648e-04 1.16942795e-06\n",
      " 6.46865828e-07 9.96176004e-01 1.07262611e-06 9.99978065e-01\n",
      " 9.99918222e-01 9.99999046e-01 1.52281882e-05 9.86213386e-01\n",
      " 6.92072213e-01 2.05732044e-03 9.99980092e-01 3.56441387e-03\n",
      " 7.59041131e-06 2.70075702e-06 5.57275534e-01 1.55163198e-05\n",
      " 1.31171337e-05 9.99454916e-01 9.57162559e-01 9.99961972e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 195 [0/106 (0%)]\tTrain Loss: 0.000488\n",
      "Train Epoch: 195 [10/106 (9%)]\tTrain Loss: 1.025044\n",
      "Train Epoch: 195 [20/106 (19%)]\tTrain Loss: 0.000009\n",
      "Train Epoch: 195 [30/106 (28%)]\tTrain Loss: 0.000047\n",
      "Train Epoch: 195 [40/106 (38%)]\tTrain Loss: 0.000030\n",
      "Train Epoch: 195 [50/106 (47%)]\tTrain Loss: 0.000010\n",
      "Train Epoch: 195 [60/106 (57%)]\tTrain Loss: 0.000053\n",
      "Train Epoch: 195 [70/106 (66%)]\tTrain Loss: 0.000031\n",
      "Train Epoch: 195 [80/106 (75%)]\tTrain Loss: 0.001860\n",
      "Train Epoch: 195 [90/106 (85%)]\tTrain Loss: 0.000030\n",
      "Train Epoch: 195 [100/106 (94%)]\tTrain Loss: 0.004061\n",
      "\n",
      "Train set: Average loss: 0.0003, Accuracy: 421/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [9.85689326e-07 1.84331675e-05 5.49979916e-07 3.67595180e-06\n",
      " 1.28848728e-06 2.32415857e-07 5.97853841e-06 2.77836389e-05\n",
      " 5.83925555e-07 6.55722260e-07 4.91894252e-06 5.79830214e-07\n",
      " 1.04379214e-04 3.91193134e-05 1.60885713e-04 9.59107524e-07\n",
      " 5.25813739e-05 7.11924513e-05 3.10034443e-06 9.99839902e-01\n",
      " 7.36553106e-04 9.28915324e-05 8.18281114e-01 9.99991775e-01\n",
      " 2.93485122e-03 9.99962568e-01 9.99998689e-01 2.74957001e-05\n",
      " 2.88040726e-04 4.06044092e-05 9.96996880e-01 9.99262512e-01\n",
      " 1.11510165e-01 2.24492123e-06 9.68386644e-07 4.53220792e-07\n",
      " 8.36336369e-07 3.50173605e-06 1.36127946e-05 3.81116615e-06\n",
      " 7.51202315e-05 1.78246648e-06 1.94352251e-05 8.63531932e-06\n",
      " 1.24583385e-04 2.15172577e-06 2.59709486e-05 2.00921670e-03\n",
      " 9.99772847e-01 2.43168161e-05 9.99858499e-01 1.23161513e-07\n",
      " 5.19731020e-06 4.36557912e-07 5.38269232e-04 3.55472963e-07\n",
      " 2.23381743e-01 3.69375073e-07 3.31715160e-06 1.75135119e-05\n",
      " 9.99994159e-01 9.99850273e-01 9.99998808e-01 9.99997258e-01\n",
      " 9.99967217e-01 2.47148853e-02 9.98770535e-01 9.99998689e-01\n",
      " 9.98745084e-01 9.99997258e-01 9.99751508e-01 9.99756277e-01\n",
      " 9.99975085e-01 9.99997735e-01 9.99942064e-01 9.99973297e-01\n",
      " 9.99983668e-01 9.99838829e-01 9.99956489e-01 9.99992967e-01\n",
      " 9.99992967e-01 9.99593794e-01 9.99999285e-01 9.99985456e-01\n",
      " 9.99966383e-01 9.99935031e-01 1.10514811e-04 4.27495470e-05\n",
      " 9.94017601e-01 2.18289811e-03 9.99998689e-01 8.00807393e-05\n",
      " 1.67004018e-05 9.99861598e-01 8.09506774e-01 1.57398154e-05\n",
      " 9.44498879e-06 9.99618292e-01 3.00151478e-05 9.99997973e-01\n",
      " 9.99970675e-01 9.99997735e-01 5.92334371e-04 9.99944329e-01\n",
      " 9.99910593e-01 9.99897718e-01 9.99990582e-01 1.66992527e-02\n",
      " 2.71831523e-05 2.17828347e-04 1.09497339e-01 2.89672171e-04\n",
      " 4.56254976e-03 9.99861836e-01 1.38621097e-02 9.99915957e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "Train Epoch: 196 [0/106 (0%)]\tTrain Loss: 0.007066\n",
      "Train Epoch: 196 [10/106 (9%)]\tTrain Loss: 0.000018\n",
      "Train Epoch: 196 [20/106 (19%)]\tTrain Loss: 0.000090\n",
      "Train Epoch: 196 [30/106 (28%)]\tTrain Loss: 0.000034\n",
      "Train Epoch: 196 [40/106 (38%)]\tTrain Loss: 0.005843\n",
      "Train Epoch: 196 [50/106 (47%)]\tTrain Loss: 0.001222\n",
      "Train Epoch: 196 [60/106 (57%)]\tTrain Loss: 0.000244\n",
      "Train Epoch: 196 [70/106 (66%)]\tTrain Loss: 0.000103\n",
      "Train Epoch: 196 [80/106 (75%)]\tTrain Loss: 0.000137\n",
      "Train Epoch: 196 [90/106 (85%)]\tTrain Loss: 0.000086\n",
      "Train Epoch: 196 [100/106 (94%)]\tTrain Loss: 0.000518\n",
      "\n",
      "Train set: Average loss: 0.0004, Accuracy: 416/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [8.80116349e-05 6.69336703e-04 1.19555014e-04 2.71011173e-04\n",
      " 1.02535414e-04 4.54459514e-05 2.44138297e-04 4.27112653e-04\n",
      " 1.28254134e-04 1.65511461e-04 3.16735706e-04 9.96259332e-05\n",
      " 7.36600079e-04 5.14034356e-04 1.05731725e-03 2.51624471e-04\n",
      " 1.44403765e-03 3.53433588e-03 4.58802708e-04 9.97929096e-01\n",
      " 2.29280349e-03 7.58861840e-01 9.23307598e-01 9.99783695e-01\n",
      " 5.79852201e-02 9.99904037e-01 9.99970913e-01 5.26836258e-04\n",
      " 8.29537050e-04 9.87009611e-04 5.22030378e-03 6.82076532e-03\n",
      " 3.68850655e-03 6.17130077e-04 2.21065580e-04 7.18822703e-05\n",
      " 1.19347496e-04 5.63718088e-04 4.83535754e-04 1.10502483e-03\n",
      " 4.88946447e-03 4.70094819e-04 4.99211135e-04 4.77357389e-04\n",
      " 1.51504180e-03 1.75462614e-04 3.22609296e-04 3.52371542e-04\n",
      " 6.88346326e-01 4.84675547e-04 4.49087054e-01 7.77954774e-05\n",
      " 8.07953882e-04 9.24969572e-05 7.13735702e-04 1.07723121e-04\n",
      " 4.33467823e-04 6.68790162e-05 4.05366794e-04 1.02272525e-03\n",
      " 6.75725222e-01 1.15407407e-02 9.99520659e-01 9.99489427e-01\n",
      " 9.99863744e-01 1.54815370e-03 3.03222123e-03 9.99993205e-01\n",
      " 4.18447018e-01 9.97536778e-01 1.65466429e-03 1.18539494e-03\n",
      " 2.21034940e-02 9.99438107e-01 9.98990119e-01 9.99275625e-01\n",
      " 9.99973536e-01 9.99968052e-01 9.99993801e-01 2.20660940e-01\n",
      " 9.99852657e-01 7.37999752e-02 9.99996543e-01 9.99728620e-01\n",
      " 9.96469259e-01 1.16025498e-02 3.83355049e-03 1.41311320e-03\n",
      " 2.46488750e-02 1.20933098e-03 9.99987364e-01 7.02154648e-04\n",
      " 8.75868602e-04 9.99650955e-01 1.07624382e-03 6.27028465e-04\n",
      " 6.21713873e-04 3.17145996e-02 7.73373467e-04 9.99947309e-01\n",
      " 9.94488060e-01 9.99817908e-01 1.25834509e-03 7.56777532e-04\n",
      " 1.67477073e-03 1.43094396e-03 2.01402185e-03 1.08488267e-02\n",
      " 9.93232825e-04 7.67960970e-04 2.63887197e-01 3.41389328e-03\n",
      " 5.14224498e-03 9.30481195e-01 2.68821418e-03 9.91822064e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "Train Epoch: 197 [0/106 (0%)]\tTrain Loss: 0.000036\n",
      "Train Epoch: 197 [10/106 (9%)]\tTrain Loss: 0.002268\n",
      "Train Epoch: 197 [20/106 (19%)]\tTrain Loss: 0.000287\n",
      "Train Epoch: 197 [30/106 (28%)]\tTrain Loss: 0.000540\n",
      "Train Epoch: 197 [40/106 (38%)]\tTrain Loss: 0.005716\n",
      "Train Epoch: 197 [50/106 (47%)]\tTrain Loss: 0.000016\n",
      "Train Epoch: 197 [60/106 (57%)]\tTrain Loss: 0.000097\n",
      "Train Epoch: 197 [70/106 (66%)]\tTrain Loss: 0.000053\n",
      "Train Epoch: 197 [80/106 (75%)]\tTrain Loss: 0.006352\n",
      "Train Epoch: 197 [90/106 (85%)]\tTrain Loss: 0.000406\n",
      "Train Epoch: 197 [100/106 (94%)]\tTrain Loss: 0.000201\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 420/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [6.60182559e-06 4.01406456e-03 9.94720540e-05 2.53533362e-05\n",
      " 6.53024063e-06 4.48884930e-06 1.70709833e-03 4.84088450e-05\n",
      " 2.74125487e-05 1.49761381e-05 2.32371513e-05 6.03448507e-06\n",
      " 2.04793949e-04 4.33956709e-04 1.99084356e-03 2.23880306e-05\n",
      " 2.47837277e-04 9.82660218e-04 1.88798527e-04 9.99952078e-01\n",
      " 1.16341282e-02 8.69310170e-04 9.17309802e-03 9.99925971e-01\n",
      " 1.71807548e-03 9.73400772e-01 9.99985814e-01 9.17547077e-05\n",
      " 3.19712766e-04 2.53718870e-04 5.04653319e-04 5.93699841e-03\n",
      " 1.09615350e-04 8.27104741e-05 2.04394528e-05 5.81635686e-06\n",
      " 1.47910678e-05 1.52182283e-05 5.03670490e-05 1.40430842e-04\n",
      " 5.24158869e-03 4.59375369e-05 2.03800490e-04 4.69559673e-05\n",
      " 9.25377826e-04 1.31252054e-05 5.07702935e-05 1.97848130e-04\n",
      " 3.37733030e-01 3.06620554e-04 3.53809223e-02 3.15616103e-06\n",
      " 1.85127923e-04 5.21335414e-06 1.44626026e-03 2.63255879e-05\n",
      " 1.30781147e-04 9.08044058e-06 6.50894654e-05 2.13949548e-04\n",
      " 9.98240829e-01 9.57700849e-01 9.99738634e-01 9.99400735e-01\n",
      " 9.93894517e-01 3.01353808e-04 5.39738103e-04 9.98407066e-01\n",
      " 5.99907100e-01 9.78466213e-01 2.66491692e-03 1.34175969e-03\n",
      " 9.98129785e-01 9.99596655e-01 9.71587956e-01 9.52995121e-01\n",
      " 9.99882102e-01 9.99045551e-01 2.19334871e-01 4.71125357e-03\n",
      " 9.93851304e-01 9.89249069e-03 9.99951839e-01 9.95455027e-01\n",
      " 9.84308660e-01 2.67713368e-01 2.94895819e-03 7.49161176e-04\n",
      " 9.96696472e-01 7.89485581e-04 9.99955535e-01 5.52160818e-05\n",
      " 7.59197428e-05 9.99358714e-01 8.49942619e-04 3.46555753e-04\n",
      " 1.46256716e-04 5.76381087e-01 2.05266260e-04 9.99949813e-01\n",
      " 9.97725546e-01 9.99504447e-01 1.39232681e-04 2.71207624e-04\n",
      " 9.54335369e-03 2.12842808e-03 2.85176793e-03 1.21536292e-03\n",
      " 8.05435993e-04 3.90783214e-04 1.29380287e-03 3.70779424e-04\n",
      " 1.28131418e-03 4.57725003e-02 1.05547358e-03 1.10008484e-02]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 198 [0/106 (0%)]\tTrain Loss: 0.000215\n",
      "Train Epoch: 198 [10/106 (9%)]\tTrain Loss: 0.000162\n",
      "Train Epoch: 198 [20/106 (19%)]\tTrain Loss: 0.000025\n",
      "Train Epoch: 198 [30/106 (28%)]\tTrain Loss: 0.000377\n",
      "Train Epoch: 198 [40/106 (38%)]\tTrain Loss: 0.000013\n",
      "Train Epoch: 198 [50/106 (47%)]\tTrain Loss: 0.000043\n",
      "Train Epoch: 198 [60/106 (57%)]\tTrain Loss: 0.000019\n",
      "Train Epoch: 198 [70/106 (66%)]\tTrain Loss: 0.002071\n",
      "Train Epoch: 198 [80/106 (75%)]\tTrain Loss: 0.012840\n",
      "Train Epoch: 198 [90/106 (85%)]\tTrain Loss: 0.000008\n",
      "Train Epoch: 198 [100/106 (94%)]\tTrain Loss: 0.000116\n",
      "\n",
      "Train set: Average loss: 0.0002, Accuracy: 418/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.01297468e-07 2.93567768e-06 5.73916772e-08 7.15999491e-08\n",
      " 3.50099647e-08 9.07413700e-09 1.13232920e-06 9.74958780e-08\n",
      " 4.66374814e-08 1.67326686e-08 6.20535303e-08 1.06211324e-08\n",
      " 2.21977785e-07 3.28014949e-07 3.57345471e-06 1.00222842e-07\n",
      " 1.27164458e-06 4.78958736e-06 9.22094117e-08 9.98230517e-01\n",
      " 6.67335257e-07 1.10052849e-06 4.97246906e-03 9.99549448e-01\n",
      " 1.60406501e-06 9.98961329e-01 9.99948859e-01 7.29926342e-07\n",
      " 8.20091373e-07 5.18707282e-07 7.46075571e-01 4.59740229e-04\n",
      " 5.44759757e-07 5.71830810e-07 1.11157703e-07 2.56087578e-08\n",
      " 4.61295819e-08 2.78426171e-08 4.13129193e-08 3.88551229e-07\n",
      " 1.86017951e-06 9.65559650e-08 2.89370377e-07 1.30231953e-07\n",
      " 9.78998514e-07 2.88459656e-08 2.74946132e-07 1.00247769e-06\n",
      " 3.22466157e-02 1.48770994e-07 5.62716007e-01 8.49433679e-09\n",
      " 2.01803573e-07 2.37207125e-08 2.89314357e-07 3.83100804e-08\n",
      " 1.34205789e-06 1.62205183e-08 1.18267188e-07 2.79475444e-07\n",
      " 9.91483867e-01 3.72965515e-05 9.99878049e-01 9.99893665e-01\n",
      " 9.94492769e-01 6.30588772e-07 2.08215170e-06 9.99978185e-01\n",
      " 9.92275417e-01 9.99792159e-01 5.37058622e-06 2.26203633e-06\n",
      " 9.99958992e-01 9.99797404e-01 9.99657273e-01 9.99946952e-01\n",
      " 9.99955058e-01 9.99992609e-01 6.92743897e-01 9.98450279e-01\n",
      " 9.99959469e-01 9.99630570e-01 9.99988079e-01 7.62848914e-01\n",
      " 1.22560828e-04 5.84155358e-02 1.34521520e-06 8.19958416e-07\n",
      " 1.00186022e-04 2.67922883e-06 9.99974728e-01 3.27428637e-07\n",
      " 1.29572527e-07 7.22318146e-05 1.35702965e-06 4.47645931e-07\n",
      " 8.58879972e-08 1.39338135e-05 1.89884688e-07 9.99559700e-01\n",
      " 9.34663951e-01 9.99953508e-01 1.35809358e-07 1.68316296e-06\n",
      " 2.05028991e-05 2.76847641e-06 4.74956823e-06 1.03846946e-06\n",
      " 5.10273992e-07 3.00042160e-07 1.04956362e-05 1.17011507e-06\n",
      " 7.17889134e-06 2.49582314e-04 4.39158981e-07 1.51889137e-04]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 199 [0/106 (0%)]\tTrain Loss: 0.004602\n",
      "Train Epoch: 199 [10/106 (9%)]\tTrain Loss: 0.000066\n",
      "Train Epoch: 199 [20/106 (19%)]\tTrain Loss: 0.000030\n",
      "Train Epoch: 199 [30/106 (28%)]\tTrain Loss: 0.000060\n",
      "Train Epoch: 199 [40/106 (38%)]\tTrain Loss: 0.000031\n",
      "Train Epoch: 199 [50/106 (47%)]\tTrain Loss: 0.000110\n",
      "Train Epoch: 199 [60/106 (57%)]\tTrain Loss: 0.000095\n",
      "Train Epoch: 199 [70/106 (66%)]\tTrain Loss: 0.000085\n",
      "Train Epoch: 199 [80/106 (75%)]\tTrain Loss: 0.093780\n",
      "Train Epoch: 199 [90/106 (85%)]\tTrain Loss: 0.000758\n",
      "Train Epoch: 199 [100/106 (94%)]\tTrain Loss: 0.000098\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 413/424 (97%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.82970507e-05 3.11206910e-04 1.65386446e-05 3.41131563e-05\n",
      " 1.26844661e-05 5.32121976e-06 9.48705565e-05 1.30779663e-04\n",
      " 2.62637495e-05 2.89978634e-05 4.14062561e-05 3.12420489e-05\n",
      " 1.64093741e-04 3.77153832e-04 1.43778150e-03 5.68338291e-05\n",
      " 1.44518723e-04 3.09852301e-03 7.91918646e-05 9.96751666e-01\n",
      " 2.03567062e-04 4.37007082e-04 9.99688148e-01 9.99989510e-01\n",
      " 2.36317108e-04 9.99997616e-01 9.99999642e-01 4.73796739e-04\n",
      " 3.67416476e-04 3.46262503e-04 7.72532880e-01 9.66613889e-01\n",
      " 5.92749178e-01 5.83794972e-05 2.13584099e-05 2.66947918e-05\n",
      " 2.94559613e-05 1.28984131e-04 1.20745855e-04 1.96199660e-04\n",
      " 7.46826292e-04 9.34821073e-05 9.89746213e-01 1.63195116e-04\n",
      " 9.95780587e-01 6.79889126e-05 1.52322216e-04 1.76672749e-02\n",
      " 9.99936223e-01 3.01224267e-04 9.99999404e-01 5.72051795e-06\n",
      " 1.92302672e-04 1.18223761e-05 5.46371739e-04 3.92001966e-05\n",
      " 9.99289393e-01 1.64570538e-05 1.14026436e-04 2.17521607e-04\n",
      " 9.99994516e-01 9.95109141e-01 9.99999285e-01 9.99998450e-01\n",
      " 9.99989986e-01 2.38113135e-01 9.99991775e-01 9.99999642e-01\n",
      " 9.99654531e-01 9.99999881e-01 8.95550668e-01 4.18751352e-02\n",
      " 9.99997854e-01 9.99999881e-01 9.99619603e-01 9.99999881e-01\n",
      " 9.99998569e-01 1.00000000e+00 9.99999642e-01 9.99991059e-01\n",
      " 9.99999166e-01 9.98117328e-01 9.99999881e-01 9.99974728e-01\n",
      " 3.93576443e-01 9.99933600e-01 2.48929083e-01 5.72409364e-04\n",
      " 9.89930212e-01 1.77219321e-04 9.99999762e-01 4.14106267e-04\n",
      " 3.48964386e-04 9.99999881e-01 9.13269699e-01 1.21607387e-04\n",
      " 1.13606860e-04 9.98506486e-01 3.88341432e-05 9.98826563e-01\n",
      " 9.99838471e-01 9.99999642e-01 1.35323993e-04 9.94346321e-01\n",
      " 9.99886513e-01 9.99825656e-01 9.99959946e-01 1.98004025e-04\n",
      " 7.58306051e-05 3.92737915e-04 7.42454024e-04 9.50204611e-01\n",
      " 9.59862530e-01 2.08542615e-01 2.99075793e-04 9.99985099e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "Train Epoch: 200 [0/106 (0%)]\tTrain Loss: 0.000010\n",
      "Train Epoch: 200 [10/106 (9%)]\tTrain Loss: 0.000100\n",
      "Train Epoch: 200 [20/106 (19%)]\tTrain Loss: 0.000005\n",
      "Train Epoch: 200 [30/106 (28%)]\tTrain Loss: 0.000057\n",
      "Train Epoch: 200 [40/106 (38%)]\tTrain Loss: 0.009930\n",
      "Train Epoch: 200 [50/106 (47%)]\tTrain Loss: 0.096661\n",
      "Train Epoch: 200 [60/106 (57%)]\tTrain Loss: 0.151666\n",
      "Train Epoch: 200 [70/106 (66%)]\tTrain Loss: 0.001513\n",
      "Train Epoch: 200 [80/106 (75%)]\tTrain Loss: 0.001016\n",
      "Train Epoch: 200 [90/106 (85%)]\tTrain Loss: 0.000563\n",
      "Train Epoch: 200 [100/106 (94%)]\tTrain Loss: 0.069320\n",
      "\n",
      "Train set: Average loss: 0.0003, Accuracy: 413/424 (97%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [8.64148114e-06 8.59178777e-04 1.32717901e-06 1.84940909e-05\n",
      " 1.45995656e-07 9.97998200e-07 4.68086422e-04 6.49757931e-05\n",
      " 6.56370015e-04 3.16123942e-06 1.60474629e-05 5.81095151e-07\n",
      " 2.53984908e-04 2.10823282e-03 1.54357329e-01 3.28651513e-05\n",
      " 1.02594867e-03 7.13321492e-02 6.19080020e-05 3.87920924e-02\n",
      " 7.15064059e-04 3.98987159e-02 9.75002766e-01 9.99874234e-01\n",
      " 2.44586244e-02 9.99977231e-01 9.99998808e-01 9.99296308e-01\n",
      " 9.64991516e-04 1.08907074e-02 9.99979854e-01 9.99633670e-01\n",
      " 1.38599053e-03 1.22353912e-03 8.40984001e-07 4.15632798e-07\n",
      " 3.55262864e-06 2.25042459e-04 2.39091154e-04 6.92693293e-02\n",
      " 9.90582705e-01 6.40049344e-04 3.30929235e-02 2.00443392e-04\n",
      " 1.12138118e-03 1.22385982e-05 9.75261210e-04 1.05858743e-02\n",
      " 9.02986944e-01 4.89894347e-03 9.96714234e-01 5.72982692e-08\n",
      " 2.35410771e-05 4.60120106e-07 2.03611702e-02 2.08662595e-06\n",
      " 9.99750674e-01 1.67756059e-06 6.17586076e-04 7.05996717e-05\n",
      " 9.99431789e-01 3.07388663e-01 9.99872446e-01 9.99935031e-01\n",
      " 9.99968886e-01 9.99026537e-01 1.95702329e-01 9.99990940e-01\n",
      " 1.13882102e-01 9.99975324e-01 1.52507983e-02 1.35185327e-02\n",
      " 9.99701083e-01 9.99993801e-01 9.99997258e-01 9.99913096e-01\n",
      " 9.99809206e-01 9.99815047e-01 9.99995232e-01 9.99994278e-01\n",
      " 9.99929070e-01 2.00889722e-01 9.99998569e-01 9.99621034e-01\n",
      " 8.91816914e-01 9.43167686e-01 1.59310788e-01 1.87439367e-01\n",
      " 2.33863831e-01 9.73130405e-01 9.99916315e-01 1.31829304e-03\n",
      " 6.50321366e-04 9.99997020e-01 3.56017873e-02 2.09940728e-04\n",
      " 1.55098271e-04 3.21941935e-02 2.89516647e-05 9.99994516e-01\n",
      " 9.89767969e-01 9.99982238e-01 7.26503029e-04 9.82048810e-01\n",
      " 2.45750584e-02 2.79711490e-03 9.96768713e-01 9.98869240e-01\n",
      " 2.36946996e-02 4.06308472e-03 1.75863340e-01 3.22259106e-02\n",
      " 6.02499545e-01 9.99777973e-01 2.28979997e-02 9.99986768e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 29 TN= 53 FN= 27 FP= 7\n",
      "TP+FP 36\n",
      "precision 0.8055555555555556\n",
      "recall 0.5178571428571429\n",
      "F1 0.6304347826086957\n",
      "acc 0.7068965517241379\n",
      "AUCp 0.700595238095238\n",
      "AUC 0.8354166666666667\n",
      "\n",
      " The epoch is 200, average recall: 0.5179, average precision: 0.8056,average F1: 0.6304, average accuracy: 0.7069, average AUC: 0.8354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 201 [0/106 (0%)]\tTrain Loss: 0.058089\n",
      "Train Epoch: 201 [10/106 (9%)]\tTrain Loss: 0.000047\n",
      "Train Epoch: 201 [20/106 (19%)]\tTrain Loss: 0.000453\n",
      "Train Epoch: 201 [30/106 (28%)]\tTrain Loss: 0.000104\n",
      "Train Epoch: 201 [40/106 (38%)]\tTrain Loss: 0.014051\n",
      "Train Epoch: 201 [50/106 (47%)]\tTrain Loss: 0.000067\n",
      "Train Epoch: 201 [60/106 (57%)]\tTrain Loss: 0.000016\n",
      "Train Epoch: 201 [70/106 (66%)]\tTrain Loss: 0.000068\n",
      "Train Epoch: 201 [80/106 (75%)]\tTrain Loss: 0.000101\n",
      "Train Epoch: 201 [90/106 (85%)]\tTrain Loss: 0.000033\n",
      "Train Epoch: 201 [100/106 (94%)]\tTrain Loss: 0.000652\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 419/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [3.26201625e-05 4.13720449e-03 1.16903436e-06 4.55000873e-06\n",
      " 4.43389553e-07 1.86153875e-06 6.94634393e-04 3.37001575e-06\n",
      " 1.55936257e-04 3.46377201e-05 3.54255326e-06 5.74311571e-06\n",
      " 1.92631360e-05 5.00738993e-03 9.21902806e-02 2.11483111e-05\n",
      " 3.38463404e-04 9.71804321e-01 5.73448313e-04 9.99995351e-01\n",
      " 3.96975083e-03 9.99909639e-01 9.98602688e-01 9.99997497e-01\n",
      " 1.90180112e-02 1.00000000e+00 1.00000000e+00 4.38226610e-02\n",
      " 3.24916164e-03 2.15746718e-03 9.99994040e-01 9.99977589e-01\n",
      " 2.76965293e-04 9.55199008e-04 4.93703681e-07 2.14175429e-07\n",
      " 5.27754707e-07 5.64726142e-05 5.18008819e-05 2.67819464e-01\n",
      " 9.89730597e-01 2.35027343e-04 7.99019448e-03 8.10698475e-05\n",
      " 1.46835241e-02 3.99350683e-05 7.37021444e-04 5.00092236e-03\n",
      " 9.99678135e-01 2.13352405e-03 9.99960065e-01 2.65228532e-08\n",
      " 9.13875265e-05 5.20152753e-07 9.76300836e-01 4.90494203e-05\n",
      " 9.99994040e-01 2.29600664e-06 2.51320918e-04 3.15433601e-04\n",
      " 9.99949813e-01 2.26635396e-01 9.99986291e-01 9.99993324e-01\n",
      " 9.99995947e-01 1.40700350e-03 2.53235083e-02 9.99992251e-01\n",
      " 6.89188600e-01 9.99942064e-01 9.55415606e-01 9.88840282e-01\n",
      " 9.99974370e-01 9.99999881e-01 1.00000000e+00 9.99945402e-01\n",
      " 9.99975681e-01 9.99984980e-01 9.99995589e-01 9.99999166e-01\n",
      " 9.99999285e-01 9.99995351e-01 1.00000000e+00 9.99874234e-01\n",
      " 8.72412205e-01 9.98942435e-01 7.02954590e-01 9.91230965e-01\n",
      " 9.97695029e-01 9.98131216e-01 9.99999523e-01 4.12020832e-04\n",
      " 4.61108924e-04 9.99999881e-01 6.52005076e-02 1.24860590e-03\n",
      " 2.73233070e-03 1.49386339e-02 2.31610215e-03 9.99999881e-01\n",
      " 9.99913335e-01 9.99996543e-01 1.21071906e-04 9.99872684e-01\n",
      " 9.99928832e-01 3.58509749e-01 9.99997258e-01 5.67497872e-02\n",
      " 3.47134434e-02 2.72684032e-03 9.99846458e-01 2.35889815e-02\n",
      " 9.69677269e-01 9.98991907e-01 9.88842249e-01 9.99997258e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1.]\n",
      "Train Epoch: 202 [0/106 (0%)]\tTrain Loss: 0.000060\n",
      "Train Epoch: 202 [10/106 (9%)]\tTrain Loss: 0.000343\n",
      "Train Epoch: 202 [20/106 (19%)]\tTrain Loss: 0.001581\n",
      "Train Epoch: 202 [30/106 (28%)]\tTrain Loss: 0.000015\n",
      "Train Epoch: 202 [40/106 (38%)]\tTrain Loss: 0.000048\n",
      "Train Epoch: 202 [50/106 (47%)]\tTrain Loss: 0.029831\n",
      "Train Epoch: 202 [60/106 (57%)]\tTrain Loss: 0.000014\n",
      "Train Epoch: 202 [70/106 (66%)]\tTrain Loss: 0.000032\n",
      "Train Epoch: 202 [80/106 (75%)]\tTrain Loss: 0.000015\n",
      "Train Epoch: 202 [90/106 (85%)]\tTrain Loss: 0.097444\n",
      "Train Epoch: 202 [100/106 (94%)]\tTrain Loss: 0.011544\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 418/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [6.95554036e-06 1.93909495e-04 6.91888829e-07 7.78431684e-07\n",
      " 2.82327392e-07 8.01025124e-07 5.72007375e-05 9.94981860e-07\n",
      " 2.51472120e-05 3.82863909e-05 5.58024385e-06 7.81171366e-06\n",
      " 1.26771747e-05 7.83426585e-05 8.89817078e-04 2.06559776e-06\n",
      " 9.62348622e-06 4.31669317e-02 2.11453662e-05 9.99952555e-01\n",
      " 3.94733716e-03 9.84147787e-01 9.96706903e-01 9.99994874e-01\n",
      " 1.50537142e-03 9.99996662e-01 9.99999881e-01 1.00995792e-04\n",
      " 4.51375163e-05 4.81814321e-04 2.32659459e-01 9.56626296e-01\n",
      " 8.12204307e-05 1.63873938e-05 3.12927796e-07 2.18056471e-07\n",
      " 3.95599670e-07 6.16997522e-06 1.30856351e-05 1.05196808e-03\n",
      " 1.10268490e-02 4.49978370e-06 1.76724268e-03 2.89375039e-05\n",
      " 5.35521889e-04 8.56178758e-06 1.72532047e-04 2.02961825e-03\n",
      " 9.96942580e-01 1.17940269e-02 9.99862194e-01 3.53698582e-08\n",
      " 9.68338099e-06 1.33434540e-07 6.49827644e-02 5.33556977e-06\n",
      " 9.99751031e-01 1.34622132e-06 4.90319799e-05 2.26540687e-05\n",
      " 9.99857783e-01 9.27627325e-01 9.99985814e-01 9.99987602e-01\n",
      " 9.98982489e-01 5.38625864e-05 4.19307128e-03 9.99988317e-01\n",
      " 8.96966364e-03 9.80491877e-01 5.82703855e-03 2.23354157e-02\n",
      " 9.99969840e-01 9.99999762e-01 9.99997973e-01 7.53596902e-01\n",
      " 9.99917746e-01 9.99984026e-01 1.61940325e-02 9.99992371e-01\n",
      " 9.99975801e-01 4.82246727e-01 9.99999881e-01 9.95276570e-01\n",
      " 1.84688549e-02 9.92751479e-01 1.36418520e-02 4.13764715e-02\n",
      " 9.89072323e-01 9.68094885e-01 9.99999166e-01 7.42854172e-05\n",
      " 3.46538473e-05 1.00000000e+00 1.10236304e-02 1.25331292e-03\n",
      " 1.70691242e-03 2.24226460e-04 5.94522826e-05 9.99999046e-01\n",
      " 8.86560977e-01 9.99999642e-01 4.36769642e-06 9.97639537e-01\n",
      " 9.97100174e-01 2.28967611e-02 9.99706447e-01 2.00853962e-03\n",
      " 2.76569190e-04 2.35331609e-04 9.97911394e-01 2.22664722e-03\n",
      " 8.34225357e-01 9.97662067e-01 2.48324126e-03 9.99805152e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1.]\n",
      "Train Epoch: 203 [0/106 (0%)]\tTrain Loss: 0.000035\n",
      "Train Epoch: 203 [10/106 (9%)]\tTrain Loss: 0.000003\n",
      "Train Epoch: 203 [20/106 (19%)]\tTrain Loss: 0.004852\n",
      "Train Epoch: 203 [30/106 (28%)]\tTrain Loss: 0.000046\n",
      "Train Epoch: 203 [40/106 (38%)]\tTrain Loss: 0.000504\n",
      "Train Epoch: 203 [50/106 (47%)]\tTrain Loss: 0.000063\n",
      "Train Epoch: 203 [60/106 (57%)]\tTrain Loss: 0.014030\n",
      "Train Epoch: 203 [70/106 (66%)]\tTrain Loss: 0.000786\n",
      "Train Epoch: 203 [80/106 (75%)]\tTrain Loss: 0.000035\n",
      "Train Epoch: 203 [90/106 (85%)]\tTrain Loss: 0.000407\n",
      "Train Epoch: 203 [100/106 (94%)]\tTrain Loss: 0.000007\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 421/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.62375215e-06 1.49728992e-04 3.70422782e-07 2.67385815e-07\n",
      " 7.35644221e-08 5.46845001e-07 2.01998264e-05 2.80959426e-07\n",
      " 3.62607170e-06 2.19604044e-06 3.38882643e-07 1.05750723e-06\n",
      " 8.49291325e-07 4.80533090e-05 1.70847471e-03 5.09756489e-07\n",
      " 1.22234121e-06 9.95560765e-01 3.66153677e-06 8.48399758e-01\n",
      " 6.62076636e-05 8.72617811e-02 6.49093790e-03 9.99997616e-01\n",
      " 2.11511317e-04 9.99998212e-01 1.00000000e+00 1.15142684e-05\n",
      " 8.58945623e-06 1.20289966e-04 2.19154614e-03 9.55297768e-01\n",
      " 7.68691825e-05 1.39511462e-06 9.03224944e-08 4.06986516e-08\n",
      " 7.27502396e-08 3.63101299e-06 4.86801446e-06 1.95507426e-04\n",
      " 2.49573868e-03 1.16613592e-06 1.19693635e-03 9.90221361e-06\n",
      " 6.32401179e-06 1.11476118e-06 5.89796873e-05 8.88384180e-04\n",
      " 9.93591726e-01 2.55689029e-05 9.99714315e-01 1.54691673e-08\n",
      " 9.27587735e-07 6.25520329e-08 7.36258982e-04 3.23920261e-07\n",
      " 9.99945879e-01 1.77056350e-07 4.63713923e-06 1.93809365e-06\n",
      " 9.99951005e-01 1.21414982e-01 9.99999642e-01 9.99999762e-01\n",
      " 3.29819977e-01 6.07870097e-05 7.76055560e-04 1.00000000e+00\n",
      " 3.81196546e-03 9.90016997e-01 9.35651478e-04 4.37854964e-04\n",
      " 9.99996424e-01 1.00000000e+00 9.99918699e-01 3.29484418e-02\n",
      " 9.99982715e-01 9.99998808e-01 3.82352620e-01 1.00000000e+00\n",
      " 9.99997735e-01 8.49976670e-03 1.00000000e+00 9.99500155e-01\n",
      " 2.26496700e-02 9.99213457e-01 3.00831464e-03 2.42167502e-03\n",
      " 7.58593064e-03 3.93007576e-01 9.99999762e-01 3.29551331e-05\n",
      " 2.00531103e-05 9.99999881e-01 2.99425272e-04 1.87278347e-05\n",
      " 4.14114083e-06 1.87755184e-04 5.01572958e-06 9.99987841e-01\n",
      " 1.07927192e-02 1.00000000e+00 1.63744403e-06 9.83476281e-01\n",
      " 9.48788971e-03 1.19303448e-04 9.97688770e-01 1.61417091e-04\n",
      " 7.95800661e-05 3.26861191e-05 9.80594218e-01 5.04444586e-04\n",
      " 5.52026276e-03 3.67844244e-03 2.48946599e-04 9.99999285e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 204 [0/106 (0%)]\tTrain Loss: 0.000031\n",
      "Train Epoch: 204 [10/106 (9%)]\tTrain Loss: 0.000005\n",
      "Train Epoch: 204 [20/106 (19%)]\tTrain Loss: 0.060673\n",
      "Train Epoch: 204 [30/106 (28%)]\tTrain Loss: 0.000029\n",
      "Train Epoch: 204 [40/106 (38%)]\tTrain Loss: 0.000010\n",
      "Train Epoch: 204 [50/106 (47%)]\tTrain Loss: 0.000065\n",
      "Train Epoch: 204 [60/106 (57%)]\tTrain Loss: 0.002083\n",
      "Train Epoch: 204 [70/106 (66%)]\tTrain Loss: 0.000031\n",
      "Train Epoch: 204 [80/106 (75%)]\tTrain Loss: 0.000057\n",
      "Train Epoch: 204 [90/106 (85%)]\tTrain Loss: 0.000011\n",
      "Train Epoch: 204 [100/106 (94%)]\tTrain Loss: 0.000555\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 419/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [5.46641859e-05 1.28521852e-03 1.17882510e-05 1.76574704e-05\n",
      " 4.57204396e-06 5.98334100e-06 2.36952008e-04 1.63335935e-05\n",
      " 3.05408539e-05 2.56588901e-05 1.37942598e-05 8.76784998e-06\n",
      " 5.89177471e-05 2.36676526e-04 1.57151173e-03 7.97268694e-06\n",
      " 4.61355776e-05 9.99081135e-01 4.07168591e-05 5.55583136e-03\n",
      " 2.28636578e-04 3.81016098e-02 6.61836122e-04 9.99946237e-01\n",
      " 6.68646884e-04 9.91002619e-01 1.00000000e+00 5.02235962e-05\n",
      " 7.55338333e-05 1.20011240e-03 1.78213359e-03 1.24343708e-01\n",
      " 1.64597994e-03 2.08260226e-05 3.05109893e-06 2.31590002e-06\n",
      " 4.77764388e-06 1.61639779e-04 8.18533183e-04 5.94558194e-04\n",
      " 4.71689133e-03 8.89710191e-06 1.36700077e-02 5.88272997e-05\n",
      " 5.54899270e-05 2.73006299e-05 1.46214606e-03 1.46060931e-02\n",
      " 9.90414202e-01 3.77640790e-05 9.98926699e-01 2.61021995e-07\n",
      " 1.18592334e-05 7.10053598e-07 9.26999783e-04 6.11601990e-06\n",
      " 1.22251585e-01 3.25764609e-06 5.88714647e-05 4.11166111e-05\n",
      " 9.99701679e-01 7.13692605e-01 9.99997377e-01 9.99999166e-01\n",
      " 9.26669464e-02 2.53721781e-04 2.95079267e-03 9.99999881e-01\n",
      " 2.87604555e-02 9.62231815e-01 4.65384638e-03 1.42368150e-03\n",
      " 9.98334348e-01 9.99998450e-01 5.29566072e-02 2.41195392e-02\n",
      " 9.99930620e-01 9.99997020e-01 9.73454583e-03 9.99986291e-01\n",
      " 9.99022245e-01 1.40578095e-02 9.99999881e-01 9.99963641e-01\n",
      " 6.79179370e-01 9.94857669e-01 5.87380817e-03 3.18081537e-03\n",
      " 1.19281206e-02 4.53804666e-03 9.99995708e-01 4.64203709e-04\n",
      " 4.61903808e-04 9.99869943e-01 1.28324074e-03 1.19102377e-04\n",
      " 1.24750768e-05 9.45904851e-03 7.51273983e-05 9.98927057e-01\n",
      " 9.26883519e-01 1.00000000e+00 6.02148612e-05 9.20127854e-02\n",
      " 3.30373645e-02 1.45404623e-03 6.25333905e-01 3.86230007e-04\n",
      " 1.09229190e-03 9.40185681e-04 9.71371830e-01 4.23102975e-02\n",
      " 1.52240451e-02 1.54270437e-02 3.94160190e-04 9.99479473e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "Train Epoch: 205 [0/106 (0%)]\tTrain Loss: 0.000066\n",
      "Train Epoch: 205 [10/106 (9%)]\tTrain Loss: 0.000037\n",
      "Train Epoch: 205 [20/106 (19%)]\tTrain Loss: 0.003975\n",
      "Train Epoch: 205 [30/106 (28%)]\tTrain Loss: 0.000002\n",
      "Train Epoch: 205 [40/106 (38%)]\tTrain Loss: 0.000003\n",
      "Train Epoch: 205 [50/106 (47%)]\tTrain Loss: 0.194039\n",
      "Train Epoch: 205 [60/106 (57%)]\tTrain Loss: 0.000046\n",
      "Train Epoch: 205 [70/106 (66%)]\tTrain Loss: 0.000012\n",
      "Train Epoch: 205 [80/106 (75%)]\tTrain Loss: 0.000232\n",
      "Train Epoch: 205 [90/106 (85%)]\tTrain Loss: 0.000012\n",
      "Train Epoch: 205 [100/106 (94%)]\tTrain Loss: 0.000023\n",
      "\n",
      "Train set: Average loss: 0.0001, Accuracy: 422/424 (100%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [7.19181526e-06 4.95300279e-04 1.63316918e-06 6.58739566e-07\n",
      " 1.80525532e-07 8.25363941e-07 1.01547877e-04 8.50894935e-07\n",
      " 1.19564747e-05 5.05850176e-06 4.41807998e-07 2.25688245e-06\n",
      " 1.32724358e-06 2.21696009e-05 5.08812082e-04 4.56967143e-07\n",
      " 3.71131046e-06 9.95024145e-01 9.80982895e-06 1.54599309e-01\n",
      " 3.05801077e-04 1.27732316e-02 6.18589763e-03 9.99915957e-01\n",
      " 1.35097333e-04 9.99774516e-01 9.99999404e-01 5.44092472e-05\n",
      " 2.19630401e-05 4.69315040e-04 3.27477716e-02 6.23637795e-01\n",
      " 6.38398400e-04 2.19353706e-06 1.80425033e-07 1.04063318e-07\n",
      " 1.76937831e-07 5.32218883e-06 7.26358267e-05 9.02662723e-05\n",
      " 1.94575940e-03 7.03044634e-07 4.45449771e-03 2.67579685e-06\n",
      " 4.68949656e-06 3.47959849e-06 2.71701691e-04 5.27746743e-03\n",
      " 9.75170612e-01 1.57086088e-05 9.98569965e-01 1.41017180e-08\n",
      " 2.22142421e-06 4.34307985e-08 5.26541546e-02 6.22578000e-07\n",
      " 9.97318089e-01 1.64241285e-07 2.02491792e-05 8.20165496e-06\n",
      " 9.99474108e-01 2.07823053e-01 9.99943852e-01 9.99971747e-01\n",
      " 9.96226907e-01 2.21092178e-05 4.99952585e-04 9.99983430e-01\n",
      " 2.84578502e-02 9.86564636e-01 5.09990053e-03 1.14234933e-03\n",
      " 9.99704182e-01 9.99994874e-01 9.99425173e-01 4.44340169e-01\n",
      " 9.99501586e-01 9.99897838e-01 8.27602506e-01 9.99938965e-01\n",
      " 9.99873161e-01 5.25158405e-01 9.99999404e-01 9.99692082e-01\n",
      " 9.74379897e-01 9.97501671e-01 6.88604489e-02 2.12933514e-02\n",
      " 8.98551583e-01 2.69030896e-03 9.99988317e-01 7.21302858e-05\n",
      " 5.60822955e-05 9.99949694e-01 5.55038801e-04 3.40873703e-05\n",
      " 2.09412769e-06 2.20292658e-02 9.85953739e-06 9.97101367e-01\n",
      " 9.88287508e-01 9.99999404e-01 4.77141930e-06 9.08042770e-03\n",
      " 8.01496487e-03 4.05921601e-03 9.97058868e-01 5.24501629e-05\n",
      " 4.36030678e-04 1.07262997e-04 9.87264633e-01 2.75979210e-02\n",
      " 1.18711675e-02 1.15056215e-02 2.34103674e-04 9.99734342e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "Train Epoch: 206 [0/106 (0%)]\tTrain Loss: 0.000017\n",
      "Train Epoch: 206 [10/106 (9%)]\tTrain Loss: 0.000046\n",
      "Train Epoch: 206 [20/106 (19%)]\tTrain Loss: 0.000024\n",
      "Train Epoch: 206 [30/106 (28%)]\tTrain Loss: 0.008147\n",
      "Train Epoch: 206 [40/106 (38%)]\tTrain Loss: 0.000014\n",
      "Train Epoch: 206 [50/106 (47%)]\tTrain Loss: 0.000017\n",
      "Train Epoch: 206 [60/106 (57%)]\tTrain Loss: 0.000161\n",
      "Train Epoch: 206 [70/106 (66%)]\tTrain Loss: 0.000146\n",
      "Train Epoch: 206 [80/106 (75%)]\tTrain Loss: 0.000088\n",
      "Train Epoch: 206 [90/106 (85%)]\tTrain Loss: 0.000891\n",
      "Train Epoch: 206 [100/106 (94%)]\tTrain Loss: 0.000074\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 419/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.44171315e-05 3.02052766e-04 3.93719574e-06 4.13802582e-06\n",
      " 1.64704136e-06 2.47576554e-06 8.96576166e-05 4.23704068e-06\n",
      " 2.39278434e-05 1.62990200e-05 4.05337551e-06 6.75017509e-06\n",
      " 1.43705347e-05 8.41380897e-05 6.25809305e-04 2.94957727e-06\n",
      " 2.02768333e-05 1.90944076e-02 6.04301131e-05 9.16629791e-01\n",
      " 1.28258206e-03 2.16160296e-03 1.17835797e-01 9.99982715e-01\n",
      " 1.69581894e-04 9.99934912e-01 1.00000000e+00 2.01512681e-04\n",
      " 7.69402541e-05 6.33904594e-04 1.11992713e-02 4.98290956e-02\n",
      " 4.15178220e-04 1.80611351e-05 1.15800810e-06 1.13589829e-06\n",
      " 1.84395833e-06 9.04155422e-06 6.04825182e-05 6.13946904e-05\n",
      " 7.11551344e-04 2.68045051e-06 3.31369549e-04 1.62883280e-05\n",
      " 4.49975851e-05 1.60764594e-05 2.79036089e-04 2.28705117e-03\n",
      " 8.68012667e-01 8.48862837e-05 9.98220623e-01 8.98398582e-08\n",
      " 1.28404236e-05 2.68716633e-07 1.10187707e-02 6.61705371e-06\n",
      " 9.96778071e-01 1.80056304e-06 5.11226208e-05 2.83600984e-05\n",
      " 9.99615312e-01 3.10253978e-01 9.99973774e-01 9.99986410e-01\n",
      " 9.92177665e-01 4.53877365e-05 4.84282151e-04 9.99994516e-01\n",
      " 7.96643365e-03 9.71016407e-01 3.94365843e-03 1.11283362e-03\n",
      " 9.99947786e-01 9.99999881e-01 9.99736249e-01 1.69412903e-02\n",
      " 9.99119580e-01 9.99789178e-01 2.23769993e-02 9.99737203e-01\n",
      " 9.99952793e-01 5.53219914e-02 1.00000000e+00 9.99707639e-01\n",
      " 3.51034105e-01 9.30024922e-01 2.09997855e-02 7.62484968e-03\n",
      " 9.17091429e-01 1.99351693e-03 9.99998927e-01 5.18499110e-05\n",
      " 7.13191039e-05 9.99997973e-01 1.90566131e-03 1.43497891e-04\n",
      " 3.04671084e-05 4.68434952e-03 5.75027479e-05 9.98719096e-01\n",
      " 5.01165271e-01 9.99999881e-01 1.38816768e-05 8.06646142e-03\n",
      " 9.20811761e-03 4.80367336e-03 9.99763906e-01 4.83601179e-04\n",
      " 4.24676487e-04 4.38448071e-04 9.91610587e-01 5.72817167e-03\n",
      " 1.48336068e-02 6.58578128e-02 3.57914891e-04 9.95858610e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 207 [0/106 (0%)]\tTrain Loss: 0.004227\n",
      "Train Epoch: 207 [10/106 (9%)]\tTrain Loss: 0.000293\n",
      "Train Epoch: 207 [20/106 (19%)]\tTrain Loss: 0.000060\n",
      "Train Epoch: 207 [30/106 (28%)]\tTrain Loss: 0.000175\n",
      "Train Epoch: 207 [40/106 (38%)]\tTrain Loss: 0.042777\n",
      "Train Epoch: 207 [50/106 (47%)]\tTrain Loss: 0.000020\n",
      "Train Epoch: 207 [60/106 (57%)]\tTrain Loss: 0.000026\n",
      "Train Epoch: 207 [70/106 (66%)]\tTrain Loss: 0.000055\n",
      "Train Epoch: 207 [80/106 (75%)]\tTrain Loss: 0.000291\n",
      "Train Epoch: 207 [90/106 (85%)]\tTrain Loss: 0.000013\n",
      "Train Epoch: 207 [100/106 (94%)]\tTrain Loss: 0.000019\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 421/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.27013755e-05 1.55078305e-04 1.18279763e-06 9.89116984e-07\n",
      " 3.42134200e-07 1.99342117e-06 3.80866113e-05 1.47104674e-06\n",
      " 1.72235068e-05 1.19938750e-05 8.42411225e-07 5.86062288e-06\n",
      " 1.49873620e-06 2.09799346e-05 1.46229242e-04 6.95181313e-07\n",
      " 3.68526753e-06 1.01707615e-01 1.72730997e-05 9.85926270e-01\n",
      " 8.50829994e-04 2.13731127e-03 4.70127493e-01 9.99958515e-01\n",
      " 8.26621399e-05 9.99866605e-01 9.99999523e-01 2.87128292e-04\n",
      " 5.37729284e-05 4.75448760e-04 1.60493881e-01 1.71874389e-01\n",
      " 5.02838753e-04 4.11038036e-06 3.93159468e-07 2.00523786e-07\n",
      " 2.55198273e-07 2.16556805e-06 1.34986185e-05 1.80121842e-05\n",
      " 2.31597849e-04 7.13001327e-07 9.07572976e-05 3.22103233e-06\n",
      " 1.44198875e-05 1.21500998e-05 2.01717819e-04 2.58760224e-03\n",
      " 9.86613333e-01 7.10351160e-05 9.98901725e-01 3.80694125e-08\n",
      " 6.29993337e-06 1.56086202e-07 2.78142281e-02 3.00945408e-06\n",
      " 9.98305321e-01 6.18535978e-07 3.18546372e-05 1.39520962e-05\n",
      " 9.99303102e-01 4.19182301e-01 9.99919415e-01 9.99949336e-01\n",
      " 9.98983681e-01 1.45485556e-05 2.43184171e-04 9.99980688e-01\n",
      " 1.48963118e-02 9.93885934e-01 2.01656613e-02 3.63229704e-03\n",
      " 9.99915123e-01 9.99999166e-01 9.99943614e-01 1.63437560e-01\n",
      " 9.98919606e-01 9.99683380e-01 7.97908843e-01 9.99746501e-01\n",
      " 9.99938726e-01 6.57126307e-01 9.99999762e-01 9.99492884e-01\n",
      " 7.22174227e-01 9.79916871e-01 9.60963294e-02 5.19417487e-02\n",
      " 9.71910357e-01 1.07055018e-03 9.99991298e-01 4.82425676e-05\n",
      " 3.49483162e-05 9.99994159e-01 1.97202619e-03 8.43501766e-05\n",
      " 1.37288516e-05 5.69805922e-03 1.85509689e-05 9.96856570e-01\n",
      " 5.65700643e-02 9.99998212e-01 3.64744278e-06 6.05427613e-03\n",
      " 3.50698121e-02 9.48417559e-03 9.99713719e-01 7.91461862e-05\n",
      " 2.42553157e-04 2.03861928e-04 9.89982188e-01 1.54778697e-02\n",
      " 4.03255261e-02 3.17552872e-02 1.35612048e-04 9.96764898e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "Train Epoch: 208 [0/106 (0%)]\tTrain Loss: 0.000515\n",
      "Train Epoch: 208 [10/106 (9%)]\tTrain Loss: 0.011568\n",
      "Train Epoch: 208 [20/106 (19%)]\tTrain Loss: 0.092017\n",
      "Train Epoch: 208 [30/106 (28%)]\tTrain Loss: 0.000006\n",
      "Train Epoch: 208 [40/106 (38%)]\tTrain Loss: 0.000262\n",
      "Train Epoch: 208 [50/106 (47%)]\tTrain Loss: 0.003346\n",
      "Train Epoch: 208 [60/106 (57%)]\tTrain Loss: 0.000042\n",
      "Train Epoch: 208 [70/106 (66%)]\tTrain Loss: 0.000005\n",
      "Train Epoch: 208 [80/106 (75%)]\tTrain Loss: 0.000047\n",
      "Train Epoch: 208 [90/106 (85%)]\tTrain Loss: 0.000011\n",
      "Train Epoch: 208 [100/106 (94%)]\tTrain Loss: 0.000027\n",
      "\n",
      "Train set: Average loss: 0.0021, Accuracy: 420/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.38691048e-05 1.98035734e-04 2.14469469e-06 1.63536208e-06\n",
      " 5.69500912e-07 2.10604253e-06 6.60653750e-05 1.89330785e-06\n",
      " 1.63504264e-05 1.37109255e-05 1.76602327e-06 6.82811606e-06\n",
      " 3.52589655e-06 4.17876945e-05 3.09038791e-04 1.30635135e-06\n",
      " 6.90515799e-06 3.29081453e-02 3.33179269e-05 9.87177610e-01\n",
      " 2.00458360e-03 1.44652545e-03 7.48647213e-01 9.99975801e-01\n",
      " 7.38225135e-05 9.99954700e-01 9.99999881e-01 3.74419353e-04\n",
      " 5.24025163e-05 6.58539240e-04 1.48550734e-01 2.23905861e-01\n",
      " 4.04459919e-04 5.90256695e-06 6.99051952e-07 4.16529986e-07\n",
      " 6.14734745e-07 4.00648833e-06 1.77661459e-05 2.70212495e-05\n",
      " 2.32370003e-04 1.26204827e-06 1.26058701e-04 6.67419408e-06\n",
      " 2.55040741e-05 1.67629187e-05 2.19047579e-04 2.32858420e-03\n",
      " 9.63547945e-01 8.17895998e-05 9.98563111e-01 6.08542408e-08\n",
      " 1.15098437e-05 2.25281710e-07 5.32035865e-02 4.65396533e-06\n",
      " 9.97871757e-01 1.33776052e-06 3.69682966e-05 2.82766505e-05\n",
      " 9.98887718e-01 5.96584193e-02 9.99931335e-01 9.99961019e-01\n",
      " 9.99405146e-01 2.18717832e-05 3.16441729e-04 9.99983907e-01\n",
      " 1.28820268e-02 9.87974286e-01 1.29213603e-02 2.01414083e-03\n",
      " 9.99956012e-01 9.99999762e-01 9.99971986e-01 1.28631860e-01\n",
      " 9.98374820e-01 9.99655008e-01 9.61672544e-01 9.99841452e-01\n",
      " 9.99977946e-01 8.69001985e-01 9.99999881e-01 9.99332726e-01\n",
      " 4.95466590e-01 9.86835063e-01 1.02232739e-01 4.20645140e-02\n",
      " 9.73653018e-01 1.28528092e-03 9.99998331e-01 5.84038826e-05\n",
      " 5.48714270e-05 9.99996066e-01 1.61760848e-03 1.66050813e-04\n",
      " 2.50311969e-05 4.50417865e-03 2.83763293e-05 9.93850827e-01\n",
      " 5.77727892e-02 9.99999523e-01 7.15073975e-06 6.70320541e-03\n",
      " 9.05894022e-03 2.71414481e-02 9.99819815e-01 5.67912466e-05\n",
      " 1.73689667e-04 2.11685037e-04 9.66055870e-01 6.35311939e-03\n",
      " 2.32356209e-02 3.72446440e-02 1.72796121e-04 9.96805310e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "Train Epoch: 209 [0/106 (0%)]\tTrain Loss: 0.000036\n",
      "Train Epoch: 209 [10/106 (9%)]\tTrain Loss: 0.000148\n",
      "Train Epoch: 209 [20/106 (19%)]\tTrain Loss: 0.000084\n",
      "Train Epoch: 209 [30/106 (28%)]\tTrain Loss: 0.005556\n",
      "Train Epoch: 209 [40/106 (38%)]\tTrain Loss: 0.000036\n",
      "Train Epoch: 209 [50/106 (47%)]\tTrain Loss: 0.004119\n",
      "Train Epoch: 209 [60/106 (57%)]\tTrain Loss: 0.000075\n",
      "Train Epoch: 209 [70/106 (66%)]\tTrain Loss: 0.000028\n",
      "Train Epoch: 209 [80/106 (75%)]\tTrain Loss: 0.000044\n",
      "Train Epoch: 209 [90/106 (85%)]\tTrain Loss: 0.000062\n",
      "Train Epoch: 209 [100/106 (94%)]\tTrain Loss: 0.000049\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 420/424 (99%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.82062483e-05 4.39618656e-04 3.90235573e-06 5.07474124e-06\n",
      " 1.87194098e-06 3.30133435e-06 1.52811466e-04 6.52855260e-06\n",
      " 3.82604449e-05 3.40150509e-05 5.74959859e-06 1.04075461e-05\n",
      " 1.52708708e-05 1.24061160e-04 9.34949261e-04 3.32564400e-06\n",
      " 2.42941223e-05 5.37719019e-02 1.29098815e-04 9.97113585e-01\n",
      " 3.11698276e-03 5.64924255e-03 8.37699592e-01 9.99996185e-01\n",
      " 2.66537361e-04 9.99989986e-01 1.00000000e+00 9.49058856e-04\n",
      " 1.76347326e-04 1.15915027e-03 2.90661216e-01 2.28763521e-01\n",
      " 8.21533031e-04 1.74974848e-05 1.38741393e-06 1.19419076e-06\n",
      " 1.77338677e-06 1.01472697e-05 5.84393165e-05 7.39537209e-05\n",
      " 1.09803420e-03 3.33633034e-06 4.85137280e-04 1.83246739e-05\n",
      " 1.24290877e-04 3.22378037e-05 4.59252711e-04 4.29818640e-03\n",
      " 9.88039196e-01 1.50147316e-04 9.99553978e-01 9.93862059e-08\n",
      " 2.28530880e-05 4.37622504e-07 3.38263325e-02 1.17808559e-05\n",
      " 9.99057353e-01 3.38903124e-06 7.90885606e-05 6.39924110e-05\n",
      " 9.99892712e-01 8.87454808e-01 9.99996424e-01 9.99997258e-01\n",
      " 9.99902487e-01 6.56989141e-05 9.99629730e-04 9.99998927e-01\n",
      " 2.05735564e-02 9.97910559e-01 3.85767184e-02 8.69161729e-03\n",
      " 9.99996066e-01 1.00000000e+00 9.99997497e-01 1.76051229e-01\n",
      " 9.99813855e-01 9.99969125e-01 8.99621725e-01 9.99965787e-01\n",
      " 9.99998689e-01 7.94249773e-01 1.00000000e+00 9.99864817e-01\n",
      " 6.26859963e-01 9.82253850e-01 1.71050459e-01 9.98627916e-02\n",
      " 9.83470976e-01 3.83708254e-03 1.00000000e+00 8.68918432e-05\n",
      " 1.20302240e-04 9.99999881e-01 6.80657709e-03 2.51463091e-04\n",
      " 6.54152100e-05 1.18776923e-02 8.51619625e-05 9.99460638e-01\n",
      " 5.41865528e-01 1.00000000e+00 1.77987731e-05 3.01661268e-02\n",
      " 6.42916933e-02 5.52490950e-02 9.99900818e-01 5.42790687e-04\n",
      " 5.78228151e-04 7.23711099e-04 9.94431317e-01 1.65684838e-02\n",
      " 6.18105568e-02 1.32082403e-01 5.80834749e-04 9.97582912e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 210 [0/106 (0%)]\tTrain Loss: 0.000040\n",
      "Train Epoch: 210 [10/106 (9%)]\tTrain Loss: 0.133727\n",
      "Train Epoch: 210 [20/106 (19%)]\tTrain Loss: 0.000024\n",
      "Train Epoch: 210 [30/106 (28%)]\tTrain Loss: 0.105691\n",
      "Train Epoch: 210 [40/106 (38%)]\tTrain Loss: 0.000016\n",
      "Train Epoch: 210 [50/106 (47%)]\tTrain Loss: 0.000127\n",
      "Train Epoch: 210 [60/106 (57%)]\tTrain Loss: 0.000269\n",
      "Train Epoch: 210 [70/106 (66%)]\tTrain Loss: 0.000003\n",
      "Train Epoch: 210 [80/106 (75%)]\tTrain Loss: 0.000065\n",
      "Train Epoch: 210 [90/106 (85%)]\tTrain Loss: 0.000045\n",
      "Train Epoch: 210 [100/106 (94%)]\tTrain Loss: 0.000011\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 417/424 (98%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.08072363e-05 1.25785315e-04 1.31374509e-06 9.17917532e-07\n",
      " 3.92857402e-07 1.78222331e-06 4.00994722e-05 1.25514543e-06\n",
      " 1.52189532e-05 1.19573861e-05 1.22041581e-06 5.05433081e-06\n",
      " 2.49215805e-06 2.11055558e-05 1.93486398e-04 7.29088697e-07\n",
      " 4.35022457e-06 2.65757088e-02 2.74635731e-05 9.95878220e-01\n",
      " 1.55404583e-03 2.41021975e-03 2.39822209e-01 9.99976039e-01\n",
      " 7.37310475e-05 9.99925852e-01 9.99999881e-01 4.04852006e-04\n",
      " 6.19272978e-05 3.87479085e-04 2.62699902e-01 1.27218693e-01\n",
      " 2.33112078e-04 6.61265722e-06 5.08917140e-07 2.82243917e-07\n",
      " 3.99745602e-07 2.25024132e-06 1.16258761e-05 2.21946466e-05\n",
      " 4.08378779e-04 7.38902486e-07 1.13946873e-04 3.73566309e-06\n",
      " 3.63760555e-05 1.02826079e-05 1.22767378e-04 1.00418099e-03\n",
      " 9.78542805e-01 7.48305974e-05 9.99228954e-01 4.25307860e-08\n",
      " 8.69408632e-06 1.65070389e-07 3.92270014e-02 3.26383770e-06\n",
      " 9.98520434e-01 8.91713228e-07 3.20430809e-05 1.74301676e-05\n",
      " 9.99553144e-01 3.64075691e-01 9.99950051e-01 9.99958992e-01\n",
      " 9.99560177e-01 1.17386999e-05 2.33778905e-04 9.99981880e-01\n",
      " 6.77100429e-03 9.88451958e-01 1.02137495e-02 1.44852069e-03\n",
      " 9.99939084e-01 9.99999285e-01 9.99969959e-01 1.35012269e-01\n",
      " 9.99189079e-01 9.99744356e-01 5.32426953e-01 9.99835372e-01\n",
      " 9.99955416e-01 8.38922977e-01 9.99999762e-01 9.99412656e-01\n",
      " 4.81813282e-01 9.57371533e-01 1.36098146e-01 5.45568317e-02\n",
      " 9.87533212e-01 1.04384532e-03 9.99995232e-01 4.17267765e-05\n",
      " 2.26715729e-05 9.99997377e-01 2.97780614e-03 1.14450093e-04\n",
      " 2.51020247e-05 5.34451334e-03 2.20132370e-05 9.99047220e-01\n",
      " 1.84923351e-01 9.99998569e-01 4.29860393e-06 1.11298664e-02\n",
      " 3.08563095e-02 1.31103825e-02 9.99568403e-01 1.48706822e-04\n",
      " 1.65059464e-04 1.50422755e-04 9.94790792e-01 6.71360223e-03\n",
      " 1.03878275e-01 1.31860767e-02 1.26464802e-04 9.97491837e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 27 TN= 53 FN= 29 FP= 7\n",
      "TP+FP 34\n",
      "precision 0.7941176470588235\n",
      "recall 0.48214285714285715\n",
      "F1 0.6\n",
      "acc 0.6896551724137931\n",
      "AUCp 0.6827380952380953\n",
      "AUC 0.8227678571428572\n",
      "\n",
      " The epoch is 210, average recall: 0.4821, average precision: 0.7941,average F1: 0.6000, average accuracy: 0.6897, average AUC: 0.8228\n",
      "Train Epoch: 211 [0/106 (0%)]\tTrain Loss: 0.000077\n",
      "Train Epoch: 211 [10/106 (9%)]\tTrain Loss: 0.000076\n",
      "Train Epoch: 211 [20/106 (19%)]\tTrain Loss: 0.000029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/data/taoliu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-12-efee3a4b46be>\", line 5, in <module>\n",
      "    train_acc, train_obj = train(train_loader, model, criterion, optimizer)\n",
      "  File \"<ipython-input-11-731ad0c3d451>\", line 33, in train\n",
      "    optimizer.step()\n",
      "  File \"/data/taoliu/anaconda3/lib/python3.6/site-packages/torch/optim/lr_scheduler.py\", line 66, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/data/taoliu/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\", line 103, in step\n",
      "    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/taoliu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/taoliu/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/data/taoliu/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/data/taoliu/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/data/taoliu/anaconda3/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/data/taoliu/anaconda3/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/data/taoliu/anaconda3/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/data/taoliu/anaconda3/lib/python3.6/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/data/taoliu/anaconda3/lib/python3.6/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/data/taoliu/anaconda3/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/data/taoliu/anaconda3/lib/python3.6/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "for epoch in range(total_epoch):\n",
    "\n",
    "    scheduler.step()\n",
    "    model.drop_path_prob = args.drop_path_prob * epoch / args.epochs\n",
    "    train_acc, train_obj = train(train_loader, model, criterion, optimizer)\n",
    "    targetlist, scorelist, predlist = infer(val_loader, model, criterion,epoch)\n",
    "    \n",
    "    print('target',targetlist)\n",
    "    print('score',scorelist)\n",
    "    print('predict',predlist)\n",
    "    vote_pred = vote_pred + predlist \n",
    "    vote_score = vote_score + scorelist \n",
    "\n",
    "    if epoch % votenum == 0:\n",
    "        \n",
    "        # major vote\n",
    "        vote_pred[vote_pred <= (votenum/2)] = 0\n",
    "        vote_pred[vote_pred > (votenum/2)] = 1\n",
    "        vote_score = vote_score/votenum\n",
    "        \n",
    "        print('vote_pred', vote_pred)\n",
    "        print('targetlist', targetlist)\n",
    "        TP = ((vote_pred == 1) & (targetlist == 1)).sum()\n",
    "        TN = ((vote_pred == 0) & (targetlist == 0)).sum()\n",
    "        FN = ((vote_pred == 0) & (targetlist == 1)).sum()\n",
    "        FP = ((vote_pred == 1) & (targetlist == 0)).sum()\n",
    "        \n",
    "        \n",
    "        print('TP=',TP,'TN=',TN,'FN=',FN,'FP=',FP)\n",
    "        print('TP+FP',TP+FP)\n",
    "        p = TP / (TP + FP)\n",
    "        print('precision',p)\n",
    "        p = TP / (TP + FP)\n",
    "        r = TP / (TP + FN)\n",
    "        print('recall',r)\n",
    "        F1 = 2 * r * p / (r + p)\n",
    "        acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "        print('F1',F1)\n",
    "        print('acc',acc)\n",
    "        AUC = roc_auc_score(targetlist, vote_score)\n",
    "        print('AUCp', roc_auc_score(targetlist, vote_pred))\n",
    "        print('AUC', AUC)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         if epoch == total_epoch:\n",
    "        torch.save(model.state_dict(), \"model_backup/{}.pt\".format(modelname))  \n",
    "        \n",
    "        vote_pred = np.zeros(valset.__len__())\n",
    "        vote_score = np.zeros(valset.__len__())\n",
    "        print('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
    "        epoch, r, p, F1, acc, AUC))\n",
    "\n",
    "        f = open('model_result/{}.txt'.format(modelname), 'a+')\n",
    "        f.write('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
    "        epoch, r, p, F1, acc, AUC))\n",
    "        f.close()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
